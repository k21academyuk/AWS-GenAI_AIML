{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d89c5b-ae42-4442-b9dc-de68f4fddcdb",
   "metadata": {},
   "source": [
    "# Abstractive Text Summarization with Amazon Titan Using LangChain\n",
    "\n",
    "## Why This Lab?\n",
    "\n",
    "This lab demonstrates how to perform **abstractive text summarization** using **Amazon Titan** with the help of **LangChain**. Abstractive summarization is an essential task in natural language processing (NLP) where the goal is to generate a concise summary that captures the essential meaning of a document. This lab will guide you through leveraging **Amazon Titan's powerful language model** and **LangChain's flexible pipeline** to automate this process, making it easier to work with large amounts of text data.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Tools & Technologies\n",
    "\n",
    "In this lab, we will use:\n",
    "\n",
    "- **Amazon Titan**: A state-of-the-art language model provided by Amazon Web Services, used for generating summaries and performing other text-based tasks.\n",
    "- **LangChain**: A framework for developing applications using language models in a structured way.\n",
    "- **AWS SDK (`boto3`)**: A Python SDK to interact with Amazon Web Services, particularly **Amazon Titan**.\n",
    "- **Python**: The programming language used to implement and run the code in this lab.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Install Required Libraries\n",
    "\n",
    "Before we start, we need to install and upgrade a few necessary libraries that will help us interact with **Amazon Titan** and **LangChain**. Run the following command in a code cell to install the required packages:\n",
    "### Explanation:\n",
    "\n",
    "- **langchain**: This library provides a framework to integrate language models and external tools (like AWS services) together.\n",
    "- **langchain-core**: This package contains the core functionalities of LangChain, enabling model interactions.\n",
    "- **langchain-community**: This package contains community-driven integrations for LangChain.\n",
    "- **langchain-aws**: Provides the necessary integrations to interact with AWS services, including Amazon Titan.\n",
    "\n",
    "⬇️Run this code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de282786-f826-49a1-ad39-d8de1c1410df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T11:11:22.429871Z",
     "iopub.status.busy": "2025-10-23T11:11:22.429614Z",
     "iopub.status.idle": "2025-10-23T11:11:24.579517Z",
     "shell.execute_reply": "2025-10-23T11:11:24.578728Z",
     "shell.execute_reply.started": "2025-10-23T11:11:22.429849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: langchain-core in /opt/conda/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.12/site-packages (0.4)\n",
      "Requirement already satisfied: langchain-aws in /opt/conda/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langchain) (1.0.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/conda/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/conda/lib/python3.12/site-packages (from langchain-core) (0.3.45)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/conda/lib/python3.12/site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/conda/lib/python3.12/site-packages (from langchain-core) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (1.0.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/conda/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /opt/conda/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/conda/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/conda/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (3.11.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (4.10.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langsmith<1.0.0,>=0.3.45->langchain-core) (1.26.19)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (2.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy_extensions>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: boto3>=1.40.19 in /opt/conda/lib/python3.12/site-packages (from langchain-aws) (1.40.57)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.57 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.40.19->langchain-aws) (1.40.57)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.40.19->langchain-aws) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.40.19->langchain-aws) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.41.0,>=1.40.57->boto3>=1.40.19->langchain-aws) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.57->boto3>=1.40.19->langchain-aws) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain langchain-core langchain-community langchain-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d641d9-ec06-450e-b6ff-a6094918572c",
   "metadata": {},
   "source": [
    "## Restart the Kernel\n",
    "\n",
    "Once the libraries are installed and the environment is set up, it's often a good practice to **restart the kernel** to ensure that all changes take effect properly. This can help in clearing any cached variables or memory, ensuring that your environment is clean and up-to-date.\n",
    "\n",
    "To restart the kernel:\n",
    "\n",
    "1. Go to the **Kernel** menu in the Jupyter notebook.\n",
    "2. Click on **Restart**.\n",
    "\n",
    "### Why This Step?\n",
    "\n",
    "- **Clears memory**: Restarting the kernel helps in clearing out old variables and functions that may cause conflicts or errors.\n",
    "- **Ensures proper initialization**: It ensures that the libraries and environment variables set in Step 1 are properly loaded and available in the current session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e6fa9-2dfa-427a-8c17-b7835b54bc5f",
   "metadata": {},
   "source": [
    "## Step 2: Set Up AWS Environment Variables\n",
    "\n",
    "Before interacting with AWS services, we need to configure the **AWS environment variables**. These variables will allow us to specify settings like the **AWS region**, which are essential for interacting with services such as **Amazon Titan**.\n",
    "\n",
    "### Why This Code?\n",
    "\n",
    "This code sets the **AWS region** in which we will operate, allowing the application to interact with AWS resources like **Amazon Titan**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea85d21-f059-4ea7-af5b-93a185da8f5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T11:11:33.489133Z",
     "iopub.status.busy": "2025-10-23T11:11:33.488949Z",
     "iopub.status.idle": "2025-10-23T11:11:33.493875Z",
     "shell.execute_reply": "2025-10-23T11:11:33.493235Z",
     "shell.execute_reply.started": "2025-10-23T11:11:33.489114Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your AWS environment variables\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"  # Update if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676cad92-8fd8-4c84-a44c-d043a8d0c157",
   "metadata": {},
   "source": [
    "## Step 3: Initialize LangChain Client\n",
    "\n",
    "Now that the environment is set up, we need to initialize the **LangChain client** to interact with **Amazon Titan**. This step involves specifying the **model ID**, **region**, and **AWS credentials** for accessing Amazon Titan and configuring its parameters.\n",
    "\n",
    "### Why This Code?\n",
    "\n",
    "This code initializes the **Amazon Titan model** through **LangChain** by specifying the model ID, region, and credentials. It also defines important model parameters such as the **maximum token count**, **temperature**, and **topP** for controlling the behavior of the model.\n",
    "\n",
    "### Explanation of the Code:\n",
    "\n",
    "- **`model_id=\"amazon.titan-text-express-v1\"`**:  \n",
    "  This specifies the **Amazon Titan** model that we want to use. The model ID uniquely identifies the specific variant of Amazon Titan. In this case, we're using the **text express version** of Amazon Titan (`amazon.titan-text-express-v1`).\n",
    "\n",
    "- **`region_name=os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")`**:  \n",
    "  This line sets the **AWS region** in which the model is deployed. It retrieves the region from the environment variables (e.g., `us-east-1`). If the region is not set, it defaults to **`us-east-1`**. You can change the default region to match where your AWS resources are hosted.\n",
    "\n",
    "- **`credentials_profile_name=os.environ.get(\"AWS_PROFILE\", None)`**:  \n",
    "  This specifies the **AWS profile** used for authentication. The `os.environ.get` function looks for a profile in the environment variables. If not provided, it defaults to **None** and uses the **default AWS profile**. You can configure different profiles for different AWS accounts or roles.\n",
    "\n",
    "- **`model_kwargs`**:\n",
    "  - **`maxTokenCount`**: Defines the **maximum number of tokens** (words or characters) the model can process in a single request. The higher the token count, the larger the document the model can handle in one go. In this example, the model is configured to handle up to **4096 tokens** per request.\n",
    "  \n",
    "  - **`temperature`**: Controls the **randomness** of the model's responses. A value of **0** means the output will be **deterministic** (i.e., always the same), while higher values (e.g., **0.7**) introduce more randomness and variability in the model’s output. In this case, **0** is used to get consistent results.\n",
    "  \n",
    "  - **`topP`**: This parameter defines the **diversity** of the responses generated by the model using **nucleus sampling**. A value of **1** means no filtering of tokens, allowing the model to generate the most diverse responses. A lower value (e.g., **0.9**) narrows down the selection to more probable tokens, reducing randomness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d11d37-2934-4291-bcdf-d2da583cee27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T11:11:36.533373Z",
     "iopub.status.busy": "2025-10-23T11:11:36.533094Z",
     "iopub.status.idle": "2025-10-23T11:11:40.498021Z",
     "shell.execute_reply": "2025-10-23T11:11:40.497155Z",
     "shell.execute_reply.started": "2025-10-23T11:11:36.533348Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_aws.llms.bedrock import BedrockLLM\n",
    "\n",
    "llm = BedrockLLM(\n",
    "    model_id=\"amazon.titan-text-express-v1\",\n",
    "    region_name=os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\"),\n",
    "    credentials_profile_name=os.environ.get(\"AWS_PROFILE\", None),  # ✅ Safe fallback\n",
    "    model_kwargs={\n",
    "        \"maxTokenCount\": 4096,\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 1,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0da21-e099-43a0-acd4-3379755be581",
   "metadata": {},
   "source": [
    "## Step 4: Load the Document for Summarization\n",
    "\n",
    "In this step, we load the document that we want to summarize. The document could be in any format (e.g., text file, PDF), but for simplicity, we’ll use a plain text file.\n",
    "\n",
    "### Why This Code?\n",
    "\n",
    "This code reads the contents of a document (in this case, a text file) into a variable so that we can process the content and generate a summary based on it.\n",
    "### Explanation of the Code:\n",
    "\n",
    "- **`letter_path = \"./letters/2022-letter.txt\"`**:  \n",
    "  This line specifies the **path** to the text file that contains the document you want to summarize. In this case, it points to a file named `2022-letter.txt`. You can update this path to match the location of your document.\n",
    "\n",
    "- **`with open(letter_path, \"r\") as file:`**:  \n",
    "  This line opens the file at the specified `letter_path` in **read mode** (`\"r\"`). The **`with`** statement ensures that the file is automatically closed after the operations are completed.\n",
    "\n",
    "- **`letter = file.read()`**:  \n",
    "  This line reads the entire content of the file and stores it in the variable `letter`. The contents of the file are now stored as a string, and we can process this string for summarization.\n",
    "\n",
    "### Why Load the Document?\n",
    "\n",
    "- **Processing Text**: We load the document so that we can process its content and generate summaries or perform other text-based tasks.\n",
    "\n",
    "- **Simplification**: By loading the entire document into memory, we simplify the process of passing it to the model for summarization, making the task more straightforward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc20f72-1d65-40c3-9a97-987a0b2b581d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T11:11:42.543495Z",
     "iopub.status.busy": "2025-10-23T11:11:42.543102Z",
     "iopub.status.idle": "2025-10-23T11:11:42.547025Z",
     "shell.execute_reply": "2025-10-23T11:11:42.546382Z",
     "shell.execute_reply.started": "2025-10-23T11:11:42.543471Z"
    }
   },
   "outputs": [],
   "source": [
    "letter_path = \"./letters/2022-letter.txt\"  # Update path if needed\n",
    "\n",
    "with open(letter_path, \"r\") as file:\n",
    "    letter = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48daf135-7dc3-469d-9aac-778e57011f7d",
   "metadata": {},
   "source": [
    "## Step 5: Split the Document into Chunks\n",
    "\n",
    "To avoid exceeding the model's token limit, we will split the document into smaller, manageable chunks. This helps ensure that the model can process the content efficiently.\n",
    "\n",
    "### Why This Code?\n",
    "\n",
    "This code splits the document into chunks of text to ensure that each chunk fits within the model### Explanation of the Code:\n",
    "\n",
    "- **`from langchain_text_splitters import RecursiveCharacterTextSplitter`**:  \n",
    "  This line imports the **`RecursiveCharacterTextSplitter`** class from the **`langchain_text_splitters`** module. This class is used to split the document into smaller chunks based on a specified character length, making it easier for the model to process.\n",
    "\n",
    "- **`text_splitter = RecursiveCharacterTextSplitter(...)`**:  \n",
    "  This line creates an instance of the `RecursiveCharacterTextSplitter` class, initializing it with specific parameters:\n",
    "  - **`separators=[\"\\n\\n\", \"\\n\"]`**: This defines the delimiters used to split the document. The text will be split wherever there are double newlines (`\\n\\n`) or single newlines (`\\n`).\n",
    "  - **`chunk_size=4000`**: This parameter specifies the **maximum size** of each chunk in terms of characters. The document will be split into chunks of up to **4000 characters**.\n",
    "  - **`chunk_overlap=100`**: This specifies the **overlap** between consecutive chunks. It ensures that each chunk overlaps with the previous one by **100 characters**, which helps maintain context between chunks.\n",
    "\n",
    "- **`docs = text_splitter.create_documents([letter])`**:  \n",
    "  This line processes the document stored in `letter` and splits it into smaller chunks based on the settings defined in the previous step. The result is stored in the `docs` list.\n",
    "\n",
    "- **`print(f\"{len(docs)} chunks loaded\")`**:  \n",
    "  This line prints the number of chunks the document has been split into, which helps to confirm how many pieces the model will process.\n",
    "\n",
    "### Why Split the Document?\n",
    "\n",
    "- **Token Limits**: Large documents may exceed the **token limits** of the model. By splitting the document into smaller chunks, we ensure that each chunk fits within the model’s processing capacity.\n",
    "  \n",
    "- **Maintaining Context**: The overlap ensures that important context is retained between chunks, making the summary more coherent.\n",
    "\n",
    "- **Simplifying Processing**: Breaking the document into smaller, manageable parts simplifies the process of passing it to the model for summarization or other NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244af6d8-beeb-461c-b93f-82c115a6b13e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T11:11:44.086930Z",
     "iopub.status.busy": "2025-10-23T11:11:44.086647Z",
     "iopub.status.idle": "2025-10-23T11:11:46.045508Z",
     "shell.execute_reply": "2025-10-23T11:11:46.044764Z",
     "shell.execute_reply.started": "2025-10-23T11:11:44.086910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 chunks loaded\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\"],\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([letter])\n",
    "print(f\"{len(docs)} chunks loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9616a-a823-4d02-b363-919098febce5",
   "metadata": {},
   "source": [
    "## Step 6: Define the Summarization Prompts\n",
    "\n",
    "In this step, we define the prompts that will guide the model to generate and refine the summary. We first create an **initial summary** and then **refine** it using additional context.\n",
    "\n",
    "### Why This Code?\n",
    "\n",
    "This code defines two prompts: one for the **initial summarization** and one for **refining the summary**. The **refining** process helps make the summary more accurate and comprehensive by adding more context from subsequent chunks of the document.\n",
    "\n",
    "### Explanation of the Code:\n",
    "\n",
    "- **`from langchain_core.prompts import PromptTemplate`**:  \n",
    "  This imports the **`PromptTemplate`** class, which allows us to create a template for the prompt that will be passed to the model. A **prompt template** is a pre-defined string that can be filled dynamically with content. This helps us structure the prompt to be used by the model.\n",
    "\n",
    "- **`initial_prompt = PromptTemplate.from_template(...)`**:  \n",
    "  This line creates the **initial summary prompt template**. The prompt asks the model to write a concise summary of the provided document. The **`{context}`** placeholder will be dynamically replaced with the content of the document. The model will use this prompt to generate the first summary of the document.\n",
    "\n",
    "- **`refine_prompt = PromptTemplate.from_template(...)`**:  \n",
    "  This defines the **refining summary prompt**. It asks the model to refine an existing summary by adding additional context. The **`{existing_answer}`** placeholder will be replaced by the previously generated summary, and the **`{context}`** placeholder will be filled with new content from the document. This helps the model improve the initial summary by incorporating more context.\n",
    "\n",
    "- **`def refine_chain(docs):`**:  \n",
    "  This function defines how the **refining process** works. It starts by generating the initial summary using the first chunk of the document. Then, it loops through the remaining chunks of the document, refining the summary by adding more context with each chunk. The refined summary is updated iteratively until all chunks are processed.\n",
    "\n",
    "- **`summary_chain = RunnableLambda(refine_chain)`**:  \n",
    "  This creates a **`RunnableLambda`** object, which is a runnable chain that allows us to execute the **`refine_chain`** function. This function will process all the document chunks and generate the final refined summary, which incorporates all context from the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10d4557-9c2d-4b4b-b981-dadee5f5ba6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T11:11:47.696347Z",
     "iopub.status.busy": "2025-10-23T11:11:47.695810Z",
     "iopub.status.idle": "2025-10-23T11:11:47.701302Z",
     "shell.execute_reply": "2025-10-23T11:11:47.700365Z",
     "shell.execute_reply.started": "2025-10-23T11:11:47.696319Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "initial_prompt = PromptTemplate.from_template(\n",
    "    \"Write a concise summary of the following document:\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "refine_prompt = PromptTemplate.from_template(\n",
    "    \"We have an existing summary:\\n\\n{existing_answer}\\n\\n\"\n",
    "    \"Refine it using the following additional context:\\n\\n{context}\\n\\n\"\n",
    "    \"Return a new, improved summary:\"\n",
    ")\n",
    "\n",
    "def refine_chain(docs):\n",
    "    summary = llm.invoke(initial_prompt.format(context=docs[0].page_content))\n",
    "    for doc in docs[1:]:\n",
    "        summary = llm.invoke(refine_prompt.format(existing_answer=summary, context=doc.page_content))\n",
    "    return summary\n",
    "\n",
    "summary_chain = RunnableLambda(refine_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfd0b9-3880-46df-9ffe-a1c031739ed9",
   "metadata": {},
   "source": [
    "## Step 7: Generate the Summary\n",
    "\n",
    "In this step, we invoke the **`summary_chain`** to generate the summary by processing the document chunks. If any errors occur during this process, they will be caught and handled.\n",
    "\n",
    "### Why This Code?\n",
    "\n",
    "This code executes the **`summary_chain`** function to generate the summary. It handles errors gracefully, ensuring that if the input is not formatted correctly, an appropriate message is shown.\n",
    "\n",
    "### Explanation of the Code:\n",
    "\n",
    "- **`output = \"\"`**:  \n",
    "  This line initializes the `output` variable as an empty string. This variable will store the generated summary once the **`summary_chain`** function is invoked.\n",
    "\n",
    "- **`try:`**:  \n",
    "  The **`try`** block is used to attempt the execution of the **`summary_chain.invoke(docs)`** function. This function processes the document chunks and generates the summary. If it executes successfully, the result is stored in the `output` variable.\n",
    "\n",
    "- **`output = summary_chain.invoke(docs)`**:  \n",
    "  This line calls the **`summary_chain.invoke(docs)`** method to generate the summary by processing the document chunks stored in **`docs`**. The generated summary is stored in the `output` variable.\n",
    "\n",
    "- **`except ValueError as error:`**:  \n",
    "  If a **`ValueError`** is raised (e.g., due to an issue with the input format), the code inside the **`except`** block will be executed.\n",
    "\n",
    "- **`if \"ValidationException\" in str(error):`**:  \n",
    "  This checks whether the error message contains **`ValidationException`**, which indicates that the input is not properly formatted. If the error matches this condition, the following message is printed:\n",
    "  \n",
    "  - **`print(f\"Validation Error: {error}. Ensure input is formatted correctly.\")`**: This prints the error message along with a suggestion to check the input format.\n",
    "\n",
    "- **`else: raise error`**:  \n",
    "  If the error is not related to input validation, the **`else`** block raises the error again, so it can be handled elsewhere or stop the program execution.\n",
    "\n",
    "- **`print(output.strip())`**:  \n",
    "  Finally, this line prints the generated summary. The **`strip()`** method is used to remove any leading or trailing whitespace from the summary.\n",
    "\n",
    "### Why Handle Errors?\n",
    "\n",
    "- **Graceful Error Handling**: This ensures that if the input format is wrong, the program will provide a helpful message rather than crashing.\n",
    "\n",
    "- **Input Validation**: If there's an issue with the formatting or input, handling errors helps identify the problem and makes debugging easier.\n",
    "\n",
    "- **Ensuring Clean Output**: The **`strip()`** method ensures that the output summary does not contain unnecessary whitespace, making it cleaner and more readable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b0eac2-6cc3-4d7d-acb0-e8fc97c769fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T11:11:49.465618Z",
     "iopub.status.busy": "2025-10-23T11:11:49.465328Z",
     "iopub.status.idle": "2025-10-23T11:13:36.148015Z",
     "shell.execute_reply": "2025-10-23T11:13:36.147221Z",
     "shell.execute_reply.started": "2025-10-23T11:11:49.465595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon CEO Dave Clark discusses the company's progress in 2022, including growth, innovation, and investment decisions. He highlights the company's commitment to customer experience, third-party marketplace, and long-term investments. Clark also mentions the challenges faced by the company in 2022, including macroeconomic conditions. Despite these challenges, Amazon was able to grow demand and innovate in its largest businesses. The company also made adjustments in its investment decisions and the way it will invent moving forward, while preserving long-term investments. Clark emphasizes the constant change in the global market and the need for companies to adapt and innovate to stay competitive. He concludes by expressing optimism and energy for the future of Amazon.\n",
      "\n",
      "There have also been times when macroeconomic conditions or operating inefficiencies have presented us with new challenges. For instance, in the 2001 dot-com crash, we had to secure letters of credit to buy inventory for the holidays, streamline costs to deliver better profitability for the business, yet still prioritized the long-term customer experience and business we were trying to build. In 2008-2009, we took several actions to manage the cost structure and efficiency of our Stores business, but we also balanced this streamlining with investment in customer experiences that we believed could be substantial future businesses with strong returns for shareholders. In 2008, AWS was still a fairly small, fledgling business. We knew we were on to something, but it still required substantial capital investment. There were voices inside and outside of the company questioning why Amazon (known mostly as an online retailer then) would be investing so much in cloud computing. But, we knew we were inventing something special that could create a lot of value for customers and Amazon in the future. We had a head start on potential competitors; and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision to continue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, with strong profitability, that has transformed how customers from start-ups to multinational companies to public sector organizations manage their technology infrastructure. Amazon would be a different company if we’d slowed investment in AWS during that 2008-2009 period.\n",
      "\n",
      "Change is always around the corner. Sometimes, you proactively invite it in, and sometimes it just comes a-knocking. But, when you see it’s coming, you have to embrace it. And, the companies that do this well over a long period of time usually succeed. I’m optimistic about our future prospects because I like the way our team is responding to the changes we see in front of us.\n",
      "\n",
      "Over the last several months, we took a deep look across the company, business by business, invention by invention, and asked ourselves whether we had conviction about each initiative’s long-term potential to drive enough revenue, operating income, free cash flow, and return on invested capital. In some cases, it led to us shuttering certain businesses. For instance, we stopped pursuing physical store concepts like our Bookstores and 4 Star stores, closed our Amazon Fabric and Amazon Care efforts, and moved on from some newer devices where we didn’t see a path to meaningful returns. In other cases, we looked at some programs that weren’t producing the returns we’d hoped (e.g. free shipping for all online grocery orders over $35) and amended them. We also reprioritized where to spend our resources, which ultimately led to the hard decision to eliminate 27,000 corporate roles. There are a number of other changes that we’ve made over the last several months to streamline our overall costs, and like most leadership teams, we’ll continue to evaluate what we’re seeing in our business and proceed adaptively.\n",
      "\n",
      "We also looked hard at how we were working together as a team and asked our corporate employees to come back to the office at least three days a week, beginning in May. During the pandemic, our employees rallied to get work done from home and did everything possible to keep up with the unexpected circumstances that presented themselves. It was impressive and I’m proud of the way our collective team came together to overcome unprecedented challenges for our customers, communities, and business. But, we don’t think it’s the best long-term approach. We’ve become convinced that collaborating and inventing is easier and more effective when we’re working together and learning from one another in person. The energy and riffing on one another’s ideas happen more freely, and many of the best Amazon inventions have had their breakthrough moments from people staying behind after a meeting and working through ideas on a whiteboard, or continuing the conversation on the walk back from a meeting, or just popping by a teammate’s office later that day with another thought. Invention is often messy. It wanders and meanders and marinates. Serendipitous interactions help it, and there are more of those in-person than virtually. It’s also significantly easier to learn, model, practice, and strengthen our culture when we’re in the office together most of the time and surrounded by our colleagues. Innovation and our unique culture have been incredibly important in our first 29 years as a company, and I expect it will be comparably so in the next 29.\n"
     ]
    }
   ],
   "source": [
    "output = \"\"\n",
    "try:\n",
    "    output = summary_chain.invoke(docs)\n",
    "except ValueError as error:\n",
    "    if \"ValidationException\" in str(error):\n",
    "        print(f\"Validation Error: {error}. Ensure input is formatted correctly.\")\n",
    "    else:\n",
    "        raise error\n",
    "\n",
    "print(output.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ea187-cb3b-40bd-8a19-55ab8f190806",
   "metadata": {},
   "source": [
    "## What You Learned\n",
    "\n",
    "In this lab, you learned how to use **Amazon Titan** and **LangChain** to perform **abstractive text summarization** on a document. You went through the following key steps:\n",
    "\n",
    "1. **Setting up the AWS environment**:  \n",
    "   You learned how to configure AWS environment variables such as the **AWS region** and **credentials** to ensure smooth communication with Amazon services.\n",
    "\n",
    "2. **Loading the document**:  \n",
    "   You learned how to load the document that needs to be summarized. The document is read into memory, making it ready for processing by the model.\n",
    "\n",
    "3. **Splitting the document into chunks**:  \n",
    "   You learned how to split large documents into smaller chunks to avoid exceeding the model’s token limit, ensuring efficient processing by the model.\n",
    "\n",
    "4. **Creating summarization prompts**:  \n",
    "   You learned how to define the **initial summary prompt** to summarize the first chunk and a **refining prompt** to improve the summary with each subsequent chunk of the document.\n",
    "\n",
    "5. **Generating the summary**:  \n",
    "   You learned how to invoke the **summary chain** to generate the final summary. You also handled errors gracefully to ensure the process runs smoothly.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Text summarization** with **Amazon Titan** and **LangChain** enables efficient summarization of large text datasets.\n",
    "- **Token limits** can be managed by splitting documents into chunks, ensuring that each chunk fits within the model’s processing capacity.\n",
    "- The **refining process** improves the coherence and accuracy of the summary by adding context as more chunks are processed.\n",
    "- **Error handling** ensures the process runs without interruptions and that issues with input format are managed appropriately.\n",
    "\n",
    "By following these steps, you gained practical experience in working with **state-of-the-art language models** and **cloud services** to automate text summarization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
