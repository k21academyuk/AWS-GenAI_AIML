{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OulShqPGCh3Q"
      },
      "source": [
        "# **Lab: Building Conversational AI Solutions with AWS Bedrock using Converse API**\n",
        "----\n",
        "\n",
        "This notebook provides sample code with step by step instructions for using Amazon Bedrock's **Converse API**.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Introduction**\n",
        "\n",
        "Welcome to this introduction to building conversational AI with Amazon Bedrock's Converse API! The primary goal of this chapter is to provide a comprehensive introduction to Amazon Bedrock APIs for generating text. While we'll explore various use cases like summarization and code generation, our focus is on understanding the API patterns.\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "1. Learn the basics of the Amazon Bedrock **Invoke API**\n",
        "2. Explore the more powerful **Converse API** and it's features like multi-turn conversation, streaming, or function calling\n",
        "3. Apply these APIs across various foundation models\n",
        "4. Compare results across different state-of-the-art models"
      ],
      "metadata": {
        "id": "7CocInM3Duye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scanario**\n",
        "\n",
        "You are developing a **conversational AI** application for a healthcare provider to assist patients with booking appointments, retrieving medical records, and answering health-related queries.\n",
        "\n",
        "The system needs to leverage **AWS Bedrock** and the Amazon Titan embedding model to process and generate accurate, contextually relevant responses.\n",
        "\n",
        "Your task is to integrate **AWS Bedrock** with the **Titan embedding model** for natural language understanding and response generation, ensuring the system can handle multiple types of queries efficiently while providing reliable and relevant answers.\n",
        "\n",
        "### **Discription:**\n",
        "\n",
        "In this lab, you will integrate **AWS Bedrock** and the Amazon Titan embedding model to build a conversational AI system.\n",
        "\n",
        "You will configure the model to process user queries and generate intelligent responses. The lab will guide you through setting up **AWS API Gateway** and **Lambda functions** to expose the conversational API, optimizing the model for performance and accuracy.\n",
        "\n",
        "You will also learn error handling, troubleshooting, and best practices to ensure the system delivers seamless and accurate responses in real-time."
      ],
      "metadata": {
        "id": "LhjIBumuEg0E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUDvvhwxCh3T"
      },
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YQ0GCIgCh3U"
      },
      "source": [
        "### **1.1 Import the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tRBLwfmCh3U"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import boto3\n",
        "import botocore\n",
        "from IPython.display import display, Markdown\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** (No output for successful import. If run in a live environment, the code would execute silently.)"
      ],
      "metadata": {
        "id": "MdiLtOcxNehG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Library Imports Explanation**\n",
        "\n",
        "In this step, we are importing specific libraries that will allow us to interact with AWS services, handle JSON data, display outputs in a Jupyter notebook, and manage timing.\n",
        "\n",
        "- **`json`**: Handles **JSON data** for parsing and generating JSON objects, commonly used in API responses from AWS.\n",
        "  \n",
        "- **`boto3`**: AWS SDK for Python, allowing interaction with **AWS services** (e.g., S3, EC2) to manage resources.\n",
        "  \n",
        "- **`botocore`**: Low-level library that **boto3** uses to manage requests, errors, and session handling with AWS.\n",
        "  \n",
        "- **`IPython.display`**: Displays **rich content** (e.g., **Markdown**) in Jupyter notebooks for better presentation.\n",
        "  \n",
        "- **`time`**: Provides time functions like **`sleep()`** to pause execution and manage time intervals.\n",
        "\n",
        "These libraries enable efficient interaction with AWS, data handling, and display management in Jupyter notebooks.\n"
      ],
      "metadata": {
        "id": "jyV3QZcWGPrR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PPacq_dCh3U"
      },
      "source": [
        "### **1.2 Initial setup for clients, global variables and helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv5pNfjlCh3V"
      },
      "outputs": [],
      "source": [
        "# Initialize a boto3 session, which allows interaction with AWS services.\n",
        "# A session is used to manage AWS credentials, configurations, and region settings.\n",
        "session = boto3.session.Session()\n",
        "\n",
        "# Get the region name of the session to configure the AWS client appropriately.\n",
        "region = session.region_name\n",
        "\n",
        "# Initialize the Bedrock client using the session's region.\n",
        "# This client will be used to interact with Amazon Bedrock's API for running foundation models.\n",
        "bedrock = boto3.client(service_name='bedrock-runtime', region_name=region)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** *(No output for successful client initialization. If run in a live environment, the code would execute silently and the Bedrock client would be ready.)*"
      ],
      "metadata": {
        "id": "YspmvqpGNhnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we are setting up the necessary clients and global variables required for interacting with **AWS Bedrock**.\n",
        "\n",
        "- **`boto3.session.Session()`**:\n",
        "  - Initializes a new **Boto3 session**. This session is responsible for managing the configuration and credentials that Boto3 uses to interact with AWS services. By creating a session, we ensure that all interactions with AWS are properly authenticated and configured.\n",
        "\n",
        "- **`region = session.region_name`**:\n",
        "  - Retrieves the **AWS region** that is currently configured for the session. This allows the application to automatically use the region specified in your environment or AWS configuration.\n",
        "\n",
        "- **`bedrock = boto3.client(service_name='bedrock-runtime', region_name=region)`**:\n",
        "  - Initializes the **AWS Bedrock client** using **Boto3**. The `bedrock-runtime` client is responsible for interacting with the Bedrock service, which provides access to foundation models and other AI services. The region is passed dynamically from the session, ensuring that the client is created for the correct AWS region.\n",
        "\n",
        "This setup prepares the environment to interact with **AWS Bedrock** and sets up the necessary client configuration for making API requests.\n"
      ],
      "metadata": {
        "id": "CcbarTeBJO5M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cw0--R4Ch3V"
      },
      "outputs": [],
      "source": [
        "# Define model IDs that will be used in this module\n",
        "\n",
        "# These model IDs are required to interact with the specific foundation models using Amazon Bedrock API.\n",
        "\n",
        "MODELS = {\n",
        "\n",
        "    \"Claude 3.7 Sonnet\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\", # Claude 3.7 Sonnet model from Anthropic, using the model ID with versioning\n",
        "    \"Claude 3.5 Sonnet\": \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\", # Claude 3.5 Sonnet model from Anthropic with versioning\n",
        "    \"Claude 3.5 Haiku\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\", # Claude 3.5 Haiku model from Anthropic, another variation with versioning\n",
        "    \"Amazon Nova Pro\": \"us.amazon.nova-pro-v1:0\",  # Amazon Nova Pro model, a generative AI model from Amazon\n",
        "    \"Amazon Nova Micro\": \"us.amazon.nova-micro-v1:0\",  # Amazon Nova Micro model, a smaller version of Amazon Nova for resource-constrained environments\n",
        "    \"Meta Llama 3.1 70B Instruct\": \"us.meta.llama3-1-70b-instruct-v1:0\" # Meta Llama 3.1 70B Instruct model from Meta, used for instruction-following tasks\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** (No output. The MODELS dictionary is defined in memory.)"
      ],
      "metadata": {
        "id": "IVwYsBAlNq0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code helps you easily reference different AI models by their unique IDs, making it simpler to switch models or call them in subsequent parts of the code.\n",
        "\n",
        "By organizing the models in a dictionary, you can easily iterate or look up specific models as needed in your application."
      ],
      "metadata": {
        "id": "gz8SnU_KKTma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUr9aLckCh3V"
      },
      "outputs": [],
      "source": [
        "# Utility function to display model responses in a more readable format\n",
        "\n",
        "def display_response(response, model_name=None):\n",
        "    # If a model name is provided, display it as a Markdown header\n",
        "    if model_name:\n",
        "        display(Markdown(f\"### Response from {model_name}\"))\n",
        "\n",
        "    # Display the model's response as Markdown content (formatted text)\n",
        "    display(Markdown(response))\n",
        "\n",
        "    # Print a separator line for better readability of the output\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** (No output. The `display_response` function is defined in memory.)"
      ],
      "metadata": {
        "id": "7ffouhrINx9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This utility function, `display_response`, is designed to display the **response from a model** in a more readable and formatted way. It enhances the readability of the output in Jupyter notebooks, making it easier to interpret results from the AI models.\n",
        "\n",
        "- **Function Parameters**:\n",
        "  - **`response`**: The model's output (usually a string) that you want to display.\n",
        "  - **`model_name`** (optional): The name of the model that generated the response. If provided, it will be included in the display header.\n",
        "\n",
        "- **What the Function Does**:\n",
        "  1. **If `model_name` is provided**: It displays a markdown header with the model's name (e.g., \"Response from Claude 3.7 Sonnet\").\n",
        "  2. **Displays the response**: The function then displays the actual **response** (the model output) using **Markdown** for proper formatting.\n",
        "  3. **Prints a separator line**: After the response, a line of dashes (`-`) is printed to clearly separate the outputs when multiple responses are displayed.\n",
        "\n",
        "- **Why We Use It**:\n",
        "  - The function makes the model responses easy to read and well-structured, especially in Jupyter notebooks where markdown formatting is useful for visual clarity."
      ],
      "metadata": {
        "id": "AuU5wTp1KifP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCKtdvlmCh3V"
      },
      "source": [
        "## **2. Text Summarization with Foundation Models**\n",
        "\n",
        "Let's start by exploring how to leverage Amazon Bedrock APIs for text summarization. We'll first use the basic Invoke API, then introduce the more powerful Converse API.\n",
        "\n",
        "As an example, let's take a paragraph about Amazon Bedrock from an [AWS blog post](https://aws.amazon.com/jp/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HD5Ja9wCh3W"
      },
      "outputs": [],
      "source": [
        "text_to_summarize = \"\"\"\n",
        "AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, \\\n",
        "a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. \\\n",
        "Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, \\\n",
        "democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs \\\n",
        "for text and images—including Amazons Titan FMs, which consist of two new LLMs we're also announcing \\\n",
        "today—through a scalable, reliable, and secure AWS managed service. With Bedrock's serverless experience, \\\n",
        "customers can easily find the right model for what they're trying to get done, get started quickly, privately \\\n",
        "customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS \\\n",
        "tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations \\\n",
        "with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** *(No output. The `text_to_summarize` variable is defined.)*"
      ],
      "metadata": {
        "id": "rBbBItTLOFn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we will use Amazon Bedrock's **Invoke API** to perform text summarization on a sample paragraph. This will demonstrate how to leverage **Foundation Models (FMs)** from **Amazon Bedrock** to quickly summarize lengthy content.\n",
        "\n",
        "**Text to Summarize:**\n",
        "\n",
        "We will start with the following text about **Amazon Bedrock** from an AWS blog post.\n",
        "\n",
        "The Above paragraph describes Amazon Bedrock, an AWS service that enables easy access to foundation models (FMs) from various providers, simplifying the process of building and scaling generative AI applications."
      ],
      "metadata": {
        "id": "HCSmbembLCE-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlWG2rWJCh3W"
      },
      "source": [
        "### **2.1 Text Summarization using the Invoke Model API**\n",
        "\n",
        "Amazon Bedrock's **Invoke Model API** serves as the most basic method for sending requests to foundation models. Since each model family has its own distinct request and response format, you'll need to craft specific JSON payloads tailored to each model.\n",
        "\n",
        "For this example, we will call Claude 3.7 Sonnet via Invoke Model API (using the `invoke_model` function of the Bedrock Runtime Client) to generate a summary of our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTFYowU6Ch3W"
      },
      "outputs": [],
      "source": [
        "# Create the prompt for summarization\n",
        "# We are using an f-string to dynamically insert the `text_to_summarize` into the prompt for the model.\n",
        "\n",
        "prompt = f\"\"\"Please provide a summary of the following text. Do not add any information that is not mentioned in the text below.\n",
        "<text>\n",
        "{text_to_summarize}  # Insert the long text to be summarized here\n",
        "</text>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** *(No output. The `prompt` variable is defined.)*"
      ],
      "metadata": {
        "id": "ADKzNHEMOTmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prompt is designed to guide the model to generate an accurate and concise summary of the provided paragraph without introducing any additional information.\n",
        "\n",
        "**Example Usage:**\n",
        "\n",
        "This prompt will be sent to a text summarization API (e.g., Amazon Bedrock) to generate a summary of the content. The result will be a shortened, easy-to-read version of the input paragraph."
      ],
      "metadata": {
        "id": "nfuJw3TMMFV7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kvJ7022Ch3W"
      },
      "outputs": [],
      "source": [
        "# Create request body for Claude 3.7 Sonnet\n",
        "\n",
        "claude_body = json.dumps({                     # Convert the dictionary into a JSON-formatted string for the API\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\", # Specifies the version of the Claude model API being called\n",
        "    \"max_tokens\": 1000,                        # Maximum number of tokens the model is allowed to generate\n",
        "    \"temperature\": 0.5,                        # Controls creativity (0 = deterministic, 1 = creative)\n",
        "    \"top_p\": 0.9,                              # Nucleus sampling to control diversity of the output\n",
        "    \"messages\": [                              # List of messages representing the conversation history\n",
        "        {\n",
        "            \"role\": \"user\",                    # Indicates that the message is coming from the user\n",
        "            \"content\": [{\"type\": \"text\", \"text\": prompt}]  # Actual text prompt asking for summarization\n",
        "        }\n",
        "    ],\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** *(No output. The `claude_body` JSON string is created.)*"
      ],
      "metadata": {
        "id": "nqtaCT3NOnZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we are using the `json.dumps()` method to prepare the **request body** for the **Claude 3.7 Sonnet** model. This request body is in **JSON format**, which is required by the **Amazon Bedrock API** to process the request.\n",
        "\n",
        "\n",
        "1. **`json.dumps()`**:\n",
        "   - We are using the `json.dumps()` method to convert a **Python dictionary** into a **JSON string**. This is necessary because APIs typically expect the data in **JSON format**.\n",
        "\n",
        "2. **`anthropic_version`**:\n",
        "   - This specifies the **version** of the Claude model being used. In this case, it is set to `bedrock-2023-05-31`. This version indicates which model and API version the request will interact with.\n",
        "\n",
        "3. **`max_tokens`**:\n",
        "   - This parameter defines the **maximum number of tokens** the model should generate in the response. Here, it is set to `1000` tokens. A **token** typically represents a word or part of a word, and this controls the length of the response generated by the model.\n",
        "\n",
        "4. **`temperature`**:\n",
        "   - This controls the **creativity** of the generated response. A value of `0.5` strikes a balance between **randomness** and **determinism**.\n",
        "   - Lower values (closer to 0) make the model's output more focused and deterministic, while higher values (closer to 1) make it more creative and diverse.\n",
        "\n",
        "5. **`top_p`**:\n",
        "   - This parameter controls **nucleus sampling**. It defines the **probability threshold** for selecting tokens.\n",
        "   - A value of `0.9` means the model will only consider the **top 90%** of possible tokens for generating the next word, ensuring that the output remains coherent and contextually relevant.\n",
        "\n",
        "6. **`messages`**:\n",
        "   - This section contains the **conversation history**.\n",
        "   - The model receives a **message** from the **user** (you), containing the **summarization prompt** as input content.\n",
        "   - The **role** (`user`) indicates who is sending the message, and the **content** is the actual **text prompt** that needs to be summarized.\n",
        "\n",
        "This request body is then sent to the **Claude 3.7 Sonnet** model, which processes the input and generates a response based on the provided prompt and settings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OXDtaaUROXm9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SENzleRtCh3W"
      },
      "outputs": [],
      "source": [
        "# Send request to Claude 3.7 Sonnet\n",
        "\n",
        "try:\n",
        "    response = bedrock.invoke_model(                     # Call the Bedrock Invoke API to run the model\n",
        "        modelId=MODELS[\"Claude 3.7 Sonnet\"],             # The model ID for Claude 3.7 Sonnet\n",
        "        body=claude_body,                                # JSON request body created earlier\n",
        "        accept=\"application/json\",                       # Expected response format\n",
        "        contentType=\"application/json\"                   # Content type of the request body\n",
        "    )\n",
        "    response_body = json.loads(response.get('body').read())  # Read and parse the model's JSON response\n",
        "\n",
        "    # Extract and display the response text\n",
        "    claude_summary = response_body[\"content\"][0][\"text\"]     # Extract the actual summary text from the response\n",
        "    display_response(claude_summary, \"Claude 3.7 Sonnet (Invoke Model API)\")  # Display formatted output\n",
        "\n",
        "except botocore.exceptions.ClientError as error:             # Handle AWS API errors\n",
        "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
        "        # Provide helpful message for access issues\n",
        "        print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
        "            \\nTo troubleshoot this issue please refer to the following resources.\\\n",
        "            \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
        "            \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
        "    else:\n",
        "        raise error                                          # Re-throw other unexpected errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Output:** (Assuming a successful API call, the output would look similar to this. The actual text will vary slightly each time.)"
      ],
      "metadata": {
        "id": "sZTq2IhSO7pL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we send a request to the **Claude 3.7 Sonnet** model using the **Amazon Bedrock API** and handle the response.\n",
        "\n",
        "#### `bedrock.invoke_model()`:\n",
        "\n",
        "- **Purpose**: This function sends a request to the **Claude 3.7 Sonnet** model (using the model ID defined in the `MODELS` dictionary) via the **Amazon Bedrock API**.\n",
        "  \n",
        "- **Parameters**:\n",
        "  - **`modelId`**: Specifies the model we are using (Claude 3.7 Sonnet).\n",
        "  - **`body`**: The prepared request body (`claude_body`), which contains the input for the model (in this case, the summarization prompt).\n",
        "  - **`accept`**: Specifies that we expect the response to be in **JSON format** (`application/json`).\n",
        "  - **`contentType`**: Specifies that the request body is also in **JSON format**.\n",
        "\n",
        "#### Response Parsing:\n",
        "\n",
        "- The **API response** is in JSON format. We use **`json.loads()`** to parse the JSON response into a Python dictionary.\n",
        "  \n",
        "- **Extracting the Summary**:\n",
        "  - After parsing the response, we extract the summary text from the response body.\n",
        "  - This accesses the **`text`** field within the **`content`** array in the response.\n",
        "\n",
        "#### Error Handling:\n",
        "\n",
        "- If the request fails due to an **Access Denied error** (denoted as `AccessDeniedException`), we display a **troubleshooting message** with relevant resources to resolve IAM permission issues.\n",
        "  \n",
        "- For other errors, the error is raised for further investigation.\n",
        "\n",
        "#### `display_response()`:\n",
        "\n",
        "- After extracting the summary from the API response, the **`display_response()`** function (defined earlier) is used to display the result in a **readable format** in the notebook.\n",
        "- This function ensures the response is formatted and displayed clearly for easy interpretation.\n",
        "\n",
        "\n",
        "This setup ensures that you can **send a request**, **process the response**, handle errors, and **display the result** from the **Claude 3.7 Sonnet** model in a structured and efficient manner.\n"
      ],
      "metadata": {
        "id": "gAgfQLwCOsR7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfiW19CxCh3W"
      },
      "source": [
        "### **2.2 Text Summarization using the Converse API (Recommended Approach)**\n",
        "\n",
        "While the **Invoke Model API** allows direct access to foundation models, it has several limitations:\n",
        "1. it uses different request/response formats for each model family;\n",
        "2. there is no built-in support for multi-turn conversations;\n",
        "3. it requires custom handling for different model capabilities.\n",
        "\n",
        "The **Converse API** addresses these limitations by providing a unified interface. Let's explore it on our text summarization task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP_i0Dt6Ch3W"
      },
      "outputs": [],
      "source": [
        "# Create a converse request with our summarization task\n",
        "\n",
        "converse_request = {\n",
        "    \"messages\": [  # List of messages representing the conversation\n",
        "        {\n",
        "            \"role\": \"user\",  # Role of the sender (user in this case)\n",
        "            \"content\": [  # Content of the message\n",
        "                {\n",
        "                    \"text\": f\"Please provide a concise summary of the following text in 2-3 sentences. Text to summarize: {text_to_summarize}\"  # The summarization prompt, including the text to summarize\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"inferenceConfig\": {  # Configuration settings for controlling the model’s response\n",
        "        \"temperature\": 0.4,  # Controls the randomness of the model's output (lower value for more deterministic output)\n",
        "        \"topP\": 0.9,  # Nucleus sampling for controlling diversity of the output\n",
        "        \"maxTokens\": 500  # Maximum number of tokens (words) for the summary response\n",
        "    }\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** *(No output. The `converse_request` dictionary is defined.)*"
      ],
      "metadata": {
        "id": "MMubmoGtPD2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we are sending a **request to the Converse API** for text summarization. Here's a breakdown of the key points:\n",
        "\n",
        "1. **`messages`**:\n",
        "   - The `messages` array contains the communication between the **user** and the **model**. In this case, the **user** is sending a request to the model to summarize the provided text in **2-3 sentences**.\n",
        "\n",
        "2. **`role`**:\n",
        "   - The `role` field specifies **who is sending the message**. Here, the role is **`user`**, indicating that the input is coming from the user.\n",
        "\n",
        "3. **`content`**:\n",
        "   - The `content` key holds the **actual text content** that we want the model to process. The **text field** contains a prompt that explicitly asks the model to **summarize** the provided text.\n",
        "\n",
        "4. **`inferenceConfig`**:\n",
        "   - **`temperature`** (0.4): This controls the **creativity** of the model’s response. A value of 0.4 keeps the output more focused and deterministic, preventing randomness.\n",
        "   - **`topP`** (0.9): This sets the **probability threshold** for token selection. A value of 0.9 means the model will only consider the **top 90%** of possible tokens, ensuring coherence and relevance.\n",
        "   - **`maxTokens`** (500): This defines the **maximum length** of the model's response. It limits the output to **500 tokens**, ensuring that the summary remains concise and manageable.\n",
        "\n",
        "##### **Purpose:**\n",
        "This request structure allows you to send the **text summarization prompt** to the **Converse API**. The model will process the input and generate a **summarized version** of the provided content, limited to **2-3 sentences**.\n",
        "\n",
        "\n",
        "This setup ensures that we can interact with the **Converse API** to perform text summarization efficiently, using well-defined configurations for response control.\n"
      ],
      "metadata": {
        "id": "ly01Qf92SB5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSlS7idHCh3X"
      },
      "outputs": [],
      "source": [
        "# Call Claude 3.7 Sonnet with Converse API\n",
        "\n",
        "try:\n",
        "    # Send the conversation request to the model using the Converse API\n",
        "    response = bedrock.converse(\n",
        "        modelId=MODELS[\"Claude 3.7 Sonnet\"],  # Model ID for Claude 3.7 Sonnet from the MODELS dictionary\n",
        "        messages=converse_request[\"messages\"],  # The message list containing the user prompt\n",
        "        inferenceConfig=converse_request[\"inferenceConfig\"]  # Configuration parameters for the model (temperature, maxTokens)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Extract the model's response from the JSON response body\n",
        "    claude_converse_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]  # Extracts the text from the response\n",
        "    display_response(claude_converse_response, \"Claude 3.7 Sonnet (Converse API)\")  # Display the model’s response using the display_response function\n",
        "\n",
        "\n",
        "except botocore.exceptions.ClientError as error:  # Handle any AWS API client errors\n",
        "    if error.response['Error']['Code'] == 'AccessDeniedException':  # If the error is due to access denial\n",
        "        # Print the error message in red, along with instructions for troubleshooting access issues\n",
        "        print(f\"\\x1b[41m{error.response['Error']['Code']}: {error.response['Error']['Message']}\\x1b[0m\")\n",
        "        print(\"Please ensure you have the necessary permissions for Amazon Bedrock.\")\n",
        "\n",
        "    else:\n",
        "        raise error  # For other errors, raise the exception for further investigation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Output:** (Assuming a successful API call, the output would look similar to this. The actual text will vary slightly but adhere to the 2-3 sentence limit.)"
      ],
      "metadata": {
        "id": "72-ki4VePPom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we send a **Converse request** to the **Claude 3.7 Sonnet** model using the **Amazon Bedrock API** and handle the response.\n",
        "\n",
        "#### `bedrock.converse()`:\n",
        "\n",
        "- **Purpose**: This function sends the **Converse request** to the **Claude 3.7 Sonnet** model via the **Amazon Bedrock API**.\n",
        "  \n",
        "- **Parameters**:\n",
        "  - **`modelId`**: Specifies the model we are using (Claude 3.7 Sonnet).\n",
        "  - **`messages`**: Contains the user input, which in this case is the **summarization prompt**.\n",
        "  - **`inferenceConfig`**: Defines the configuration for generating the response (e.g., **temperature**, **topP**, **maxTokens**).\n",
        "\n",
        "##### **Response Parsing:**\n",
        "\n",
        "- After sending the request, the **response** is parsed to extract the **summarized text**.\n",
        "\n",
        "\n",
        "##### **Error Handling:**\n",
        "\n",
        "- **Access Denied Error**:\n",
        "  - If an **Access Denied** error (`AccessDeniedException`) occurs, a troubleshooting message is displayed to guide the user on resolving **AWS IAM permission** issues.\n",
        "  \n",
        "- **Other Errors**:\n",
        "  - For any other errors, the exception is raised for further investigation.\n",
        "\n",
        "##### `display_response()`:\n",
        "\n",
        "- Once the summary is extracted, the **summarized content** is displayed in a readable format using the **`display_response()`** function. This function was defined earlier to present the output clearly.\n",
        "\n",
        "\n",
        "This code demonstrates how to send a request to the **Claude 3.7 Sonnet** model using the **Converse API**, process the response, and handle potential errors effectively.\n"
      ],
      "metadata": {
        "id": "xs0-bc1yVapm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjT44csZCh3X"
      },
      "source": [
        "### **2.3 Overview of the Converse API**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zErGKrFjCh3X"
      },
      "source": [
        "Now, that we have used the **Converse API**, let's take some time to take a closer look. To use the Converse API, you use the <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html\" target=\"_blank\">Converse</a> or <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html\" target=\"_blank\">ConverseStream</a> (for streaming responses) operations to send messages to a model.\n",
        "\n",
        "While, it is possible to use the existing base inference operations (<a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html\" target=\"_blank\">InvokeModel</a> or <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html\" target=\"_blank\">InvokeModelWithResponseStream</a>) for conversation applications as well, we recommend using the Converse API as it provides consistent API, that works with <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/converse-api.html\" target=\"_blank\">all Amazon Bedrock models that support messages</a>.\n",
        "\n",
        "This means you can write code once and use it with different models. Should a model have unique inference parameters, the Converse API also allows you to pass those unique parameters in a model specific structure.\n",
        "\n",
        "You can use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an Amazon Bedrock model.\n",
        "\n",
        "For example, you can create a chat bot that maintains a conversation over many turns and uses a persona or tone customization that is unique to your needs, such as a helpful technical support assistant. The Converse API also supports other Bedrock capabilites, like <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html\" target=\"_blank\">tool use</a> and <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-use-converse-api.html\" target=\"_blank\">guardrails</a>.\n",
        "\n",
        "\n",
        "Let's break down its key components (you can also review the <a href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\" target=\"_blank\">documentation</a> for a full list of parameters):\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"modelId\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\", // Required: Model identifier for the Claude 3.7 Sonnet model\n",
        "  \n",
        "  \"messages\": [ // Required: Conversation history\n",
        "    {\n",
        "      \"role\": \"user\", // Specifies that the message is from the user\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"text\": \"Your prompt or message here\" // The actual message content or prompt to the model\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ],\n",
        "  \n",
        "  \"system\": [ // Optional: System instructions that guide the model's behavior\n",
        "    {\n",
        "      \"text\": \"You are a helpful AI assistant.\" // Example: A simple instruction for the model to act as a helpful assistant\n",
        "    }\n",
        "  ],\n",
        "  \n",
        "  \"inferenceConfig\": { // Optional: Parameters for controlling model inference behavior\n",
        "    \"temperature\": 0.7, // Controls the randomness of the response (0.0 = deterministic, 1.0 = very random)\n",
        "    \"topP\": 0.9, // Controls diversity by setting a threshold for token selection, making the response more diverse\n",
        "    \"maxTokens\": 2000, // Maximum number of tokens (words/parts of words) allowed in the response\n",
        "    \"stopSequences\": [] // Optional: Define sequences to stop the model's response generation (e.g., specific words/phrases)\n",
        "  },\n",
        "  \n",
        "  \"toolConfig\": { // Optional: Settings for function calling setup (if using external tools or APIs)\n",
        "    \"tools\": [], // List of tools or functions the model can use (empty in this case)\n",
        "    \"toolChoice\": {\n",
        "      \"auto\": {} // Let the model decide automatically when to use the tools\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObCTS6ApCh3X"
      },
      "source": [
        "### **2.4 Easily switch between models**\n",
        "\n",
        "One of the biggest advantages of the Converse API is the ability to easily switch between models using the exact same request format. Let's compare summaries across different foundation models by looping over the model dictionary we defined above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vssNjdmCh3X"
      },
      "outputs": [],
      "source": [
        "# call different models with the same converse request\n",
        "\n",
        "results = {}  # Initialize an empty dictionary to store results for each model\n",
        "\n",
        "# Loop over all models defined in the MODELS dictionary\n",
        "for model_name, model_id in MODELS.items():  # model_name is the name of the model, model_id is its identifier\n",
        "    try:\n",
        "        # Record the start time to calculate response time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Send the converse request to the model\n",
        "        response = bedrock.converse(\n",
        "            modelId=model_id,  # The model ID to be used\n",
        "            messages=converse_request[\"messages\"],  # The messages to be sent to the model\n",
        "            inferenceConfig=converse_request[\"inferenceConfig\"] if \"inferenceConfig\" in converse_request else None  # Optional inference config\n",
        "        )\n",
        "\n",
        "        # Record the end time after receiving the response\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Extract the model's response from the API response\n",
        "        model_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
        "\n",
        "        # Calculate the response time\n",
        "        response_time = round(end_time - start_time, 2)\n",
        "\n",
        "        # Store the response and time in the results dictionary\n",
        "        results[model_name] = {\n",
        "            \"response\": model_response,  # Store the model's response\n",
        "            \"time\": response_time  # Store the time taken to get the response\n",
        "        }\n",
        "\n",
        "        # Print success message with model name and response time\n",
        "        print(f\"✅ Successfully called {model_name} (took {response_time} seconds)\")\n",
        "\n",
        "    except Exception as e:  # If an error occurs during the request\n",
        "        # Print the error message\n",
        "        print(f\"❌ Error calling {model_name}: {str(e)}\")\n",
        "\n",
        "        # Store the error message and time in the results dictionary\n",
        "        results[model_name] = {\n",
        "            \"response\": f\"Error: {str(e)}\",  # Store the error message\n",
        "            \"time\": None  # No time in case of an error\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Output:** (The output below simulates successful, time-tracked calls. Actual times will vary.)"
      ],
      "metadata": {
        "id": "zVeWQRZyPbit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step demonstrates how we call multiple models using the **Converse API** and track their responses and processing time.\n",
        "\n",
        "#### Key Points:\n",
        "\n",
        "1. **Looping through Models**:\n",
        "   - The `for` loop iterates over all the models listed in the `MODELS` dictionary. For each model, we extract the **`model_name`** (e.g., \"Claude\", \"Titan\") and the **`model_id`** (the model's unique identifier).\n",
        "\n",
        "2. **Sending the Converse Request**:\n",
        "   - For each model, the `bedrock.converse()` function is called with the following parameters:\n",
        "     - **`modelId`**: The unique identifier for the model.\n",
        "     - **`messages`**: The input prompt or conversation history sent to the model.\n",
        "     - **`inferenceConfig`** (optional): Parameters for controlling the response, like temperature, topP, and maxTokens.\n",
        "\n",
        "3. **Tracking Time**:\n",
        "   - **`start_time`** is recorded before making the request to monitor how long the model takes to process the input.\n",
        "   - **`end_time`** is recorded after receiving the response.\n",
        "   - The **`response_time`** is calculated by subtracting **`start_time`** from **`end_time`**.\n",
        "\n",
        "4. **Extracting the Model’s Response**:\n",
        "   - After receiving the response from the API, the **model's response** (summary or text output) is extracted from the returned JSON structure.\n",
        "   - This gets the **text content** of the model's response.\n",
        "\n",
        "5. **Handling Errors**:\n",
        "   - If an **error** occurs while calling the model (e.g., permission issues, API errors), an **exception** is caught.\n",
        "   - The error message is printed, and the error response is stored in the `results` dictionary for future review and troubleshooting.\n",
        "\n",
        "6. **Storing Results**:\n",
        "   - The **response** and **response time** for each model are stored in the `results` dictionary.\n",
        "   - This allows us to track both the **output** and the **time taken** for each API call.\n",
        "\n",
        "7. **Output**:\n",
        "   - If the **call is successful**, the model’s response and **response time** are printed in a **success message**.\n",
        "   - If there’s an **error**, an **error message** is printed, and the error response is stored in the `results`.\n",
        "\n",
        "---\n",
        "\n",
        "This approach ensures that we can call multiple models efficiently, compare their responses, and monitor how long each model takes to process the request. The results for each model, including the **response** and **response time**, are stored in the `results` dictionary for further analysis.\n"
      ],
      "metadata": {
        "id": "5uT_dcjMX41v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz-6vjoSCh3X"
      },
      "outputs": [],
      "source": [
        "# Display results in a formatted way\n",
        "\n",
        "for model_name, result in results.items():  # Loop through all models and their results\n",
        "    if \"Error\" not in result[\"response\"]:  # Check if there is no error in the model's response\n",
        "        # Display the model name and the time taken to process the request\n",
        "        display(Markdown(f\"### {model_name} (took {result['time']} seconds)\"))\n",
        "\n",
        "        # Display the model's response (summarization or output text)\n",
        "        display(Markdown(result[\"response\"]))\n",
        "\n",
        "        # Print a separator line for readability\n",
        "        print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Output:** (The following are representative examples of what the models might output, using the time placeholders from the previous step. The actual summary text would be the result of the prompt asking for a concise 2-3 sentence summary.)"
      ],
      "metadata": {
        "id": "xLtnnSeWP1LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we loop through the results of each model and display them in a **structured and readable format**.\n",
        "\n",
        "#### Key Points:\n",
        "\n",
        "1. **Looping through Results**:\n",
        "   - The `for` loop iterates through the `results` dictionary, where each **`model_name`** (e.g., \"Claude\", \"Titan\") and its corresponding **`result`** (response and time) are processed.\n",
        "\n",
        "2. **Checking for Errors**:\n",
        "   - The condition `if \"Error\" not in result[\"response\"]:` ensures that **only successful responses** are displayed. If there was an error in the model response (e.g., \"Error: Access Denied\"), that model’s result is skipped.\n",
        "   - This ensures that only the correct outputs are shown and that any errors are excluded from the displayed results.\n",
        "\n",
        "3. **Displaying Results**:\n",
        "   - For each successful model response, the **model name** and the **time taken** to get the response are displayed as a **Markdown heading**.\n",
        "   - The **model’s response** (summary or output text) is displayed underneath the heading.\n",
        "   - A separator line (`\"-\" * 80`) is printed after each model’s result for better readability, visually separating the outputs of different models.\n",
        "\n",
        "\n",
        "This approach helps organize the results of each model in a **clear and readable format**, making it easier to compare the outputs and analyze the performance of each model.\n"
      ],
      "metadata": {
        "id": "kYW5bhzjZsLP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj5nodqmCh3X"
      },
      "source": [
        "### **2.5 Cross-Regional Inference in Amazon Bedrock**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4QE0P4xCh3X"
      },
      "source": [
        "Amazon Bedrock offers Cross-Regional Inference which automatically selects the optimal AWS Region within your geography to process your inference requests.\n",
        "\n",
        "Cross-Regional Inference offers higher throughput limits (up to 2x allocated quotas) and seamlessly manages traffic bursts by dynamically routing requests across multiple AWS regions, enhancing application resilience during peak demand periods without additional routing or data transfer costs.\n",
        "\n",
        "Customers can control where their inference data flows by selecting from a pre-defined set of regions, helping them comply with applicable data residency requirements and sovereignty laws. Moreover, this capability prioritizes the connected Bedrock API source region when possible, helping to minimize latency and improve responsiveness.\n",
        "\n",
        "As a result, customers can enhance their applications' reliability, performance, and efficiency. Please review the list of <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html\" target=\"_blank\">supported regions and models for inference profiles</a>.\n",
        "\n",
        "To use Cross-Regional Inference, you simply need to specify a cross-region inference profile as the `modelId` when making a request. Cross-region inference profiles are identified by including a region prefix (e.g., `us.` or `eu.`) before the model name.\n",
        "\n",
        "For example:\n",
        "```json\n",
        "{\n",
        "    \"Amazon Nova Pro\": \"amazon.nova-pro-v1:0\",  # Regular model ID\n",
        "    \"Amazon Nova Pro (CRIS)\": \"us.amazon.nova-pro-v1:0\"  # Cross-regional model ID\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O_0zzEqCh3X"
      },
      "source": [
        "Let's see how easy it is to use Cross Region Inference by invoking the Claude 3.5 Sonnet model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usvD03JGCh3X"
      },
      "outputs": [],
      "source": [
        "# Regular model invocation (standard region)\n",
        "\n",
        "standard_response = bedrock.converse(\n",
        "    modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Standard model ID (default region)\n",
        "    messages=converse_request[\"messages\"]  # Passing the conversation history\n",
        ")\n",
        "\n",
        "# Cross-region inference (note the \"us.\" prefix)\n",
        "cris_response = bedrock.converse(\n",
        "    modelId=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Cross-region model ID (US region)\n",
        "    messages=converse_request[\"messages\"]  # Passing the conversation history\n",
        ")\n",
        "\n",
        "# Print responses\n",
        "print(\"Standard response:\", standard_response[\"output\"][\"message\"][\"content\"][0][\"text\"])  # Print the model response from the standard region\n",
        "print(\"Cross-region response:\", cris_response[\"output\"][\"message\"][\"content\"][0][\"text\"])  # Print the model response from the cross-region\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Output:** (The actual text will be very similar or identical, as the underlying model is the same, but they will be sourced from different endpoints.)"
      ],
      "metadata": {
        "id": "p2ztpAnNP-mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we demonstrate how to invoke a model both in the **standard region** and via **cross-region inference** using the `bedrock.converse()` function.\n",
        "\n",
        "#### Key Points:\n",
        "\n",
        "1. **Standard Model Invocation**:\n",
        "   - The first `bedrock.converse()` function call is a **standard invocation** using the model ID `\"anthropic.claude-3-5-sonnet-20240620-v1:0\"`. This sends the conversation request to the model in the **default region** (based on your AWS setup). This is the default configuration unless specified otherwise.\n",
        "\n",
        "2. **Cross-Region Inference**:\n",
        "   - The second `bedrock.converse()` function call is for **cross-region inference**. The model ID starts with `\"us.\"`, which specifies the region for the model, meaning the model is hosted in the **US region**.\n",
        "   - By specifying a **regional prefix** in the model ID, you can instruct the request to go to a specific region (in this case, the US). This can be important when working with **cross-region requests**, which may affect the **latency**, **availability**, or **compliance** of the request.\n",
        "\n",
        "3. **Response Extraction**:\n",
        "   - After both API calls, the responses from the models are parsed. The **text content** of the model's output is extracted using the key `\"content\"`.\n",
        "   - The responses from both the **standard region** and the **cross-region** models are printed for comparison.\n",
        "\n",
        "4. **Purpose**:\n",
        "   - This setup demonstrates how to invoke models both in the **standard region** (default AWS setup) and in a **cross-region** scenario by specifying a **regional prefix** in the model ID.\n",
        "   - The comparison between responses from both models helps **understand the impact of region-specific deployment**, such as response time and model consistency.\n",
        "\n",
        "This approach helps you explore and compare the differences between invoking models in the **default region** and using **cross-region inference**, providing insights into the behavior of models hosted in different regions.\n"
      ],
      "metadata": {
        "id": "fhveURNLasqX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14XOifueCh3X"
      },
      "source": [
        "### **2.6 Multi-turn Conversations**\n",
        "The Converse API makes multi-turn conversations simple. Let's see it in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIlczlxmCh3X"
      },
      "outputs": [],
      "source": [
        "# Example of a multi-turn conversation with Converse API\n",
        "\n",
        "multi_turn_messages = [\n",
        "    {\n",
        "        \"role\": \"user\",  # First message from the user (initial summary request)\n",
        "        \"content\": [{\"text\": f\"Please summarize this text: {text_to_summarize}\"}]  # User asks to summarize the text\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",  # Response from the assistant (Claude model)\n",
        "        \"content\": [{\"text\": results[\"Claude 3.7 Sonnet\"][\"response\"]}]  # Assistant’s first response (summary)\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",  # Follow-up message from the user (asking for a shorter summary)\n",
        "        \"content\": [{\"text\": \"Can you make this summary even shorter, just 1 sentence?\"}]  # User asks for a more concise summary\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Send the multi-turn conversation to the model using the Converse API\n",
        "    response = bedrock.converse(\n",
        "        modelId=MODELS[\"Claude 3.7 Sonnet\"],  # Specify the model (Claude 3.7 Sonnet)\n",
        "        messages=multi_turn_messages,  # Provide the conversation history (multi-turn)\n",
        "        inferenceConfig={\"temperature\": 0.2, \"maxTokens\": 500}  # Set inference configuration to control creativity and length\n",
        "    )\n",
        "\n",
        "    # Extract the model's response using the correct structure\n",
        "    follow_up_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]  # Extract the summarized response\n",
        "\n",
        "    # Display the follow-up response from the assistant\n",
        "    display_response(follow_up_response, \"Claude 3.7 Sonnet (Multi-turn conversation)\")\n",
        "\n",
        "except Exception as e:\n",
        "    # Catch and display any errors encountered during the request\n",
        "    print(f\"Error: {str(e)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Output:** (The response will be a single sentence, demonstrating the model maintained context from the previous turn.)"
      ],
      "metadata": {
        "id": "mUqgI40RQzhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we explore how to handle **multi-turn conversations** using the Converse API, where the model generates responses based on the previous messages in the conversation.\n",
        "\n",
        "#### Key Points:\n",
        "\n",
        "1. **Multi-turn Conversation Setup**:\n",
        "   - The `multi_turn_messages` list contains multiple messages, each representing an interaction between the **user** and the **assistant** (model).\n",
        "   - **First message**: A summarization prompt from the **user** asking the model to summarize the given text.\n",
        "   - **Second message**: The **assistant's** response (model's output) to the summarization prompt.\n",
        "   - **Third message**: A follow-up question from the **user**, requesting the model to shorten the summary even further.\n",
        "\n",
        "2. **Calling the Converse API**:\n",
        "   - The `bedrock.converse()` function sends the entire **multi-turn conversation** to the model specified by the **modelId**.\n",
        "   - The **`inferenceConfig`** includes parameters like **`temperature`** (controls creativity) and **`maxTokens`** (limits response length).\n",
        "\n",
        "3. **Extracting and Displaying the Response**:\n",
        "   - After receiving the response from the API, the `follow_up_response` is extracted from the returned JSON structure.\n",
        "   - The response is then displayed using the `display_response()` function, which presents it in a user-friendly format.\n",
        "\n",
        "4. **Error Handling**:\n",
        "   - If any error occurs (e.g., network issues, model errors), an exception is caught, and an error message is printed for troubleshooting.\n",
        "   - This helps ensure smooth execution of the API calls and helps identify and resolve any issues quickly.\n",
        "\n",
        "\n",
        "This example demonstrates how to **manage multi-turn conversations** with the **Converse API**, enabling more **interactive communication** with the model. It allows the model to reference previous exchanges, providing more meaningful and contextually aware responses.\n"
      ],
      "metadata": {
        "id": "Zdep8Oq8bq95"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lphqa3KrCh3X"
      },
      "source": [
        "### **2.7 Streaming Responses with ConverseStream API**\n",
        "\n",
        "For longer generations, you might want to receive the content as it's being generated. The ConverseStream API supports streaming, which allows you to process the response incrementally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsJJSpFPCh3Y"
      },
      "outputs": [],
      "source": [
        "# Example of streaming with Converse API\n",
        "\n",
        "def stream_converse(model_id, messages, inference_config=None):\n",
        "    if inference_config is None:\n",
        "        inference_config = {}  # Set default inference config if none provided\n",
        "\n",
        "    print(\"Streaming response (chunks will appear as they are received):\\n\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    full_response = \"\"  # Initialize an empty string to store the full response\n",
        "\n",
        "    try:\n",
        "        # Sending the conversation to the model and enabling streaming\n",
        "        response = bedrock.converse_stream(\n",
        "            modelId=model_id,  # Model ID to specify which model to use\n",
        "            messages=messages,  # The conversation history to be sent to the model\n",
        "            inferenceConfig=inference_config  # Inference configuration (temperature, max tokens, etc.)\n",
        "        )\n",
        "\n",
        "        # Retrieve the stream of the response from the model\n",
        "        response_stream = response.get('stream')\n",
        "        if response_stream:\n",
        "            for event in response_stream:  # Iterate through each event in the response stream\n",
        "\n",
        "                # Check for message start event and display the role (user or assistant)\n",
        "                if 'messageStart' in event:\n",
        "                    print(f\"\\nRole: {event['messageStart']['role']}\")\n",
        "\n",
        "                # If a content block delta is present, display the text content\n",
        "                if 'contentBlockDelta' in event:\n",
        "                    print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
        "\n",
        "                # Check for message stop event and display the stop reason\n",
        "                if 'messageStop' in event:\n",
        "                    print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
        "\n",
        "                # If metadata is present, display usage and latency information\n",
        "                if 'metadata' in event:\n",
        "                    metadata = event['metadata']\n",
        "                    if 'usage' in metadata:  # Display token usage information\n",
        "                        print(\"\\nToken usage\")\n",
        "                        print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
        "                        print(f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
        "                        print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
        "                    if 'metrics' in event['metadata']:  # Display latency information\n",
        "                        print(f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
        "\n",
        "            # End of the stream, display a separator\n",
        "            print(\"\\n\" + \"-\" * 80)\n",
        "\n",
        "        return full_response  # Return the full response (not updated here, as the response is printed in chunks)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during the streaming process\n",
        "        print(f\"Error in streaming: {str(e)}\")\n",
        "        return None  # Return None if an error occurs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** *(No output. The `stream_converse` function is defined in memory.)*"
      ],
      "metadata": {
        "id": "VwQF6BoBQ8LR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we explore how to **stream** responses using the **Converse API**, which allows real-time interaction with the model. Streaming enables us to receive model responses as they are generated, providing a faster and more dynamic experience.\n",
        "\n",
        "#### Key Points:\n",
        "\n",
        "1. **Function Definition**:\n",
        "   - The function `stream_converse()` is used to stream responses from a model via the Converse API. It accepts three parameters:\n",
        "     - **`model_id`**: The **ID** of the model to be used.\n",
        "     - **`messages`**: The **conversation history/messages** to be sent to the model.\n",
        "     - **`inference_config`** (optional): **Configuration options** to control the model’s behavior (e.g., temperature, max tokens).\n",
        "\n",
        "2. **Streaming the Response**:\n",
        "   - `bedrock.converse_stream()` is used to send the conversation to the model and receive the response **in real-time** as **streamed chunks**.\n",
        "   - Each chunk can contain:\n",
        "     - **Message Start**: The **role** of the speaker (either the user or assistant).\n",
        "     - **Content Block**: The **actual text** generated by the model.\n",
        "     - **Message Stop**: A **marker** indicating the end of the message.\n",
        "     - **Metadata**: Includes **token usage** (input/output tokens) and **latency** (response time).\n",
        "\n",
        "3. **Real-Time Output**:\n",
        "   - As the response is streamed, chunks of the message are printed **live**.\n",
        "   - The code tracks **token usage**, **latency**, and displays this information **in real-time** as the model generates text.\n",
        "\n",
        "4. **Error Handling**:\n",
        "   - If any error occurs during the streaming process (e.g., API connection issues), it is caught and printed for troubleshooting.\n",
        "\n",
        "5. **Purpose**:\n",
        "   - This code demonstrates how to **stream responses** from a model using the **Converse API**, allowing for **real-time interaction** with the model. It’s especially useful for tasks that require **fast feedback** or **continuous updates**, such as generating long-form text or interactive conversations.\n",
        "   \n",
        "6. **Streaming Benefits**:\n",
        "   - **Dynamic interaction** with AI models.\n",
        "   - **Real-time processing** for use cases like **interactive chatbots**, **real-time summarization**, or any application requiring **immediate feedback** from the model.\n",
        "\n",
        "\n",
        "Streaming with the Converse API provides a **dynamic way** to interact with models, making it ideal for use cases that need real-time responses and iterative communication with the model.\n"
      ],
      "metadata": {
        "id": "eJMl8Pn1cVpM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO4vCo0oCh3Y"
      },
      "outputs": [],
      "source": [
        "# Let's try streaming a longer summary\n",
        "\n",
        "# Define the streaming request, which contains the user's conversation history.\n",
        "# In this case, we are asking the model to provide a detailed summary of the text provided above.\n",
        "\n",
        "streaming_request = [\n",
        "    {\n",
        "        \"role\": \"user\",  # The 'role' indicates that the user is sending the request to the model.\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"text\": f\"\"\"Please provide a detailed summary of the following text, explaining its key points and implications:\n",
        "\n",
        "                {text_to_summarize}  # This is the actual text content that we want summarized.\n",
        "\n",
        "                Make your summary comprehensive but clear.  # Additional instructions for the model to ensure clarity and comprehensiveness.\n",
        "                \"\"\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:** *(No output. The `streaming_request` list is defined.)*"
      ],
      "metadata": {
        "id": "DAwl8vu9RH8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we modify the conversation to request a more **detailed summary** using the **Converse API**. The goal is to generate a **comprehensive yet clear summary** of the provided text, which will be streamed as it is processed.\n",
        "\n",
        "#### Key Points:\n",
        "\n",
        "1. **Streaming Request**:\n",
        "   - We define a `streaming_request` list, which contains the **user's message** with a more detailed prompt.\n",
        "   - The prompt requests the model to **summarize the text** while explaining the key points and implications, making the summary **comprehensive** but **clear**.\n",
        "\n",
        "2. **Request Format**:\n",
        "   - The **role** of the speaker is set to **\"user\"**.\n",
        "   - The **content** is a list of messages, where the **text** field contains the actual prompt sent to the model.\n",
        "\n",
        "3. **Purpose**:\n",
        "   - This request is designed to test the model’s ability to provide a **longer, more detailed summary** of the given text. It also shows how the model can handle more complex requests and deliver summaries that are **informative** yet concise.\n",
        "   \n",
        "   This setup can be useful when needing deeper insights or a more thorough analysis of the content.\n",
        "   \n",
        "\n",
        "This method demonstrates how to interact with the **Converse API** to request more **detailed summaries** and process the information in a **streaming** manner, providing a better, more dynamic user experience.\n"
      ],
      "metadata": {
        "id": "3VIUh3o4dxiI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNQaWNnhCh3Y"
      },
      "outputs": [],
      "source": [
        "# Only run this when you're ready to see streaming output\n",
        "\n",
        "# This line of code calls the 'stream_converse' function to initiate the streaming conversation with the specified model.\n",
        "\n",
        "streamed_response = stream_converse(\n",
        "    MODELS[\"Claude 3.7 Sonnet\"],  # The model being used for this conversation, \"Claude 3.7 Sonnet\" in this case.\n",
        "    streaming_request,  # The conversation request (including the detailed prompt) defined earlier.\n",
        "    inference_config={\"temperature\": 0.4, \"maxTokens\": 1000}  # Inference parameters to control creativity and output length.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Output:** (The output would appear in real-time chunks, followed by metadata upon completion. This is a simulated, completed streaming output.)"
      ],
      "metadata": {
        "id": "hxQuQf_GRVWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we execute the **streaming request** to generate a detailed summary using the **Converse API**. This part of the code will allow us to see **real-time streaming output** as the model generates the summary.\n",
        "\n",
        "#### Key Points:\n",
        "\n",
        "1. **Function Call**:\n",
        "   - `stream_converse()` is called with the following parameters:\n",
        "     - **`model_id`**: The model to use for the request, in this case, **\"Claude 3.7 Sonnet\"** from the `MODELS` dictionary.\n",
        "     - **`messages`**: The streaming request (i.e., `streaming_request`), which contains the conversation history and user prompt.\n",
        "     - **`inference_config`**: Parameters to control the response, such as **temperature** (0.4 for less creativity) and **maxTokens** (1000 to allow for a longer response).\n",
        "\n",
        "2. **Streaming Output**:\n",
        "   - The model will stream the output in real-time, and you’ll see each chunk of the response as it’s processed. The model will continue to generate content until it completes the summary or reaches the **maxTokens** limit.\n",
        "\n",
        "3. **Purpose**:\n",
        "   - This function demonstrates **real-time interaction** with the Converse API, allowing you to observe the model's output as it's being generated.\n",
        "   - By setting **temperature** to 0.4 and **maxTokens** to 1000, we're requesting a **detailed** and **coherent** summary, while ensuring that the response stays concise and focused.\n",
        "\n",
        "\n",
        "This is an effective way to handle **real-time data processing** and interact with the model in a **dynamic manner** to retrieve longer, more detailed summaries as they are generated.\n"
      ],
      "metadata": {
        "id": "IUcXo8yWgYSt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHdE0zrdCh3i"
      },
      "source": [
        "## **Conclusion**\n",
        "\n",
        "In this notebook, we explored how to effectively interact with Amazon **Bedrock** and **Converse API** to perform tasks like **text summarization** and **real-time streaming**. Here's a summary of what we've learned:\n",
        "\n",
        "1. **Text Summarization with Invoke API**:\n",
        "   - We began by demonstrating how to summarize text using the **Invoke Model API**, where we sent a single request to the model and extracted the response.\n",
        "   - We further extended this by using the **Converse API**, which simplifies multi-turn conversations and allows us to send more complex, dynamic requests like **multi-turn conversations** and **real-time streaming**.\n",
        "\n",
        "2. **Cross-Region Requests**:\n",
        "   - We explored how to call models both in the **default region** and with **cross-region inference**, allowing us to better understand the impact of latency and model availability.\n",
        "\n",
        "3. **Streaming Responses**:\n",
        "   - We integrated **real-time streaming** of model responses using the **Converse API**, which enabled us to receive and display outputs as they are generated, making interactions more dynamic.\n",
        "\n",
        "4. **Error Handling**:\n",
        "   - Throughout the examples, we implemented robust **error handling** to catch common issues like permission errors, ensuring the code runs smoothly and provides helpful troubleshooting resources when necessary.\n",
        "\n",
        "5. **Benefits of Converse API**:\n",
        "   - The **Converse API** proved to be a powerful tool for simplifying requests and responses by standardizing the format, supporting multi-turn conversations, and offering easy configuration options for controlling model behavior (like temperature, maxTokens, etc.).\n",
        "\n",
        "\n",
        "The integration of **Amazon Bedrock**, **Converse API**, and **Claude 3.7 Sonnet** offers a streamlined way to interact with advanced foundation models, enabling real-time, high-quality text generation tasks such as summarization, multi-turn conversations, and interactive feedback, ideal for use in a wide range of applications such as chatbots, content generation, and AI-driven applications.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}