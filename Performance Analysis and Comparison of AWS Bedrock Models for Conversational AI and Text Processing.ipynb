{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install requirements\n",
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "session = boto3.Session(\n",
    "    region_name=os.getenv(\"AWS_REGION\", \"us-east-1\"), profile_name=os.getenv(\"AWS_PROFILE\")\n",
    ")\n",
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    endpoint_url='https://bedrock-runtime.'+os.getenv('AWS_REGION', 'us-east-1')+'.amazonaws.com',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt = \"Hello, world\"\n",
    "\n",
    "body = json.dumps({\n",
    "    \"max_tokens\": 4096,\n",
    "    \"temperature\": 0.5,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "response = bedrock_client.invoke_model(body=body, modelId=\"anthropic.claude-3-haiku-20240307-v1:0\")\n",
    "response_body = json.loads(response.get(\"body\").read())      \n",
    "print(response_body['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "def get_llm(model_kwargs, model_id=\"anthropic.claude-3-haiku-20240307-v1:0\"):\n",
    "    \"\"\"Creates the LLM object for the langchain conversational bedrock agent.\n",
    "    Parameters:\n",
    "        model_kwargs (dict): Dictionary of model_kwargs to be passed to the Bedrock model.\n",
    "    Returns:\n",
    "        langchain.llms.bedrock.Bedrock: Bedrock model\n",
    "    \"\"\"\n",
    "    session = boto3.Session(\n",
    "        region_name=os.getenv(\"AWS_REGION\", \"us-east-1\"), profile_name=os.getenv(\"AWS_PROFILE\")\n",
    "    )\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        endpoint_url='https://bedrock-runtime.'+os.getenv('AWS_REGION', 'us-east-1')+'.amazonaws.com',\n",
    "        )\n",
    "    if (model_id == \"anthropic.claude-3-haiku-20240307-v1:0\") or (model_id == \"anthropic.claude-3-sonnet-20240229-v1:0\"):\n",
    "        llm = BedrockChat(\n",
    "            client=bedrock_client,\n",
    "            model_id=model_id,\n",
    "            model_kwargs=model_kwargs)\n",
    "    else:\n",
    "        llm = Bedrock(\n",
    "            client=bedrock_client,\n",
    "            model_id=model_id,\n",
    "            model_kwargs=model_kwargs)\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us test with 2 different models that have different APIs to access to show that Langchain can help abstract this away and always call the model the same way\n",
    "\n",
    "import time\n",
    "llm = get_llm(model_id=\"anthropic.claude-v2:1\", model_kwargs={\"max_tokens_to_sample\": 4096,\"temperature\": 0.1})\n",
    "llm_2 = get_llm(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\", model_kwargs={\"max_tokens\": 4096,\"temperature\": 0.1})\n",
    "\n",
    "# For Claude 2.1\n",
    "start_time = time.time()\n",
    "response = llm.invoke(\"Tell me about the top 3 trends in Industrial Manufacturing\")\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"Claude 2.1\\n\\n\" + response)\n",
    "print(f\"\\nExecution time: {execution_time:.2f} seconds\\n\\n\")\n",
    "\n",
    "# For Claude 3 Haiku\n",
    "start_time2 = time.time()\n",
    "response2 = llm_2.invoke(\"Tell me about the top 3 trends in Industrial Manufacturing\")\n",
    "end_time2 = time.time()\n",
    "execution_time2 = end_time2 - start_time2\n",
    "print(\"\\n\\nClaude 3 Haiku \\n\\n\" + response2.content)\n",
    "print(f\"\\nExecution time: {execution_time2:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer().fit_transform([response, response2.content])\n",
    "similarity = cosine_similarity(vectorizer[0:1], vectorizer[1:2])\n",
    "print(f\"Similarity between responses: {similarity[0][0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "sentiment_1 = TextBlob(response).sentiment\n",
    "sentiment_2 = TextBlob(response2.content).sentiment\n",
    "print(\"Claude 2.1 Sentiment:\", sentiment_1)\n",
    "print(\"Claude 3 Haiku Sentiment:\", sentiment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Explain Automation and Robotics\",\n",
    "    \"Explain Digital Transformation and Industry 4.0\",\n",
    "    \"Explain Sustainability and Environmental Responsibility\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for prompt in prompts:\n",
    "    start = time.time()\n",
    "    res = llm.invoke(prompt)\n",
    "    end = time.time()\n",
    "    results.append({\"prompt\": prompt, \"response\": res, \"execution_time\": end - start})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\n",
    "exec_times = [execution_time, execution_time2]\n",
    "\n",
    "plt.bar([\"Claude 2.1\", \"Claude 3 Haiku\"], exec_times)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Execution Time (seconds)\")\n",
    "plt.title(\"Execution Time Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_1 = len(response.split())\n",
    "word_count_2 = len(response2.content.split())\n",
    "\n",
    "plt.bar([\"Claude 2.1\", \"Claude 3 Haiku\"], [word_count_1, word_count_2])\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Word Count\")\n",
    "plt.title(\"Word Count Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textstat\n",
    "\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "readability_1 = flesch_reading_ease(response)\n",
    "readability_2 = flesch_reading_ease(response2.content)\n",
    "print(\"Readability Score for Claude 2.1:\", readability_1)\n",
    "print(\"Readability Score for Claude 3 Haiku:\", readability_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_multilingual = llm.invoke(\"Expliquez les tendances actuelles dans la fabrication industrielle.\")\n",
    "print(\"Claude 2.1 (French):\", response_multilingual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_examples = \"\"\"\n",
    "Example 1: What are the benefits of AI? -> AI enables automation and improves efficiency.\n",
    "Example 2: Explain cloud computing. -> Cloud computing is the delivery of computing services over the internet.\n",
    "Now, explain industrial manufacturing trends.\n",
    "\"\"\"\n",
    "response_few_shot = llm.invoke(prompt_with_examples)\n",
    "print(response_few_shot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_prompt = \"Write a detailed report on the impact of robotics in industrial manufacturing.\"\n",
    "long_response = llm.invoke(long_prompt)\n",
    "print(long_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
