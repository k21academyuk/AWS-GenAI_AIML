{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc40c48b-0c95-4757-a067-563cfccd51a5",
   "metadata": {
    "id": "dc40c48b-0c95-4757-a067-563cfccd51a5",
    "tags": []
   },
   "source": [
    "# Invoke FM (Amazon Titan Model)  for Generating Text (Email Responses) Using Bedrock API\n",
    "\n",
    "In this lab, you learn how to use Large Language Model (LLM) to generate an email response to a customer who provided negative feedback on the quality of customer service they received from the support engineer. In this notebook, you generate an email with a thank you note based on the customer's previous email. You use the Amazon Titan model using the **Amazon Bedrock API with Boto3 client**.\n",
    "\n",
    "The prompt used in this task is called a zero-shot prompt. In a zero-shot prompt, you describe the task or desired output to the language model in plain language. The model then uses its pre-trained knowledge and capabilities to generate a response or complete the task based solely on the provided prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a413e2-3c34-4073-9000-d8556537bb6a",
   "metadata": {
    "id": "c9a413e2-3c34-4073-9000-d8556537bb6a"
   },
   "source": [
    "#### Scenario\n",
    "You are an AI/ML Engineer at K21Technologies, and your company receives a significant amount of customer feedback, including both positive and negative responses regarding the support provided by your customer service team. One of your tasks is to generate personalized and empathetic responses to customers who have expressed dissatisfaction, aiming to address their concerns and improve customer satisfaction. Using a large language model (LLM), you need to automate the process of drafting responses based on their sentiment from previous communications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2797f9",
   "metadata": {
    "id": "8e2797f9"
   },
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "776fd083",
   "metadata": {
    "id": "776fd083",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries to work with AWS, files, and environment variables\n",
    "import json  # For working with JSON data\n",
    "import os  # For interacting with the operating system (e.g., file paths)\n",
    "import sys  # For modifying system paths\n",
    "\n",
    "# Importing Boto3 (AWS SDK for Python) and botocore (handles low-level AWS interactions)\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Adding the parent directory to the system path so we can import files from there\n",
    "module_path = \"..\"  # This points to the parent directory\n",
    "sys.path.append(os.path.abspath(module_path))  # Adds the parent directory to the search path for modules\n",
    "\n",
    "# Create a connection to Amazon Bedrock service using the Boto3 client\n",
    "# 'bedrock-runtime' specifies the service we want to connect to\n",
    "# The region_name is fetched from the environment variable AWS_DEFAULT_REGION\n",
    "bedrock_client = boto3.client(\n",
    "    'bedrock-runtime',  # Service name: Bedrock\n",
    "    region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None)  # AWS region, if available from environment settings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGyiqd5_jpXL",
   "metadata": {
    "id": "HGyiqd5_jpXL"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "#### **Importing Libraries:**\n",
    "\n",
    "- **json**: This library is used to handle **JSON data**, such as parsing or creating JSON files.\n",
    "- **os**: This module helps interact with the **operating system**, for tasks like accessing file paths or environment variables.\n",
    "- **sys**: Used for modifying **system paths**, allowing the script to access modules from different locations.\n",
    "\n",
    "#### **Boto3 and Botocore:**\n",
    "\n",
    "- **boto3**: This is the **AWS SDK for Python**, which allows you to interact with **AWS services** like Amazon Bedrock.\n",
    "- **botocore**: This library handles the **low-level interaction** with AWS services, managing tasks such as making requests and receiving responses.\n",
    "\n",
    "#### **Modifying System Path:**\n",
    "\n",
    "- The code adds the **parent directory** (`\"..\"`) to the **system path**, enabling the script to access files or modules from that directory.\n",
    "- **`sys.path.append()`** ensures that Python can find and import the necessary files from the parent folder.\n",
    "\n",
    "#### **Connecting to Amazon Bedrock:**\n",
    "\n",
    "- **`boto3.client()`**: This creates a **connection** to Amazon Bedrock using the service name `'bedrock-runtime'`.\n",
    "- **`region_name`** is fetched from the environment variable **`AWS_DEFAULT_REGION`**, which defines the region to connect to (e.g., `us-east-1`). If not set, it defaults to `None`.\n",
    "- This step **establishes communication** with the Amazon Bedrock service, allowing you to interact with its **AI models** for tasks like **text generation** or other language-related tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f634211-3de1-4390-8c3f-367af5554c39",
   "metadata": {
    "id": "4f634211-3de1-4390-8c3f-367af5554c39"
   },
   "source": [
    "## 2. Generate text\n",
    "\n",
    "In this task, you prepare an input for the Amazon Bedrock service to generate an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45ee2bae-6415-4dba-af98-a19028305c98",
   "metadata": {
    "id": "45ee2bae-6415-4dba-af98-a19028305c98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the prompt to be sent to the AI model for generating an email\n",
    "\n",
    "# This is the instruction that tells the model what to do\n",
    "\n",
    "prompt_data = \"\"\"\n",
    "Command: Write an email from Bob, Customer Service Manager, AnyCompany to the customer \"John Doe\"\n",
    "who provided negative feedback on the service provided by our customer support\n",
    "engineer\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aFMbeyGkwab",
   "metadata": {
    "id": "4aFMbeyGkwab"
   },
   "source": [
    "This code is preparing an instruction (known as a prompt) that will be sent to an AI model to generate an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8af670eb-ad02-40df-a19c-3ed835fac8d9",
   "metadata": {
    "id": "8af670eb-ad02-40df-a19c-3ed835fac8d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the data to send to the AI model in JSON format\n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data,  # The prompt we created earlier (email request)\n",
    "    \"textGenerationConfig\": {  # Settings for generating the text\n",
    "        \"maxTokenCount\": 8192,  # Max length of the response\n",
    "        \"stopSequences\": [],  # No specific stop point\n",
    "        \"temperature\": 0,  # Predictable, less random response\n",
    "        \"topP\": 0.9  # Controls how varied the response can be\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CDp763Cwl_GM",
   "metadata": {
    "id": "CDp763Cwl_GM"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This section of the code prepares the **data** to send to the AI model for generating text. The data is formatted in **JSON** format and contains the prompt and configuration settings.\n",
    "\n",
    "1. **`inputText`**:\n",
    "   - This is the **prompt** that we created earlier (the request to generate an email). It contains the task for the AI model to perform â€” in this case, writing an email in response to customer feedback.\n",
    "\n",
    "2. **`textGenerationConfig`**:\n",
    "   - This defines the **settings** for how the AI should generate the text. It includes the following parameters:\n",
    "   \n",
    "   - **`maxTokenCount`**: This sets the maximum length of the generated text, in terms of tokens. A **token** can be a word or part of a word, so setting it to **8192** allows the model to generate a reasonably long response.\n",
    "   \n",
    "   - **`stopSequences`**: This defines any specific **stop points** where the AI should halt generating text. In this case, it's left as an empty list (`[]`), meaning the AI will not stop until it reaches the **end of the prompt** or maximum token count.\n",
    "   \n",
    "   - **`temperature`**: This controls the **randomness** of the generated response. A value of `0` means the response will be **predictable** and **deterministic** (i.e., the model will choose the most likely next token). Higher values like `0.7` would make the response more **random**.\n",
    "   \n",
    "   - **`topP`**: This parameter controls how **varied** the model's response can be. A value of `0.9` means the model will consider the top **90% of probable next tokens** for generating the response. This helps the model produce more natural and creative outputs while avoiding overly repetitive or irrelevant text.\n",
    "\n",
    "### Purpose:\n",
    "The code creates a structured **request** to send to the AI model. The `inputText` provides the instructions for the task (writing the email), while the `textGenerationConfig` ensures that the model generates the response with the desired length, creativity, and predictability. This setup helps guide the AI model in generating the response according to specific requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088cf6bf-dd73-4710-a0cc-6c11d220c431",
   "metadata": {
    "id": "088cf6bf-dd73-4710-a0cc-6c11d220c431"
   },
   "source": [
    "## 3. Invoke the Amazon Titan Large language model\n",
    "\n",
    "In this task, you explore how the model generates an output based on the prompt created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379498f2",
   "metadata": {
    "id": "379498f2"
   },
   "source": [
    "### Complete Output Generation\n",
    "\n",
    "This email is generated using the Amazon Titan model by understanding the input request and utilizing its inherent understanding of different modalities. The request to the API is synchronous and waits for the entire output to be generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecaceef1-0f7f-4ae5-8007-ff7c25335251",
   "metadata": {
    "id": "ecaceef1-0f7f-4ae5-8007-ff7c25335251",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Bedrock runtime client (supported region)\n",
    "bedrock_client = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# Nova Lite model\n",
    "modelId = \"amazon.nova-lite-v1:0\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "# ðŸ”‘ Use the prompt defined in the ABOVE cell\n",
    "body = json.dumps({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"text\": prompt_data}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "try:\n",
    "    # Invoke Nova Lite\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body,\n",
    "        modelId=modelId,\n",
    "        accept=accept,\n",
    "        contentType=contentType\n",
    "    )\n",
    "\n",
    "    # Parse response\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\n",
    "        print(\n",
    "            f\"{error.response['Error']['Message']}\\n\"\n",
    "            \"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\n\"\n",
    "            \"https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\n\"\n",
    "        )\n",
    "    else:\n",
    "        raise error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9QU6ddzmbsg",
   "metadata": {
    "id": "c9QU6ddzmbsg"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This code interacts with **Amazon Bedrock** to generate text from an AI model, using the **boto3 client** to send requests and handle responses.\n",
    "\n",
    "1. **Setting Model ID and Request Headers**:\n",
    "   - **`modelId`**: Specifies the version of the AI model to use (in this case, `'amazon.nova-lite-v1:0'`).\n",
    "   - **`accept`**: Defines the format of the expected response, which is **JSON**.\n",
    "   - **`contentType`**: Specifies that the request body will be in **JSON** format.\n",
    "   - **`outputText`**: Initializes an empty string that will later store the generated response.\n",
    "\n",
    "2. **Invoking the AI Model**:\n",
    "   - **`bedrock_client.invoke_model()`** sends the request to the **Amazon Bedrock service** using the specified model ID and the provided data (`body`).\n",
    "   - The response is expected in **JSON** format, as specified by the **`accept`** and **`contentType`** headers.\n",
    "\n",
    "3. **Reading and Parsing the Response**:\n",
    "   - The response from the AI model is received in **JSON** format and is parsed using **`json.loads()`** to convert the response into a Python dictionary.\n",
    "\n",
    "4. **Extracting the Generated Text**:\n",
    "   - The **`outputText`** is extracted from the response, which is stored under the **`results`** key. The generated text is retrieved from the first result (`[0]`), specifically from the **`outputText`** field.\n",
    "\n",
    "5. **Handling Errors**:\n",
    "   - If an error occurs (such as access being denied), the **`except`** block catches the error.\n",
    "   - If the error is an **AccessDeniedException**, troubleshooting resources are printed to the console to help resolve the issue.\n",
    "   - If itâ€™s another type of error, the error is re-raised to be handled elsewhere or logged.\n",
    "\n",
    "This code prepares the request, sends it to the model, handles potential errors, and extracts the generated text to be used in the application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3748383a-c140-407f-a7f6-8f140ad57680",
   "metadata": {
    "id": "3748383a-c140-407f-a7f6-8f140ad57680",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Addressing Your Recent Feedback\n",
      "\n",
      "Dear John Doe,\n",
      "\n",
      "I hope this message finds you well.\n",
      "\n",
      "I am writing to you personally as the Customer Service Manager at AnyCompany, to address the feedback you recently provided regarding the service you experienced with our customer support engineer. I sincerely apologize for any inconvenience or frustration you may have encountered during your interaction with us.\n",
      "\n",
      "Your satisfaction is of utmost importance to us, and we are committed to providing you with the highest quality of service. It is with a heavy heart that I read your feedback, and I want to assure you that we are taking your concerns very seriously.\n",
      "\n",
      "To better understand the issues you faced and to ensure they are resolved promptly, I would appreciate it if you could provide more details about your experience. Specifically, it would be helpful to know:\n",
      "\n",
      "1. The date and time of your interaction with our customer support team.\n",
      "2. The name of the support engineer you spoke with, if possible.\n",
      "3. A summary of the issues you encountered and how they were handled.\n",
      "4. Any specific suggestions you may have for improving our service.\n",
      "\n",
      "Please feel free to reach out to me directly at [Your Email Address] or [Your Phone Number]. We are committed to resolving any issues you may have and restoring your confidence in our services.\n",
      "\n",
      "Additionally, I would like to personally follow up with you to ensure that your concerns are fully addressed. I will be in touch shortly to discuss this further and to explore ways we can improve your experience moving forward.\n",
      "\n",
      "Thank you for bringing this matter to our attention, and for being a valued customer. We truly appreciate your feedback and are dedicated to making things right.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Bob  \n",
      "Customer Service Manager  \n",
      "AnyCompany  \n",
      "[Your Contact Information]  \n",
      "[Company Website]  \n",
      "\n",
      "---\n",
      "\n",
      "Note: If you have any immediate concerns or require urgent assistance, please do not hesitate to contact our support team at [Support Team Contact Information]. We are here to help you 24/7.\n"
     ]
    }
   ],
   "source": [
    "# The relevant part of the response starts after the first newline character '\\n'.\n",
    "# Extract the email text by slicing the response string from after the first newline.\n",
    "\n",
    "email = response_body[\"output\"][\"message\"][\"content\"][0][\"text\"]  # Find the first newline and get everything after it\n",
    "print(email)  # Print the generated email\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yP4uh5lGnC-u",
   "metadata": {
    "id": "yP4uh5lGnC-u"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This section of the code extracts the **generated email** from the **AI model's response**.\n",
    "\n",
    "1. **Extracting the Relevant Text**:\n",
    "   - The **generated text** (`outputText`) includes the entire response from the model, and the **email content** starts after the first newline character (`\\n`).\n",
    "   - **`outputText.index('\\n')`** finds the index of the first **newline character** in the response.\n",
    "\n",
    "2. **Slicing the String**:\n",
    "   - **`outputText[outputText.index('\\n')+1:]`** slices the `outputText` string, starting **just after** the first newline character, effectively extracting the email content.\n",
    "\n",
    "3. **Printing the Email**:\n",
    "   - The **email text** is stored in the `email` variable and printed using `print(email)` so that you can view the generated email content.\n",
    "\n",
    "### Purpose:\n",
    "This code isolates the **email content** from the modelâ€™s response, removing the initial instructions or other text, and prints the **final email** generated by the AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d69e1a0",
   "metadata": {
    "id": "2d69e1a0"
   },
   "source": [
    "### Streaming Output Generation\n",
    "\n",
    "Bedrock also supports that the output can be streamed as it is generated by the model in form of chunks. This email is generated by invoking the model with streaming option. `invoke_model_with_response_stream` returns a `ResponseStream` which you can read from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad073290",
   "metadata": {
    "id": "ad073290",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\u001b[31m**Chunk 1**\u001b[0m\n",
      "Subject: Addressing Your Recent Experience with Our Customer Support\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well.\n",
      "\n",
      "I am Bob, the Customer Service Manager at AnyCompany, and I am writing to you in response to the feedback you recently provided regarding the service you received from our customer support engineer. First and foremost, I sincerely apologize for any inconvenience or frustration this may have caused you.\n",
      "\n",
      "At AnyCompany, we strive to provide the highest level of service and support to all our customers, and it is concerning to hear that your experience did not meet your expectations. Your feedback is incredibly valuable to us, as it helps us identify\n",
      "\n",
      "\t\t\u001b[31m**Chunk 2**\u001b[0m\n",
      " areas for improvement and take corrective actions.\n",
      "\n",
      "I would like to take this opportunity to understand more about the issues you encountered in detail. Could you please provide further information or specific instances where our service fell short of your expectations? This will help us address the problem more effectively and ensure it does not happen again in the future.\n",
      "\n",
      "Additionally, we are committed to making things right. Please let me know how we can resolve this issue to your satisfaction. If you prefer, we can arrange a call or meeting at your convenience to discuss this matter in\n",
      "\n",
      "\t\t\u001b[31m**Chunk 3**\u001b[0m\n",
      " more detail and find a suitable resolution.\n",
      "\n",
      "We greatly appreciate your patience and your willingness to provide us with this important feedback. Your satisfaction is our top priority, and we are dedicated to ensuring you have a positive experience with AnyCompany moving forward.\n",
      "\n",
      "Thank you for bringing this to our attention. I look forward to hearing from you soon.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Bob  \n",
      "Customer Service Manager  \n",
      "AnyCompany  \n",
      "[Contact Information]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the output chunks\n",
    "output = []\n",
    "chunk_texts = []  # Temporary storage for grouping text\n",
    "\n",
    "try:\n",
    "    # Invoke the model with a response stream to get results as chunks\n",
    "    response = bedrock_client.invoke_model_with_response_stream(\n",
    "        body=body,\n",
    "        modelId=modelId,\n",
    "        accept=accept,\n",
    "        contentType=contentType\n",
    "    )\n",
    "    stream = response.get('body')\n",
    "\n",
    "    chunk_number = 1  # Counter for display\n",
    "    chunk_size = 50   # Group every 50 text pieces into 1 chunk marker (gives 4-5 total chunks)\n",
    "    text_count = 0    # Count of text pieces\n",
    "    \n",
    "    if stream:\n",
    "        # Process each chunk of data from the stream\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                # Convert the chunk from bytes to a JSON object\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                \n",
    "                # NOVA LITE: Extract text from contentBlockDelta\n",
    "                if 'contentBlockDelta' in chunk_obj:\n",
    "                    delta = chunk_obj['contentBlockDelta'].get('delta', {})\n",
    "                    if 'text' in delta:\n",
    "                        text = delta['text']\n",
    "                        output.append(text)\n",
    "                        chunk_texts.append(text)\n",
    "                        text_count += 1\n",
    "                        \n",
    "                        # Print a chunk marker after every 50 text pieces\n",
    "                        if text_count % chunk_size == 0:\n",
    "                            combined_text = ''.join(chunk_texts)\n",
    "                            print(f'\\t\\t\\x1b[31m**Chunk {chunk_number}**\\x1b[0m\\n{combined_text}\\n')\n",
    "                            chunk_texts = []\n",
    "                            chunk_number += 1\n",
    "        \n",
    "        # Print any remaining text that didn't reach chunk_size\n",
    "        if chunk_texts:\n",
    "            combined_text = ''.join(chunk_texts)\n",
    "            print(f'\\t\\t\\x1b[31m**Chunk {chunk_number}**\\x1b[0m\\n{combined_text}\\n')\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Message']}\\n\"\n",
    "              \"To troubleshoot this issue, please refer to the following resources.\\n\"\n",
    "              \"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\n\"\n",
    "              \"https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "    else:\n",
    "        raise error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a788be5",
   "metadata": {
    "id": "9a788be5"
   },
   "source": [
    "The stream with response approach helps to quickly obtain the output of the model and allows the service to complete it as you read. This assists in use cases where you request the model to generate longer pieces of text. You can later combine all the chunks generated to form the complete output and use it for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xCwWSqwbnToH",
   "metadata": {
    "id": "xCwWSqwbnToH"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This code interacts with **Amazon Bedrock** using the **response stream** to process and print the model's output in **chunks**.\n",
    "\n",
    "1. **Initialize an Empty List**:\n",
    "   - **`output = []`**: Initializes an empty list to store the chunks of the generated text.\n",
    "\n",
    "2. **Invoking the Model with Response Stream**:\n",
    "   - The **`invoke_model_with_response_stream()`** method sends the **request** to the AI model and retrieves the **response stream**.\n",
    "   - The **`body`**, **`modelId`**, **`accept`**, and **`contentType`** are passed to specify the request details.\n",
    "   - **`stream = response.get('body')`** gets the body of the response, which is a stream of data chunks.\n",
    "\n",
    "3. **Processing the Stream**:\n",
    "   - **`i = 1`**: A counter to keep track of the chunks as they are processed.\n",
    "   - The `for` loop iterates over the **response stream**, and each **chunk** of data is extracted and processed one by one.\n",
    "   - **`chunk = event.get('chunk')`** retrieves the chunk from the event in the stream.\n",
    "   - **`json.loads(chunk.get('bytes').decode())`** decodes the chunk from **bytes** to a JSON object.\n",
    "   - **`text = chunk_obj['outputText']`** extracts the **generated text** (email) from the JSON object.\n",
    "   - **`output.append(text)`** stores the generated text in the `output` list.\n",
    "   - **`print(f'\\t\\t\\x1b[31m**Chunk {i}**\\x1b[0m\\n{text}\\n')`** prints the current chunk, with the chunk number (`i`) highlighted in red.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - The **`try`** block ensures the process runs, and the **`except`** block handles any **AWS access errors**.\n",
    "   - If an **AccessDeniedException** occurs, an error message with troubleshooting links is displayed.\n",
    "   - Any other errors are re-raised for further handling.\n",
    "\n",
    "### Purpose:\n",
    "This code allows **real-time processing of the AI modelâ€™s output** in **chunks**, printing each chunk of the generated text as it is received. It helps handle larger outputs efficiently and provides clear feedback for debugging or error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02d48c73",
   "metadata": {
    "id": "02d48c73",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\u001b[31m**COMPLETE OUTPUT**\u001b[0m\n",
      "\n",
      "Subject: Addressing Your Recent Experience with Our Customer Support\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well.\n",
      "\n",
      "I am Bob, the Customer Service Manager at AnyCompany, and I am writing to you in response to the feedback you recently provided regarding the service you received from our customer support engineer. First and foremost, I sincerely apologize for any inconvenience or frustration this may have caused you.\n",
      "\n",
      "At AnyCompany, we strive to provide the highest level of service and support to all our customers, and it is concerning to hear that your experience did not meet your expectations. Your feedback is incredibly valuable to us, as it helps us identify areas for improvement and take corrective actions.\n",
      "\n",
      "I would like to take this opportunity to understand more about the issues you encountered in detail. Could you please provide further information or specific instances where our service fell short of your expectations? This will help us address the problem more effectively and ensure it does not happen again in the future.\n",
      "\n",
      "Additionally, we are committed to making things right. Please let me know how we can resolve this issue to your satisfaction. If you prefer, we can arrange a call or meeting at your convenience to discuss this matter in more detail and find a suitable resolution.\n",
      "\n",
      "We greatly appreciate your patience and your willingness to provide us with this important feedback. Your satisfaction is our top priority, and we are dedicated to ensuring you have a positive experience with AnyCompany moving forward.\n",
      "\n",
      "Thank you for bringing this to our attention. I look forward to hearing from you soon.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Bob  \n",
      "Customer Service Manager  \n",
      "AnyCompany  \n",
      "[Contact Information]\n"
     ]
    }
   ],
   "source": [
    "# Combine all the chunks of text into a single string\n",
    "print('\\t\\t\\x1b[31m**COMPLETE OUTPUT**\\x1b[0m\\n')\n",
    "\n",
    "# Join all the pieces of text from the output list into one complete string\n",
    "complete_output = ''.join(output)\n",
    "\n",
    "# Print the complete generated email\n",
    "print(complete_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jDxxPza7ndaz",
   "metadata": {
    "id": "jDxxPza7ndaz"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This section of the code combines all the chunks of text that were received in the previous step and prints the **final complete output**.\n",
    "\n",
    "1. **Printing the Header**:\n",
    "   - **`print('\\t\\t\\x1b[31m**COMPLETE OUTPUT**\\x1b[0m\\n')`**: This prints a header indicating the start of the final output.\n",
    "   - The `\\x1b[31m` makes the text **red** (for emphasis) and `\\x1b[0m` resets the color after the header.\n",
    "\n",
    "2. **Combining All Chunks into One String**:\n",
    "   - **`complete_output = ''.join(output)`**: This line combines all the **chunks** of text stored in the `output` list into one **single string**. The `join()` function concatenates each chunk in the list without any spaces in between.\n",
    "   - **`output`** is a list that contains the individual chunks of text generated by the AI, and **`complete_output`** will hold the final full response.\n",
    "\n",
    "3. **Printing the Final Output**:\n",
    "   - **`print(complete_output)`**: This prints the **final generated email**, which is now a single string combining all the chunks.\n",
    "\n",
    "### Purpose:\n",
    "This part of the code combines all the text chunks received from the model into a complete, continuous string and prints the **full generated email**. It allows you to see the final output after processing the individual chunks received from the **streaming API**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b08b3b",
   "metadata": {
    "id": "64b08b3b"
   },
   "source": [
    "\n",
    "You have now experimented with using the boto3 SDK, which provides basic exposure to the Amazon Bedrock API. Using this API, you have seen the use case of generating an email to respond to a customer's negative feedback.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
