{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import sagemaker\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner\n",
    ")\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role()\n",
    "default_bucket = \"sagemaker-data113\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291e813",
   "metadata": {},
   "source": [
    "## XGBoost Hyper-paramter Tuning and Training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Training and Validation data channels from s3 buckets (saved in 'churn_data_prep.ipynb')\n",
    "s3_input_train = TrainingInput(s3_data=f\"s3://{default_bucket}/train.csv\", content_type=\"csv\")\n",
    "s3_input_validation = TrainingInput(s3_data=f\"s3://{default_bucket}/validation.csv\", content_type=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b70b49",
   "metadata": {},
   "source": [
    "Following hyper-parameters are fixed \n",
    "\n",
    "- `metric` (default value for binary classification) error rate  = #(wrong_cases)/#(total_cases) at threshold of 0.5.\n",
    "- `objective`  logistic regression for binary classification, output probability \n",
    "- `num_round` controls the number of boosting rounds. This is essentially the subsequent models that are trained using the residuals of previous iterations. Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `rate_drop` The dropout rate that specifies the fraction of previous trees to drop during the dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c685c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_hyperparameters = {\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"100\",\n",
    "    \"rate_drop\":\"0.3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", sess.boto_region_name, \"1.5-1\")\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    hyperparameters=fixed_hyperparameters,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=\"s3://{}/output\".format(default_bucket),\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af311a4",
   "metadata": {},
   "source": [
    "Following hyperparamters are varied for tuning:-\n",
    "\n",
    "- `eta` controls how aggressive each round of boosting is. Larger values lead to more conservative boosting.\n",
    "- `min_child_weight` Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. The larger the tree, the more conservative it is.\n",
    "- `alpha` L1 regularization term on weights. Increasing this value makes models more conservative.\n",
    "- `max_depth` Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e702fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = \"validation:auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator,objective_metric_name,hyperparameter_ranges,max_jobs=10,max_parallel_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({\n",
    "    \"train\":s3_input_train,\n",
    "    \"validation\":s3_input_validation\n",
    "    },include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_result = boto3.client(\"sagemaker\").describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n",
    ")\n",
    "job_count = tuning_job_result[\"TrainingJobStatusCounters\"][\"Completed\"]\n",
    "print(\"%d training jobs have completed\" %job_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22232561",
   "metadata": {},
   "source": [
    "### Fetch Tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tuner = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_result[\"HyperParameterTuningJobName\"])\n",
    "\n",
    "full_df = tuner.dataframe()\n",
    "\n",
    "objective = tuning_job_result[\"HyperParameterTuningJobConfig\"][\"HyperParameterTuningJobObjective\"]\n",
    "is_minimize = objective[\"Type\"] != \"Maximize\"\n",
    "objective_name = objective[\"MetricName\"]\n",
    "\n",
    "if len(full_df) > 0:\n",
    "    df = full_df[full_df[\"FinalObjectiveValue\"] > -float(\"inf\")]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values(\"FinalObjectiveValue\", ascending=is_minimize)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\": min(df[\"FinalObjectiveValue\"]), \"highest\": max(df[\"FinalObjectiveValue\"])})\n",
    "        pd.set_option(\"display.max_colwidth\", None)  # Don't truncate TrainingJobName\n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = tuning_job_result[\"BestTrainingJob\"][\"TunedHyperParameters\"]\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3911b",
   "metadata": {},
   "source": [
    "The scatter plot shows that the points are distributed quite apart from each other. Hence, we have set the ranges well for hyperparamter optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd1a68",
   "metadata": {},
   "source": [
    "## Register the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac4db4",
   "metadata": {},
   "source": [
    "Go to Amazon Sagemaker console. In the Training -> Hyperparameter Tuning Jobs, select the hyperparamter tuning job with the corresponding name ,initiated in this notebook (or use Creation Time column).  There, in the hyperparamter tuning job, there will be tab showing the Best Trained Model summary and a button \"Create Model\", to  register the model container. We will later use this image to create a deployment endpoint for real-time prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6896419-816e-4b5b-907b-c8de666ac962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
