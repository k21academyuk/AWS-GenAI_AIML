{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrpWHx41qSq3"
   },
   "source": [
    "# Build, Train and Deploy a Machine Learning Model using Amazon SageMaker\n",
    "\n",
    "In this lab, you will learn how to build, train, and deploy a machine learning model using Amazon SageMaker. You will use the XGBoost algorithm to predict whether a customer will enroll for a term deposit based on their demographic and behavioral data. This notebook demonstrates the complete ML workflow from data preparation to model deployment and evaluation.\n",
    "\n",
    "The dataset used is from a direct marketing campaign of a banking institution, and the goal is to predict whether a client will subscribe to a term deposit (binary classification problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n33-xIy5qSq8"
   },
   "source": [
    "#### Scenario\n",
    "You are a Machine Learning Engineer at a financial institution, and your task is to build a predictive model that can help the marketing team identify customers who are likely to subscribe to a term deposit. By automating this prediction, the bank can optimize its marketing campaigns and improve customer targeting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUqm70ZeqSq9"
   },
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "In this task, you will install the SageMaker SDK and import all necessary libraries for data manipulation, AWS services, and machine learning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vGVSxTeqSq_"
   },
   "outputs": [],
   "source": [
    "# Install the SageMaker Python SDK (specific version for compatibility)\n",
    "# The '<3.0.0' ensures we install a version below 3.0.0 to maintain compatibility\n",
    "!pip install 'sagemaker<3.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F2aaTlIqSrA"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- We installed the `sagemaker` library which provides Python APIs to interact with Amazon SageMaker services\n",
    "- The version constraint `<3.0.0` ensures compatibility with the code in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ishY2pYlqSrB"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data manipulation and AWS services\n",
    "import numpy as np  # For numerical operations and array handling\n",
    "import pandas as pd  # For data manipulation using DataFrames\n",
    "import matplotlib.pyplot as plt  # For creating visualizations\n",
    "from IPython.display import display  # For displaying outputs in Jupyter notebooks\n",
    "from time import gmtime, strftime  # For working with timestamps\n",
    "import sys  # For system-specific parameters and functions\n",
    "import math  # For mathematical operations\n",
    "import json  # For working with JSON data\n",
    "import os  # For interacting with the operating system\n",
    "\n",
    "# Importing Boto3 (AWS SDK for Python) to interact with AWS services\n",
    "import boto3,urllib.request\n",
    "\n",
    "# Importing SageMaker-specific modules\n",
    "import sagemaker  # Main SageMaker SDK\n",
    "from sagemaker.inputs import TrainingInput  # For specifying training data location\n",
    "from sagemaker.serializers import CSVSerializer  # For serializing data to CSV format for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H26YQwGqSrB"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **numpy** - we are importing this library for numerical computations and array operations\n",
    "- **pandas** - we are importing this library to work with tabular data in DataFrame format\n",
    "- **matplotlib** - we are importing this library for data visualization\n",
    "- **boto3** - we are importing the AWS SDK to interact with AWS services like S3\n",
    "- **sagemaker** - we are importing the SageMaker SDK to train and deploy ML models\n",
    "- **TrainingInput** - we are importing this class to specify the S3 location of training data\n",
    "- **CSVSerializer** - we are importing this serializer to format prediction requests in CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdrXerMDqSrC"
   },
   "source": [
    "## 2. Setup Environment and AWS Configuration\n",
    "\n",
    "In this task, you will configure the AWS environment by setting up the SageMaker execution role, S3 bucket, and container image for the XGBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3_uEYPlqSrD"
   },
   "outputs": [],
   "source": [
    "# Get the IAM role for SageMaker\n",
    "# This role provides permissions for SageMaker to access AWS resources like S3\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Get the AWS region where this notebook is running\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Get the default SageMaker S3 bucket for this session\n",
    "# This bucket will store training data, model artifacts, and outputs\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "\n",
    "# Define a prefix (folder path) in S3 to organize our files\n",
    "prefix = 'xgboost-as-a-built-in-algo'\n",
    "\n",
    "# Print the bucket name for verification\n",
    "print(f\"Using S3 bucket: {bucket_name}\")\n",
    "print(f\"IAM Role: {role}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-CebBK7qSrF"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **get_execution_role()** - we are retrieving the IAM role that SageMaker will use to access S3 and other AWS services\n",
    "- **region_name** - we are identifying the AWS region where our resources are located\n",
    "- **default_bucket()** - we are getting the default S3 bucket created by SageMaker for storing data\n",
    "- **prefix** - we are defining a folder structure in S3 to keep our project files organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFdYWYcHqSrF"
   },
   "outputs": [],
   "source": [
    "# Retrieve the container image URI for the XGBoost algorithm\n",
    "# SageMaker provides pre-built containers for popular algorithms like XGBoost\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=region,  # AWS region\n",
    "    framework='xgboost',  # The ML framework we want to use\n",
    "    version='1.0-1'  # Specific version of XGBoost\n",
    ")\n",
    "\n",
    "# Print the container URI\n",
    "print(f\"XGBoost container image: {container}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0OtwWgMqSrG"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **image_uris.retrieve()** - we are fetching the Docker container image URI for the XGBoost algorithm\n",
    "- This container has the XGBoost algorithm pre-installed and optimized for SageMaker\n",
    "- The version '1.0-1' specifies which XGBoost version we want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL6qIPZSqSrG"
   },
   "source": [
    "## 3. Load and Explore the Dataset\n",
    "\n",
    "In this task, you will load the banking dataset from a CSV file and examine its structure to understand the features and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  urllib.request.urlretrieve (\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \"bank_clean.csv\")\n",
    "  print('Success: downloaded bank_clean.csv.')\n",
    "except Exception as e:\n",
    "  print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jwm_hQH7qSrG"
   },
   "outputs": [],
   "source": [
    "# Try to load the dataset from a CSV file\n",
    "try:\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    # index_col=0 means the first column will be used as the row index\n",
    "    model_data = pd.read_csv('./bank_clean.csv', index_col=0)\n",
    "\n",
    "    # Print success message if data loads successfully\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "\n",
    "except Exception as e:\n",
    "    # If there's an error (file not found, corrupted file, etc.), print the error message\n",
    "    print('Data load error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqSPlQ-CqSrH"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **pd.read_csv()** - we are reading the CSV file and loading it into a pandas DataFrame\n",
    "- **index_col=0** - we are setting the first column as the index (row labels)\n",
    "- **try-except block** - we are using error handling to catch any issues during file loading\n",
    "- If the file loads successfully, we print a success message; otherwise, we print the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvcOTizWqSrH"
   },
   "source": [
    "## 4. Split Data into Training and Testing Sets\n",
    "\n",
    "In this task, you will split the dataset into training (70%) and testing (30%) sets. The training set is used to train the model, while the testing set is used to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBOEp6FhqSrH"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data randomly to ensure random distribution\n",
    "# frac=1 means we shuffle 100% of the data\n",
    "# random_state=1729 ensures reproducibility (same shuffle every time)\n",
    "shuffled = model_data.sample(frac=1, random_state=1729)\n",
    "\n",
    "# Calculate the split index for 70% training data\n",
    "# int() converts to integer, 0.7 means 70% of total data\n",
    "train_size = int(0.7 * len(shuffled))\n",
    "\n",
    "# Create training dataset (first 70% of shuffled data)\n",
    "# iloc[:train_size] selects rows from index 0 to train_size\n",
    "# copy() creates a separate copy to avoid modifying original data\n",
    "train_data = shuffled.iloc[:train_size].copy()\n",
    "\n",
    "# Create testing dataset (remaining 30% of shuffled data)\n",
    "# iloc[train_size:] selects rows from train_size to the end\n",
    "test_data = shuffled.iloc[train_size:].copy()\n",
    "\n",
    "# Print the shapes (rows, columns) of both datasets to verify the split\n",
    "print(train_data.shape, test_data.shape)\n",
    "\n",
    "# Display the first 5 rows of training data to verify columns and structure\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DnNJS0sqSrI"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **sample(frac=1)** - we are shuffling the entire dataset randomly to avoid any ordering bias\n",
    "- **random_state=1729** - we are setting a seed value to ensure the same shuffle occurs every time (reproducibility)\n",
    "- **train_size** - we are calculating 70% of the total data size for training\n",
    "- **iloc[:train_size]** - we are selecting the first 70% rows for training data\n",
    "- **iloc[train_size:]** - we are selecting the remaining 30% rows for testing data\n",
    "- **copy()** - we are creating independent copies to prevent unintended modifications\n",
    "- The split ensures the model is trained on 70% data and tested on unseen 30% data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJtvGQ2OqSrI"
   },
   "source": [
    "## 5. Prepare and Upload Training Data to S3\n",
    "\n",
    "In this task, you will prepare the training data in the format required by XGBoost (target column first, then features, no headers) and upload it to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seBy6G5lqSrI"
   },
   "outputs": [],
   "source": [
    "# Reformat the training data: target column first, then all features (XGBoost requirement)\n",
    "# pd.concat() is used to combine columns\n",
    "pd.concat(\n",
    "    [\n",
    "        train_data['y_yes'],  # Target column (1 if customer subscribed, 0 if not)\n",
    "        train_data.drop(['y_no', 'y_yes'], axis=1)  # All feature columns (drop both target columns)\n",
    "    ],\n",
    "    axis=1  # Concatenate along columns (horizontally)\n",
    ").to_csv('train.csv', index=False, header=False)  # Save as CSV without index and headers\n",
    "\n",
    "# Upload the training CSV file to S3\n",
    "# boto3.Session().resource('s3') creates an S3 resource client\n",
    "# .Bucket(bucket_name) selects the specific S3 bucket\n",
    "# .Object() specifies the file path in S3\n",
    "# .upload_file() uploads the local file to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(\n",
    "    os.path.join(prefix, 'train/train.csv')  # S3 path: xgboost-as-a-built-in-algo/train/train.csv\n",
    ").upload_file('train.csv')  # Local file to upload\n",
    "\n",
    "# Create a TrainingInput object that points to the S3 location of training data\n",
    "# This object will be used by SageMaker to access the training data\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data=f's3://{bucket_name}/{prefix}/train',  # S3 URI where training data is stored\n",
    "    content_type='text/csv'  # Specify that the data is in CSV format\n",
    ")\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Training data uploaded and TrainingInput created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAuPIf7vqSrK"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **pd.concat()** - we are combining the target column ('y_yes') and feature columns into a single DataFrame\n",
    "- **drop(['y_no', 'y_yes'])** - we are removing both target columns from features (XGBoost only needs one target column)\n",
    "- **to_csv(index=False, header=False)** - we are saving the data without row indices and column headers (XGBoost format requirement)\n",
    "- **boto3 S3 upload** - we are uploading the CSV file to the specified S3 bucket and path\n",
    "- **TrainingInput()** - we are creating a SageMaker object that references the S3 location of training data\n",
    "- The data is now stored in S3 and ready to be used by the SageMaker training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMNuiPCmqSrL"
   },
   "source": [
    "## 6. Configure and Train the XGBoost Model\n",
    "\n",
    "In this task, you will create a SageMaker Estimator, configure the XGBoost hyperparameters, and start the training job. The training process will take approximately 5-8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYmmwLUBqSrL"
   },
   "outputs": [],
   "source": [
    "# Create a SageMaker session to manage interactions with SageMaker services\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# Create an Estimator object for the XGBoost algorithm\n",
    "# An Estimator encapsulates the training configuration\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,  # Docker image URI for XGBoost (retrieved earlier)\n",
    "    role=role,  # IAM role that grants SageMaker permissions to access AWS resources\n",
    "    instance_count=1,  # Number of training instances (1 for single-instance training)\n",
    "    instance_type='ml.m4.xlarge',  # EC2 instance type for training (4 vCPUs, 16 GB RAM)\n",
    "    output_path=f's3://{bucket_name}/{prefix}/output',  # S3 path where trained model will be saved\n",
    "    sagemaker_session=sess  # SageMaker session object\n",
    ")\n",
    "\n",
    "# Set hyperparameters for the XGBoost algorithm\n",
    "# Hyperparameters control how the model learns from data\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5,  # Maximum depth of each tree (controls model complexity)\n",
    "    eta=0.2,  # Learning rate (step size shrinkage to prevent overfitting)\n",
    "    gamma=4,  # Minimum loss reduction required to make a split (regularization)\n",
    "    min_child_weight=6,  # Minimum sum of instance weight needed in a child (regularization)\n",
    "    subsample=0.8,  # Fraction of samples used for training each tree (prevents overfitting)\n",
    "    objective='binary:logistic',  # Loss function for binary classification\n",
    "    num_round=100  # Number of boosting rounds (trees to build)\n",
    ")\n",
    "\n",
    "# Start the training job\n",
    "# This will launch an EC2 instance, load the data from S3, train the model, and save it back to S3\n",
    "# The training will take approximately 5-8 minutes\n",
    "xgb.fit({'train': s3_input_train})  # Pass the training data location\n",
    "\n",
    "print(\"Training job completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRwmyaAgqSrL"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **sagemaker.Session()** - we are creating a session to interact with SageMaker services\n",
    "- **sagemaker.estimator.Estimator()** - we are defining the training job configuration (container, role, instance type, etc.)\n",
    "- **instance_count=1** - we are using a single training instance (can be increased for distributed training)\n",
    "- **instance_type='ml.m4.xlarge'** - we are specifying the EC2 instance type with 4 vCPUs and 16 GB RAM\n",
    "- **output_path** - we are specifying where the trained model artifacts will be saved in S3\n",
    "- **set_hyperparameters()** - we are configuring the XGBoost algorithm parameters:\n",
    "  - **max_depth=5** - limits tree depth to prevent overfitting\n",
    "  - **eta=0.2** - controls learning rate (smaller values = slower but more robust learning)\n",
    "  - **gamma=4** - minimum loss reduction to create a new split (higher = more conservative)\n",
    "  - **min_child_weight=6** - prevents overly specific splits\n",
    "  - **subsample=0.8** - uses 80% of data for each tree (random sampling)\n",
    "  - **objective='binary:logistic'** - optimizes for binary classification\n",
    "  - **num_round=100** - builds 100 decision trees\n",
    "- **fit()** - we are starting the training job by passing the training data location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx3mOquhqSrM"
   },
   "source": [
    "## 7. Deploy the Trained Model to an Endpoint\n",
    "\n",
    "In this task, you will deploy the trained model to a SageMaker endpoint, which allows you to make real-time predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9fiAoqtqSrM"
   },
   "outputs": [],
   "source": [
    "# Deploy the trained model to a SageMaker endpoint\n",
    "# This creates a real-time inference endpoint that can accept prediction requests\n",
    "xgb_predictor = xgb.deploy(\n",
    "    initial_instance_count=1,  #Number of instances for the endpoint (1 for single instance)\n",
    "    instance_type='ml.m4.xlarge'  #EC2 instance type for hosting the model\n",
    ")\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Model deployed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbbTpZhKqSrM"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **deploy()** - we are deploying the trained model to a real-time endpoint\n",
    "- **initial_instance_count=1** - we are using 1 instance to host the model (can be scaled for high traffic)\n",
    "- **instance_type='ml.m4.xlarge'** - we are specifying the instance type for the endpoint\n",
    "- **xgb_predictor** - we are creating a predictor object that we'll use to make predictions\n",
    "- The endpoint is now live and ready to accept prediction requests in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml-toKH8qSrM"
   },
   "source": [
    "## 8. Make Predictions on Test Data\n",
    "\n",
    "In this task, you will use the deployed model endpoint to make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b72ApsQuqSrN"
   },
   "outputs": [],
   "source": [
    "# Prepare the test data for prediction\n",
    "# Remove the target columns ('y_no', 'y_yes') and convert to numpy array\n",
    "# The model only needs feature columns, not the target\n",
    "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values\n",
    "\n",
    "# Set the serializer for the predictor to CSV format\n",
    "# This tells the endpoint to expect data in CSV format\n",
    "xgb_predictor.serializer = CSVSerializer()\n",
    "\n",
    "# Make predictions using the deployed endpoint\n",
    "# The endpoint returns predictions as a byte string\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8')\n",
    "\n",
    "# Convert the newline-separated prediction scores to a numpy array\n",
    "# fromstring() parses the string and creates an array\n",
    "# sep='\\n' indicates that values are separated by newlines\n",
    "predictions_array = np.fromstring(predictions, sep='\\n')\n",
    "\n",
    "# Print the shape of predictions array to verify\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K_Ct0kmqSrN"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **drop(['y_no', 'y_yes'])** - we are removing the target columns from the test data\n",
    "- **.values** - we are converting the DataFrame to a numpy array (required for prediction)\n",
    "- **CSVSerializer()** - we are setting the data format for sending requests to the endpoint\n",
    "- **predict()** - we are sending the test data to the endpoint and receiving predictions\n",
    "- **.decode('utf-8')** - we are converting the byte response to a readable string\n",
    "- **np.fromstring()** - we are parsing the newline-separated string into a numpy array of prediction scores\n",
    "- Each value in predictions_array represents the probability that a customer will subscribe (0 to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etEHb2wEqSrN"
   },
   "source": [
    "## 9. Evaluate Model Performance with Confusion Matrix\n",
    "\n",
    "In this task, you will create a confusion matrix to evaluate the model's performance by comparing actual vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------\n",
    "# Confusion Matrix (SAFE VERSION)\n",
    "# -------------------------------\n",
    "\n",
    "# Assuming test_data['y_yes'] are the true labels and predictions_array contains the predicted labels\n",
    "cm = pd.crosstab(\n",
    "    index=test_data['y_yes'],                 # Actual values (0 = No Purchase, 1 = Purchase)\n",
    "    columns=np.round(predictions_array),      # Predicted values (rounded to 0 or 1)\n",
    "    rownames=['Observed'],\n",
    "    colnames=['Predicted']\n",
    ")\n",
    "\n",
    "# Force matrix to be 2x2 (prevents IndexError)\n",
    "cm = cm.reindex(index=[0, 1], columns=[0, 1], fill_value=0)\n",
    "\n",
    "# -------------------------------\n",
    "# Extract Values (Correctly)\n",
    "# -------------------------------\n",
    "\n",
    "tn = cm.loc[0, 0]   # True Negatives\n",
    "fp = cm.loc[0, 1]   # False Positives\n",
    "fn = cm.loc[1, 0]   # False Negatives\n",
    "tp = cm.loc[1, 1]   # True Positives\n",
    "\n",
    "# -------------------------------\n",
    "# Accuracy\n",
    "# -------------------------------\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn) * 100\n",
    "\n",
    "# -------------------------------\n",
    "# Formatted Output\n",
    "# -------------------------------\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "print(f\"Overall Classification Rate: {accuracy:.2f}%\\n\")\n",
    "\n",
    "# Display confusion matrix in a more readable format\n",
    "print(\"{:<15}{:>10}{:>10}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"Observed\")\n",
    "\n",
    "print(\"{:<15}{:>10}{:>10}\".format(\n",
    "    \"No Purchase\",\n",
    "    f\"{tn} ({(tn/(tn+fp)*100 if (tn+fp)>0 else 0):.1f}%)\",\n",
    "    f\"{fp}\"\n",
    "))\n",
    "\n",
    "print(\"{:<15}{:>10}{:>10}\".format(\n",
    "    \"Purchase\",\n",
    "    f\"{fn}\",\n",
    "    f\"{tp} ({(tp/(tp+fp)*100 if (tp+fp)>0 else 0):.1f}%)\"\n",
    "))\n",
    "\n",
    "# -------------------------------\n",
    "# Visual Representation\n",
    "# -------------------------------\n",
    "\n",
    "# Create a heatmap for better visualization of confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Purchase', 'Purchase'], yticklabels=['No Purchase', 'Purchase'])\n",
    "plt.title('Confusion Matrix Visualization')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Observed')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr7fHiyzqSrO"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **pd.crosstab()** - we are creating a confusion matrix that compares actual vs predicted values\n",
    "- **np.round(predictions_array)** - we are rounding prediction probabilities to 0 or 1 for classification\n",
    "- **tn (True Negatives)** - we are counting correctly predicted 'No Purchase' cases\n",
    "- **fp (False Positives)** - we are counting incorrectly predicted 'Purchase' cases\n",
    "- **fn (False Negatives)** - we are counting missed 'Purchase' cases (predicted as 'No Purchase')\n",
    "- **tp (True Positives)** - we are counting correctly predicted 'Purchase' cases\n",
    "- **Accuracy** - we are calculating the percentage of correct predictions: (TP + TN) / Total\n",
    "- The confusion matrix helps us understand where the model is making errors and its overall performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnP_h-pPqSrO"
   },
   "source": [
    "## 10. Clean Up Resources\n",
    "\n",
    "In this task, you will delete the deployed endpoint, model, and S3 objects to avoid incurring unnecessary costs. **Important:** Always clean up resources after completing your work to prevent ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "endpoint_name = xgb_predictor.endpoint_name\n",
    "\n",
    "print(\"Starting SageMaker cleanup...\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# FIND ENDPOINT CONFIG NAME\n",
    "# -------------------------------\n",
    "try:\n",
    "    endpoint_desc = sm_client.describe_endpoint(\n",
    "        EndpointName=endpoint_name\n",
    "    )\n",
    "    endpoint_config_name = endpoint_desc[\"EndpointConfigName\"]\n",
    "except ClientError as e:\n",
    "    print(f\"‚ö†Ô∏è Endpoint not found: {endpoint_name}\")\n",
    "    endpoint_config_name = None\n",
    "\n",
    "# -------------------------------\n",
    "# FIND MODEL NAME\n",
    "# -------------------------------\n",
    "model_name = None\n",
    "if endpoint_config_name:\n",
    "    try:\n",
    "        endpoint_config = sm_client.describe_endpoint_config(\n",
    "            EndpointConfigName=endpoint_config_name\n",
    "        )\n",
    "        model_name = endpoint_config[\"ProductionVariants\"][0][\"ModelName\"]\n",
    "    except ClientError:\n",
    "        pass\n",
    "\n",
    "# -------------------------------\n",
    "# DELETE ENDPOINT\n",
    "# -------------------------------\n",
    "try:\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"‚úÖ Endpoint deleted: {endpoint_name}\")\n",
    "except ClientError as e:\n",
    "    print(f\"‚ö†Ô∏è Endpoint already deleted: {endpoint_name}\")\n",
    "\n",
    "# -------------------------------\n",
    "# DELETE ENDPOINT CONFIG\n",
    "# -------------------------------\n",
    "if endpoint_config_name:\n",
    "    try:\n",
    "        sm_client.delete_endpoint_config(\n",
    "            EndpointConfigName=endpoint_config_name\n",
    "        )\n",
    "        print(f\"‚úÖ Endpoint config deleted: {endpoint_config_name}\")\n",
    "    except ClientError:\n",
    "        print(f\"‚ö†Ô∏è Endpoint config already deleted: {endpoint_config_name}\")\n",
    "\n",
    "# -------------------------------\n",
    "# DELETE MODEL\n",
    "# -------------------------------\n",
    "if model_name:\n",
    "    try:\n",
    "        sm_client.delete_model(ModelName=model_name)\n",
    "        print(f\"‚úÖ Model deleted: {model_name}\")\n",
    "    except ClientError:\n",
    "        print(f\"‚ö†Ô∏è Model already deleted: {model_name}\")\n",
    "\n",
    "print(\"\\nüéØ SageMaker cleanup completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyyMx4jFqSrP"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **delete_model()** - we are deleting the SageMaker model resource\n",
    "- This removes the model configuration from SageMaker (but not the trained model file in S3)\n",
    "- The model artifacts (.tar.gz file) remain in S3 for future use\n",
    "- This is a cleanup step to remove unused resources from your SageMaker account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F24PLYOlqSrP"
   },
   "outputs": [],
   "source": [
    "# Delete all files from the S3 bucket prefix (optional cleanup)\n",
    "# This removes training data and model artifacts to free up S3 storage\n",
    "# Create an S3 resource client\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Get the bucket object\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# Delete all objects with our prefix (xgboost-as-a-built-in-algo/)\n",
    "# This removes train.csv, model artifacts, and output files\n",
    "bucket.objects.filter(Prefix=prefix).delete()\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"All objects with prefix '{prefix}' deleted from S3 bucket '{bucket_name}'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL8ZWFmdqSrP"
   },
   "source": [
    "**What we did in the above cell:**\n",
    "- **boto3.resource('s3')** - we are creating an S3 resource client to interact with S3 buckets\n",
    "- **s3.Bucket(bucket_name)** - we are accessing the specific S3 bucket that stored our data\n",
    "- **bucket.objects.filter(Prefix=prefix)** - we are selecting all objects that start with our prefix 'xgboost-as-a-built-in-algo/'\n",
    "- **.delete()** - we are deleting all those objects (training data, model artifacts, output files)\n",
    "- This cleanup step removes all project files from S3 to avoid storage costs\n",
    "- **Note:** This is optional - you may want to keep model artifacts for future redeployment"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
