{"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"pycharm":{"name":"#%%\n"},"id":"yIxGGdEFc6pM"},"source":["# <h1> **Invoke FM (Amazon Bedrock) for Text Generation using API** </h1>\n"]},{"cell_type":"markdown","metadata":{"id":"_HkCgEE2c6pP"},"source":["<h2> Overview </h2>\n","\n","This notebook demonstrates how to get started with Amazon Bedrock. We will show you how to query different models from the Bedrock API call and how prompt engineering can help improving the results of your use case."]},{"cell_type":"markdown","metadata":{"id":"m-mHVpmzc6pP"},"source":["<h2> Context </h2>\n","\n","\n","Amazon Bedrock simplifies the process of building and scaling generative AI applications by providing access to high-performing foundation models (FMs) from leading AI companies through a single API.\n","\n","**Real-Time Scenario:**\n","\n","\n","You work in customer service for a software company. A customer has provided negative feedback about delays in receiving a callback and incorrect troubleshooting advice. The company wants to automate email responses to such feedback using Amazon Bedrock’s Foundation Model (FM) to ensure timely, professional, and empathetic communication.\n","\n","\n","**Description:**\n","\n","In this lab, you'll use Amazon Bedrock's Foundation Model (FM) to automate email responses to negative customer feedback. The scenario involves a customer unhappy with delays and incorrect advice from support. The AI will generate a professional email that apologizes, acknowledges the issue, reassures the customer, and outlines steps for improvement. This approach streamlines customer support, ensuring timely and empathetic responses to maintain customer satisfaction.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TdXO7zvvc6pP"},"source":["Amazon Bedrock supports foundation models (FMs) from the following providers. For the updated list of FMs and respective documentation, see [Supported foundation models in Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html)\n","\n","To use a foundation model with the Amazon Bedrock API, you'll need its model ID. For a list for model IDs, see [Amazon Bedrock model IDs](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html).\n"]},{"cell_type":"markdown","metadata":{"id":"LPnYflMGc6pQ"},"source":["<h2>Prerequisites</h2>\n","\n","Before you can use Amazon Bedrock, you must carry out the following steps:\n","\n","- Sign up for an AWS account (if you don't already have one) and IAM Role with the necessary permissions for Amazon Bedrock, see [AWS Account and IAM Role](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#new-to-aws).\n","- Request access to the foundation models (FM) that you want to use, see [Request access to FMs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access).\n","    \n","    We have used below Foundation Models in our examples in this Notebook in `us-east-1` (N. Virginia) region.\n","    \n","| Provider Name | Foundation Model Name | Model Id |\n","| ------- | ------------- | ------------- |\n","| Amazon | Nova Lite | amazon.nova-lite-v1:0 |\n","| Anthropic | Claude 3.5 Sonnet  | anthropic.claude-3-5-sonnet-20240620-v1:0 |\n","| Meta | Llama 3 8B Instruct | meta.llama3-8b-instruct-v1:0 |\n","| Mistral AI | Mixtral 8X7B Instruct | mistral.mixtral-8x7b-instruct-v0:1 |\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"id":"jXSJTpXmc6pQ"},"source":["<h2>Setup</h2>\n","\n","⚠️ This notebook should work well with the Data Science 3.0 kernel (Python 3.10 runtime) in SageMaker Studio ⚠️\n","\n","Run the cells in this section to install the packages needed by this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"cL1L_jkic6pQ"},"outputs":[],"source":["%pip install --no-build-isolation --force-reinstall \\\n","    \"boto3>=1.28.57\" \\\n","    \"awscli>=1.29.57\" \\\n","    \"botocore>=1.31.57\""]},{"cell_type":"markdown","metadata":{"id":"9tvXsj4hc6pR"},"source":["<h2>Noteboo/Code with comments</h2>\n","\n","<h3>Create the boto3 client</h3>\n","\n","Interaction with the Bedrock API is done via the AWS SDK. We will be using AWS SDK for Python: [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) for this notebook.\n","\n","You can refer [Amazon Bedrock API references](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html#sdk) for each SDK.\n","\n","<h3>Use different clients</h3>\n","\n","- `bedrock` – Contains control plane APIs for managing, training, and deploying models. For more information, see [Amazon Bedrock Actions](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock.html) and [Amazon Bedrock Data Types](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Types_Amazon_Bedrock.html).\n","- `bedrock-runtime` – Contains data plane APIs for making inference requests for models hosted in Amazon Bedrock. For more information, see [Amazon Bedrock Runtime Actions](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Runtime.html) and [Amazon Bedrock Runtime Data Types](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Types_Amazon_Bedrock_Runtime.html).\n","\n","\n","In case of boto3, Control pane APIs such as [ListFoundationModels](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html), are supported by Amazon Bedrock client and data plane APIs such as [`InvokeModel`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) and [`InvokeModelWithResponseStream`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html) are supported by Amazon Bedrock Runtime client.\n","\n","The `get_bedrock_client()` method accepts `runtime` (default=True) parameter to return either `bedrock` or `bedrock-runtime` client.\n","\n","<h3>Use the default credential chain</h3>\n","\n","If you are running this notebook from [Amazon Sagemaker Studio](https://aws.amazon.com/sagemaker/studio/) and your Sagemaker Studio [execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) has permissions to access Bedrock, then you can just run the cells below as-is. This is also the case if you are running these notebooks from a computer whose default AWS credentials have access to Bedrock.\n","\n","<h3>Use a different AWS Region</h3>\n","\n","If you're running this notebook from your own computer or a SageMaker notebook in a different AWS Region from where Bedrock is set up, you can un-comment the `os.environ['AWS_DEFAULT_REGION']` line below and specify the region to use.\n","\n","<h3>Use a specific profile</h3>\n","\n","In case you're running this notebook from your own computer where you have setup the AWS CLI with multiple profiles, and the profile which has access to Bedrock is not the default one, you can un-comment the `os.environ['AWS_PROFILE']` line below and specify the profile to use.\n","\n","<h3>Use a different role</h3>\n","\n","In case you or your company has setup a specific, separate [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) to access Bedrock, you can specify it by un-commenting the `os.environ['BEDROCK_ASSUME_ROLE']` line below. Ensure that your current user or role have permissions to [assume](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html) such role.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"_PT4oW0Kc6pR"},"outputs":[],"source":["\"\"\"Helper utilities for working with Amazon Bedrock from Python notebooks\"\"\"\n","# Python Built-Ins:\n","import os\n","from typing import Optional\n","import sys\n","import json\n","\n","# External Dependencies:\n","import boto3\n","from botocore.config import Config\n","import botocore\n","\n","def get_bedrock_client(\n","    assumed_role: Optional[str] = None,\n","    region: Optional[str] = None,\n","    runtime: Optional[bool] = True,\n","):\n","    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n","\n","    Parameters\n","    ----------\n","    assumed_role :\n","        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n","        specified, the current active credentials will be used.\n","    region :\n","        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-2\").\n","        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n","    runtime :\n","        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n","    \"\"\"\n","    if region is None:\n","        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n","    else:\n","        target_region = region\n","\n","    print(f\"Create new client\\n  Using region: {target_region}\")\n","    session_kwargs = {\"region_name\": target_region}\n","    client_kwargs = {**session_kwargs}\n","\n","    profile_name = os.environ.get(\"AWS_PROFILE\")\n","    if profile_name:\n","        print(f\"  Using profile: {profile_name}\")\n","        session_kwargs[\"profile_name\"] = profile_name\n","\n","    retry_config = Config(\n","        region_name=target_region,\n","        retries={\n","            \"max_attempts\": 10,\n","            \"mode\": \"standard\",\n","        },\n","    )\n","    session = boto3.Session(**session_kwargs)\n","\n","    if assumed_role:\n","        print(f\"  Using role: {assumed_role}\", end='')\n","        sts = session.client(\"sts\")\n","        response = sts.assume_role(\n","            RoleArn=str(assumed_role),\n","            RoleSessionName=\"langchain-llm-1\"\n","        )\n","        print(\" ... successful!\")\n","        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n","        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n","        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n","\n","    if runtime:\n","        service_name='bedrock-runtime'\n","    else:\n","        service_name='bedrock'\n","\n","    bedrock_client = session.client(\n","        service_name=service_name,\n","        config=retry_config,\n","        **client_kwargs\n","    )\n","\n","    print(\"boto3 Bedrock client successfully created!\")\n","    print(bedrock_client._endpoint)\n","    return bedrock_client"]},{"cell_type":"markdown","source":["### Explanation to above code.\n","\n","Introduction to the `get_bedrock_client` Function\n","\n","The `get_bedrock_client` function simplifies connecting to Amazon Bedrock by creating a **boto3 client**, which lets you interact with AI models in the service. It can automatically handle the AWS region, assume an IAM role for temporary access, and set up retry logic to ensure smooth communication. This function is useful for automating tasks like text generation or summarization using Amazon Bedrock's foundation models.\n","\n","- **Region Configuration**: Uses the AWS region set in the environment or provided in the code.\n","- **IAM Role**: Allows assuming a role for specific permissions.\n","- **Retry Logic**: Ensures reliable client communication with retries in case of errors.\n","- **Client Creation**: Sets up a Bedrock client for AI tasks like text generation.\n","\n","This function provides a quick and flexible way to interact with Bedrock’s models, enabling AI-based solutions.\n"],"metadata":{"id":"5oVSg5RQ0xSK"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"vwnqpVyJc6pS"},"outputs":[],"source":["module_path = \"..\"\n","sys.path.append(os.path.abspath(module_path))\n","\n","\n","# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n","\n","os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n","# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n","# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n","\n","\n","boto3_bedrock = get_bedrock_client(\n","    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n","    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n","    runtime=False\n",")"]},{"cell_type":"markdown","source":["### Explanation to Above code\n","\n","This code configures environment variables to connect to AWS and creates a **boto3** client for Amazon Bedrock:\n","\n","1. **AWS Region**: The region (`AWS_DEFAULT_REGION`) is set to `\"us-east-1\"`, which can be changed to your desired region.\n","2. **AWS Profile**: If using a specific AWS profile, uncomment and set `AWS_PROFILE`.\n","3. **IAM Role**: If assuming an IAM role for permissions, uncomment and set `BEDROCK_ASSUME_ROLE` to the role ARN.\n","4. **Client Creation**: The `get_bedrock_client` function is called to create a client using the provided region, profile, and IAM role, with `runtime=False` for batch operations.\n","\n","This ensures your environment is ready for interacting with Amazon Bedrock based on your AWS setup.\n"],"metadata":{"id":"D0jHM72r0-uI"}},{"cell_type":"markdown","metadata":{"id":"KlGmERnCc6pS"},"source":["<h4>Validate the connection</h4>\n","\n","We can check the client works by trying out the `list_foundation_models()` method, which will tell us all the models available for us to use"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"3NslIxfmc6pS"},"outputs":[],"source":["boto3_bedrock.list_foundation_models()"]},{"cell_type":"markdown","metadata":{"id":"ySbBPVwpc6pS"},"source":["<h3>`InvokeModel` body and output</h3>\n","\n","The `invoke_model()` method of the Amazon Bedrock runtime client (`InvokeModel` API) will be the primary method we use for most of our Text Generation and Processing tasks - whichever model we're using.\n","\n","Although the method is shared, the format of input and output varies depending on the foundation model used, see [Inference parameters for foundation models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html)"]},{"cell_type":"markdown","metadata":{"id":"cPUk48eic6pS"},"source":["## Common inference parameter definitions\n","\n","<h4> Randomness and Diversity </h4>\n","\n","Foundation models support the following parameters to control randomness and diversity in the\n","response.\n","\n","**Temperature** – Large language models use probability to construct the words in a sequence. For any\n","given next word, there is a probability distribution of options for the next word in the sequence. When\n","you set the temperature closer to zero, the model tends to select the higher-probability words. When\n","you set the temperature further away from zero, the model may select a lower-probability word.\n","\n","In technical terms, the temperature modulates the probability density function for the next tokens,\n","implementing the temperature sampling technique. This parameter can deepen or flatten the density\n","function curve. A lower value results in a steeper curve with more deterministic responses, and a higher\n","value results in a flatter curve with more random responses.\n","\n","**Top K** – Temperature defines the probability distribution of potential words, and Top K defines the cut\n","off where the model no longer selects the words. For example, if K=50, the model selects from 50 of the\n","most probable words that could be next in a given sequence. This reduces the probability that an unusual\n","word gets selected next in a sequence.\n","In technical terms, Top K is the number of the highest-probability vocabulary tokens to keep for Top-\n","K-filtering - This limits the distribution of probable tokens, so the model chooses one of the highest-\n","probability tokens.\n","\n","**Top P** – Top P defines a cut off based on the sum of probabilities of the potential choices. If you set Top\n","P below 1.0, the model considers the most probable options and ignores less probable ones. Top P is\n","similar to Top K, but instead of capping the number of choices, it caps choices based on the sum of their\n","probabilities.\n","For the example prompt \"I hear the hoof beats of ,\" you may want the model to provide \"horses,\"\n","\"zebras\" or \"unicorns\" as the next word. If you set the temperature to its maximum, without capping\n","Top K or Top P, you increase the probability of getting unusual results such as \"unicorns.\" If you set the\n","temperature to 0, you increase the probability of \"horses.\" If you set a high temperature and set Top K or\n","Top P to the maximum, you increase the probability of \"horses\" or \"zebras,\" and decrease the probability\n","of \"unicorns.\"\n","\n","<h4> Length </h4>\n","\n","The following parameters control the length of the generated response.\n","\n","**Response length** – Configures the minimum and maximum number of tokens to use in the generated\n","response.\n","\n","**Length penalty** – Length penalty optimizes the model to be more concise in its output by penalizing\n","longer responses. Length penalty differs from response length as the response length is a hard cut off for\n","the minimum or maximum response length.\n","\n","In technical terms, the length penalty penalizes the model exponentially for lengthy responses. 0.0\n","means no penalty. Set a value less than 0.0 for the model to generate longer sequences, or set a value\n","greater than 0.0 for the model to produce shorter sequences.\n","\n","<h4> Repetitions </h4>\n","\n","The following parameters help control repetition in the generated response.\n","\n","**Repetition penalty (presence penalty)** – Prevents repetitions of the same words (tokens) in responses.\n","1.0 means no penalty. Greater than 1.0 decreases repetition."]},{"cell_type":"markdown","metadata":{"id":"Lsrvh02nc6pS"},"source":["\n","## **Try out the models**\n","\n","With some theory out of the way, let's see the models in action! Run the cells below to see basic, synchronous example invocations for each model:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"wb089hwXc6pT"},"outputs":[],"source":["bedrock_runtime = get_bedrock_client(\n","    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n","    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",")\n","\n","def invoke_model(body, model_id, accept, content_type):\n","    \"\"\"\n","    Invokes Amazon bedrock model to run an inference\n","    using the input provided in the request body.\n","\n","    Args:\n","        body (dict): The invokation body to send to bedrock\n","        model_id (str): the model to query\n","        accept (str): input accept type\n","        content_type (str): content type\n","    Returns:\n","        Inference response from the model.\n","    \"\"\"\n","\n","    try:\n","        response = bedrock_runtime.invoke_model(\n","            body=json.dumps(body),\n","            modelId=model_id,\n","            accept=accept,\n","            contentType=content_type\n","        )\n","\n","        return response\n","\n","    except Exception as e:\n","        print(f\"Couldn't invoke {model_id}\")\n","        raise e"]},{"cell_type":"markdown","source":["### Explanation to code\n","\n","This code sets up an Amazon Bedrock client and defines a function to invoke a model for inference. Here's a breakdown:\n","\n","1. **Creating Bedrock Client**:\n","   The `get_bedrock_client` function is called to create a client for interacting with Amazon Bedrock (`bedrock_runtime`). It retrieves the **IAM role** (`BEDROCK_ASSUME_ROLE`) and **region** (`AWS_DEFAULT_REGION`) from environment variables for configuration.\n","\n","2. **`invoke_model` Function**:\n","   - This function allows you to run inference using a specific Amazon Bedrock model by sending a request body.\n","   - **Arguments**:\n","     - `body`: A dictionary containing the request data for the model.\n","     - `model_id`: The ID of the Bedrock model to be queried.\n","     - `accept`: The response type you want to receive (e.g., `application/json`).\n","     - `content_type`: The content type for the request body (e.g., `application/json`).\n","   - **Process**: The function sends the request to the model using `bedrock_runtime.invoke_model()` and returns the inference response.\n","   - **Error Handling**: If the invocation fails, an error message is printed, and the exception is raised.\n","\n","This setup allows you to easily interact with Bedrock models for real-time inference.\n"],"metadata":{"id":"uhL69ErW2YEK"}},{"cell_type":"markdown","metadata":{"id":"ULp6iCJCc6pT"},"source":["### **Amazon Nova**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"aOBKYjf8c6pT"},"outputs":[],"source":["# If you'd like to try your own prompt, edit this parameter!\n","prompt_data = \"\"\"Command: Write me a blog about making strong business decisions as a leader.\n","\n","Blog:\n","\"\"\"\n","\n","# Define one or more messages using the \"user\" and \"assistant\" roles.\n","message_list = [{\"role\": \"user\", \"content\": [{\"text\": prompt_data}]}]\n","\n","# Configure the inference parameters.\n","inf_params = {\"max_new_tokens\": 250, \"top_p\": 0.9, \"top_k\": 20, \"temperature\": 0.7}\n","\n","body = {\n","    \"schemaVersion\": \"messages-v1\",\n","    \"messages\": message_list,\n","    \"inferenceConfig\": inf_params,\n","}\n","\n","modelId = \"amazon.nova-lite-v1:0\"\n","accept = \"application/json\"\n","contentType = \"application/json\"\n","\n","response = invoke_model(body, modelId, accept, contentType)\n","response_body = json.loads(response.get(\"body\").read())\n","\n","print(response_body.get(\"output\").get(\"message\").get(\"content\")[0].get(\"text\"))"]},{"cell_type":"markdown","source":["### Code Explanation\n","\n","This code sends a custom text-generation request to an Amazon Bedrock model **(Amazon Nova)**.  \n","\n","1. **Prompt Setup**: A text prompt (`prompt_data`) is defined asking the model to write a blog about strong business decisions.  \n","2. **Message Formatting**: The prompt is wrapped in a message list and formatted according to Bedrock’s API schema.  \n","3. **Inference Configuration**: Parameters like `max_new_tokens`, `temperature`, `top_p`, and `top_k` control the creativity and length of the model’s response.  \n","4. **Model Invocation**: The `invoke_model()` function sends the request to the specified Bedrock model (`amazon.nova-lite-v1:0`) and retrieves the response.  \n","5. **Output Display**: The model’s generated text (the blog) is extracted from the response and printed.\n","\n","In short, this code demonstrates how to use Amazon Bedrock to generate creative text based on a custom prompt.\n"],"metadata":{"id":"kbeKUVSLMlI7"}},{"cell_type":"markdown","metadata":{"id":"72CgoSrEc6pT"},"source":["### **Anthropic Claude**"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"XLMlKKayc6pT"},"outputs":[],"source":["# If you'd like to try your own prompt, edit this parameter!\n","prompt_data = \"\"\"Human: Write me a blog about making strong business decisions as a leader.\n","\n","Assistant:\n","\"\"\"\n","\n","messages = [{\"role\": \"user\", \"content\": prompt_data}]\n","\n","body={\n","        \"anthropic_version\": \"bedrock-2023-05-31\",\n","        \"max_tokens\": 250,\n","        \"messages\": messages\n","    }\n","\n","modelId = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"  # change this to use a different version from the model provider\n","accept = \"application/json\"\n","contentType = \"application/json\"\n","\n","response = invoke_model(body, modelId, accept, contentType)\n","response_body = json.loads(response.get(\"body\").read())\n","\n","print(response_body.get(\"content\")[0].get(\"text\"))"]},{"cell_type":"markdown","source":["### Code Explnantion\n","\n","**Generating Text with Claude Model on Amazon Bedrock**\n","\n","This code sends a custom prompt to the **Claude 3.5 Sonnet model** on Amazon Bedrock for text generation.\n","\n","1. **Prompt Setup**: The `prompt_data` variable contains the instruction for the model — here, to write a blog about strong business decisions.\n","2. **Message Formatting**: The prompt is structured in the format expected by the Anthropic Claude API using the `messages` list.\n","3. **Request Configuration**: The request body defines parameters like `max_tokens` to limit the response length and specifies the model version (`anthropic.claude-3-5-sonnet-20240620-v1:0`).\n","4. **Model Invocation**: The `invoke_model()` function sends the request to the model, and the response is parsed using `json.loads()` to extract the generated text.\n","\n","This code demonstrates how to use **Amazon Bedrock** with Anthropic’s Claude model to generate creative text responses based on a given prompt.\n"],"metadata":{"id":"8USbcYxxN222"}},{"cell_type":"markdown","metadata":{"id":"14rhsH0Yc6pT"},"source":["### **Meta Llama**"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"QvSwnjI0c6pT"},"outputs":[],"source":["# If you'd like to try your own prompt, edit this parameter!\n","prompt_data = \"\"\"<s>[INST] <<SYS>>\n","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n","If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n","<</SYS>>\n","\n","Write me a blog about making strong business decisions as a leader. [/INST]\"\"\"\n","\n","body = {\n","    \"prompt\": prompt_data,\n","    \"temperature\": 0.5,\n","    \"top_p\": 0.9,\n","    \"max_gen_len\": 512,\n","}\n","\n","modelId = \"meta.llama3-8b-instruct-v1:0\"\n","accept = \"application/json\"\n","contentType = \"application/json\"\n","\n","response = invoke_model(body, modelId, accept, contentType)\n","response_body = json.loads(response.get(\"body\").read())\n","\n","print(response_body[\"generation\"])"]},{"cell_type":"markdown","source":["### Code Explanation\n","\n","**Generating Text with LLaMA 3 Model on Amazon Bedrock**\n","\n","This code demonstrates how to use Meta’s **LLaMA 3** model through Amazon Bedrock to generate text.  \n","\n","1. **Prompt Setup**: The `prompt_data` includes system instructions ensuring the model responds safely and ethically, followed by a request to write a blog about strong business decisions.  \n","2. **Request Body**: Parameters like `temperature`, `top_p`, and `max_gen_len` control creativity, randomness, and output length.  \n","3. **Model Invocation**: The `invoke_model()` function sends the prompt to the `meta.llama3-8b-instruct-v1:0` model, which processes the request and generates text.  \n","4. **Output**: The response is parsed, and the generated text (blog content) is printed.\n","\n","In short, this code uses **Amazon Bedrock** to generate well-structured, responsible AI text using the LLaMA 3 model.\n"],"metadata":{"id":"eiHvMmGGOu5y"}},{"cell_type":"markdown","metadata":{"id":"J69grEq-c6pT"},"source":["### **Mistral Models**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTKQOU-Xc6pT"},"outputs":[],"source":["# If you'd like to try your own prompt, edit this parameter!\n","prompt_data = \"\"\"<s>[INST] <<SYS>>\n","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n","If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n","<</SYS>>\n","\n","Write me a blog about making strong business decisions as a leader. [/INST]\"\"\"\n","\n","body = {\n","    \"prompt\": prompt_data,\n","    'max_tokens': 500,\n","\t'top_p': 0.9,\n","\t'temperature': 0.2\n","}\n","\n","modelId = 'mistral.mixtral-8x7b-instruct-v0:1'\n","accept = 'application/json'\n","contentType = 'application/json'\n","\n","response = invoke_model(body, modelId, accept, contentType)\n","\n","response_body = json.loads(response.get(\"body\").read())\n","print(response_body.get('outputs')[0].get('text'))"]},{"cell_type":"markdown","source":["### Code Explnation\n","\n","**Generating Text with Mixtral Model on Amazon Bedrock**\n","\n","This code uses the **Mistral Mixtral 8x7B Instruct model** via Amazon Bedrock to generate AI-written content.\n","\n","1. **Prompt Setup**: The `prompt_data` defines system instructions for ethical and safe responses, followed by a request to write a blog about strong business decisions.  \n","2. **Request Configuration**: Parameters like `max_tokens`, `top_p`, and `temperature` control the length, diversity, and creativity of the output.  \n","3. **Model Invocation**: The `invoke_model()` function sends the prompt to the selected model (`mistral.mixtral-8x7b-instruct-v0:1`) for inference.  \n","4. **Output**: The response is parsed, and the generated text is extracted and printed.\n","\n","In short, this code demonstrates how to generate safe, structured text using Mistral’s Mixtral model on Amazon Bedrock.\n"],"metadata":{"id":"w7tDjZtERKwX"}},{"cell_type":"markdown","metadata":{"id":"u-lH0TRRc6pU"},"source":["## **Generate streaming output**\n","\n","For large language models, it can take noticeable time to generate long output sequences. Rather than waiting for the entire response to be available, latency-sensitive applications may like to **stream** the response to users.\n","\n","Run the code below to see how you can achieve this with Bedrock's `invoke_model_with_response_stream()` method - returning the response body in separate chunks."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"BsFSsgQdc6pU"},"outputs":[],"source":["from IPython.display import clear_output, display, display_markdown, Markdown\n","from datetime import datetime\n","\n","# If you'd like to try your own prompt, edit this parameter!\n","prompt_data = \"\"\"Command: Write me a blog about making strong business decisions as a leader.\n","\n","Blog:\n","\"\"\"\n","\n","# Define one or more messages using the \"user\" and \"assistant\" roles.\n","message_list = [{\"role\": \"user\", \"content\": [{\"text\": prompt_data}]}]\n","\n","# Configure the inference parameters.\n","inf_params = {\"max_new_tokens\": 250, \"top_p\": 0.9, \"top_k\": 20, \"temperature\": 0.7}\n","\n","body = json.dumps({\n","    \"schemaVersion\": \"messages-v1\",\n","    \"messages\": message_list,\n","    \"inferenceConfig\": inf_params,\n","})\n","\n","modelId = \"amazon.nova-lite-v1:0\"  # (Change this, and the request body, to try different models)\n","accept = \"application/json\"\n","contentType = \"application/json\"\n","\n","start_time = datetime.now()\n","\n","response = bedrock_runtime.invoke_model_with_response_stream(\n","    body=body, modelId=modelId, accept=accept, contentType=contentType\n",")\n","chunk_count = 0\n","time_to_first_token = None\n","\n","# Process the response stream\n","stream = response.get(\"body\")\n","if stream:\n","    for event in stream:\n","        chunk = event.get(\"chunk\")\n","        if chunk:\n","            # Print the response chunk\n","            chunk_json = json.loads(chunk.get(\"bytes\").decode())\n","            # Pretty print JSON\n","            # print(json.dumps(chunk_json, indent=2, ensure_ascii=False))\n","            content_block_delta = chunk_json.get(\"contentBlockDelta\")\n","            if content_block_delta:\n","                if time_to_first_token is None:\n","                    time_to_first_token = datetime.now() - start_time\n","                    print(f\"Time to first token: {time_to_first_token}\")\n","\n","                chunk_count += 1\n","                current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n","                # print(f\"{current_time} - \", end=\"\")\n","                print(content_block_delta.get(\"delta\").get(\"text\"), end=\"\")\n","    print(f\"Total chunks: {chunk_count}\")\n","else:\n","    print(\"No response stream received.\")"]},{"cell_type":"markdown","source":["### Code Explanation\n","\n","**Streaming Text Generation with Amazon Nova Model**\n","\n","This code demonstrates how to generate text using the **Amazon Nova Lite** model on Amazon Bedrock with real-time streaming output.\n","\n","1. **Prompt Setup**: The `prompt_data` defines the instruction for the model — here, to write a blog about strong business decisions.  \n","2. **Inference Configuration**: Parameters like `max_new_tokens`, `top_p`, `top_k`, and `temperature` control creativity and output length.  \n","3. **Model Invocation**: The code uses `invoke_model_with_response_stream()` to receive the model’s response in real time rather than waiting for the entire output.  \n","4. **Streaming Response**: The response is processed chunk by chunk, printing the generated text as it arrives and calculating the **time to first token** for performance insights.  \n","5. **Output**: The generated text appears progressively, simulating a live generation experience.\n","\n","In short, this code showcases how to use **Amazon Bedrock’s streaming API** to generate AI content efficiently and view results instantly as the model responds.\n"],"metadata":{"id":"Z9a53ax0ReCl"}},{"cell_type":"markdown","metadata":{"id":"WXhblpYKc6pU"},"source":["## **Prompt Engineering**\n","\n","Prompt engineering is the practice of optimizing the quality and performance of your foundation model's response to your request. Prompt engineering may involve:\n","\n","    Word choice\n","    Phrasing\n","    Providing examples (few-shot learning)\n","    Use of line breaks and content separators\n","    Following established formats that align with how the model was trained\n","    Use of stop sequences to help the model know when it should stop generating text\n","\n","**Communicating clearly**\n","\n","The art of prompt engineering is the art of communication. Large language models have been trained on a massive amount of written and transcribed human content. So just like when communicating with people, it's critical to communicate clearly with the models. Throughout these labs, you will see examples of varying levels of detail and clarity."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"NZQVC7Ojc6pU"},"outputs":[],"source":["prompt_data = \"\"\"Human: Write an email from Bob, Customer Service Manager,\n","to the customer \"John Doe\" that provided negative feedback on the service\n","provided by our customer support engineer.\n","\n","Assistant:\n","\"\"\"\n","\n","messages = [{\"role\": \"user\", \"content\": prompt_data}]\n","\n","body={\n","        \"anthropic_version\": \"bedrock-2023-05-31\",\n","        \"max_tokens\": 500,\n","        \"messages\": messages\n","    }\n","\n","modelId = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"  # change this to use a different version from the model provider\n","accept = \"application/json\"\n","contentType = \"application/json\"\n","\n","response = invoke_model(body, modelId, accept, contentType)\n","response_body = json.loads(response.get(\"body\").read())\n","\n","print(response_body.get(\"content\")[0].get(\"text\"))"]},{"cell_type":"markdown","source":["### Code Explanation\n","\n","**Generating an Email Response with Claude Model on Amazon Bedrock**\n","\n","This code uses **Anthropic’s Claude 3.5 Sonnet model** via Amazon Bedrock to generate a professional email response.\n","\n","1. **Prompt Setup**: The `prompt_data` instructs the model to write an email from a customer service manager to a customer who gave negative feedback.  \n","2. **Message Formatting**: The prompt is structured in the required format for Claude’s API using the `messages` list.  \n","3. **Request Configuration**: The `body` defines model settings such as `max_tokens` (length of the response) and the model version (`anthropic.claude-3-5-sonnet-20240620-v1:0`).  \n","4. **Model Invocation**: The `invoke_model()` function sends the request to Claude via Bedrock, and the response is parsed from JSON.  \n","5. **Output**: The generated email text is printed, showing how Claude responds to customer scenarios with empathy and professionalism.\n","\n","In short, this code demonstrates how to use **Amazon Bedrock** and **Claude** to automate professional email writing with generative AI.\n"],"metadata":{"id":"1k0VsnJyS8Zr"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"4AEdTdsDc6pU"},"outputs":[],"source":["prompt_data = \"\"\"Human: Write an email from Bob, Customer Service Manager,\n","to the customer \"John Doe\" that provided negative feedback on the service\n","provided by our customer support engineer. Here is the feedback provided.\n","<customer_feedback>\n","Hello Bob,\n","     I am very disappointed with the recent experience I had when I called your customer support and spoke with Anna Bhasin.\n","     I was expecting an immediate call back but it took three days for us to get a call back.\n","     The first suggestion to fix the problem was incorrect. Ultimately the problem was fixed after three days.\n","     We are very unhappy with the response provided and may consider taking our business elsewhere.\n","</customer_feedback>\n","\n","Assistant:\n","\"\"\"\n","\n","messages = [{\"role\": \"user\", \"content\": prompt_data}]\n","\n","body={\n","        \"anthropic_version\": \"bedrock-2023-05-31\",\n","        \"max_tokens\": 500,\n","        \"messages\": messages\n","    }\n","\n","modelId = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"  # change this to use a different version from the model provider\n","accept = \"application/json\"\n","contentType = \"application/json\"\n","\n","response = invoke_model(body, modelId, accept, contentType)\n","response_body = json.loads(response.get(\"body\").read())\n","\n","print(response_body.get(\"content\")[0].get(\"text\"))"]},{"cell_type":"markdown","source":["### Code Explnanation\n","\n","**Generating a Professional Email with Claude Model on Amazon Bedrock**\n","\n","This code uses **Anthropic’s Claude 3.5 Sonnet model** on Amazon Bedrock to generate a professional and empathetic response email based on customer feedback.\n","\n","1. **Prompt Setup**: The `prompt_data` includes both instructions and the customer’s feedback, guiding the model to craft a polite and professional reply.  \n","2. **Message Formatting**: The `messages` list defines the user role and content for the model.  \n","3. **Request Configuration**: The `body` specifies model parameters like `max_tokens` (length of response) and the Claude model version used.  \n","4. **Model Invocation**: The `invoke_model()` function sends the request to Claude via Bedrock and retrieves the generated email text.  \n","5. **Output**: The response is extracted and printed, showing the model-generated email written in a formal, customer-centric tone.\n","\n","In short, this code demonstrates how to use **Amazon Bedrock** with **Claude** to automatically generate professional responses to customer feedback.\n"],"metadata":{"id":"JlcFpfdgTCnj"}},{"cell_type":"markdown","metadata":{"id":"LnQW6LR5c6pV"},"source":["<h2>Clean up</h2>\n","\n","Only delete the notebook instance on your AWS acccount, it does not require any cleanup or additional deletion of resources."]}],"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96},{"_defaultOrder":57,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.trn1.2xlarge","vcpuNum":8},{"_defaultOrder":58,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1.32xlarge","vcpuNum":128},{"_defaultOrder":59,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1n.32xlarge","vcpuNum":128}],"instance_type":"ml.t3.medium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}