{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc40c48b-0c95-4757-a067-563cfccd51a5",
      "metadata": {
        "tags": [],
        "id": "dc40c48b-0c95-4757-a067-563cfccd51a5"
      },
      "source": [
        "# **Invoke Bedrock model through API for code generation***\n",
        "\n",
        "This lab focuses on using Amazon Bedrock to generate **high-quality code** through API-based interaction with **large language models** (LLMs). Ideal for developers and AI/ML engineers, this hands-on project demonstrates how to send zero-shot prompts to a Bedrock model and receive **clean, structured code outputs**.\n",
        "\n",
        "You will learn how to build a Python workflow using the **Boto3 Bedrock client**, define task-specific prompts, and process model responses to automate code creation. This lab showcases how powerful LLMs can understand instructions and generate accurate code without requiring any example inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Scenario**\n",
        "\n",
        "As an **AI Engineer at K21 Technologies**, your team is building an internal automation tool that generates clean, structured code snippets using **Amazon Bedrock**. You are responsible for creating an **API-based workflow** where developers can send prompts to a Bedrock foundation model and receive **high-quality, production-ready code** in return.\n",
        "\n",
        "Your task is to design and test a Python script that invokes a Bedrock model through API calls, handles the response, and ensures that the generated code meets internal coding standards for readability and accuracy.\n"
      ],
      "metadata": {
        "id": "pWAZjMwMpJuT"
      },
      "id": "pWAZjMwMpJuT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Description**\n",
        "\n",
        "In this hands-on lab, you will learn how to invoke an Amazon Bedrock model using API calls to generate code programmatically. The lab walks you through creating a Python script that sends a prompt to a **Bedrock foundation model** (such as Amazon Titan or a third-party LLM) and retrieves AI-generated code.\n",
        "\n",
        "You will understand how the API request is structured, how to manage parameters like **temperature and max tokens**, and how to process model responses.\n",
        "\n",
        "By the end, you will know how to integrate code generation into **applications, workflows, and automation systems using Amazon Bedrock**."
      ],
      "metadata": {
        "id": "scG9csAPpN5V"
      },
      "id": "scG9csAPpN5V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "### **Amazon Bedrock**\n",
        "\n",
        "Amazon Bedrock is a fully managed service that provides access to high-performance foundation models (FMs) from top AI providers such as **Anthropic, AI21 Labs, Cohere, Meta, Stability AI, and Amazon** — all through a unified API.\n",
        "\n",
        "It simplifies the development of generative AI applications by offering **secure, private, and scalable** access to LLMs without the need to manage infrastructure.\n",
        "\n",
        "Using Amazon Bedrock, developers can experiment with various LLMs, generate text or code, customize models using techniques like **fine-tuning or retrieval-augmented generation (RAG)**, and integrate AI capabilities into business workflows seamlessly.\n",
        "\n",
        "Because it is serverless, Bedrock enables you to focus solely on building applications while AWS automatically manages scaling, performance, and security."
      ],
      "metadata": {
        "id": "5Pu-YpeZpd3j"
      },
      "id": "5Pu-YpeZpd3j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Objectives of the Lab**\n",
        "\n",
        "- Learn how to invoke a Bedrock foundation model using **Python and the Boto3 SDK**.\n",
        "\n",
        "- Understand how to design a **zero-shot prompt** that instructs the LLM to generate high-quality code.\n",
        "\n",
        "- Explore how the Bedrock API processes **instructions, tasks, and input parameters** to produce code outputs.\n",
        "\n",
        "- Implement a workflow to **send prompts, receive responses, and extract the generated code** programmatically.\n",
        "\n",
        "- Demonstrate how **LLMs** can perform code generation without requiring example-based training or additional context."
      ],
      "metadata": {
        "id": "3xxsrjjXqUHr"
      },
      "id": "3xxsrjjXqUHr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Overview & Key Concepts**\n",
        "\n",
        "In this section, we explore the core ideas needed to understand API-based code generation using Amazon Bedrock:\n",
        "\n",
        "####**1. Large Language Models (LLMs)**\n",
        "\n",
        "LLMs are advanced AI models trained on massive datasets of text and code. They can understand **instructions, generate code, answer questions, and perform complex reasoning tasks**.\n",
        "\n",
        "####**2. Zero-Shot Prompting**\n",
        "\n",
        "Zero-shot prompting refers to providing only a **natural-language instruction** without any example input or output.\n",
        "The model uses its learned knowledge to produce the answer directly.\n",
        "\n",
        "####**3. Bedrock API & Boto3 Client**\n",
        "\n",
        "The Bedrock API allows applications to communicate with foundation models.\n",
        "Using the **Boto3 Bedrock client**, you can send text prompts, specify generation parameters, and retrieve structured responses from the model.\n",
        "\n",
        "####**4. Code Generation Task**\n",
        "\n",
        "In this lab, the Bedrock model receives a task such as **“generate a Python function to sort a list”** and returns high-quality code.\n",
        "The model interprets the instruction, understands the problem, and writes syntactically correct code.\n",
        "\n",
        "####**5. Model Parameters (Temperature, Max Tokens, etc.)**\n",
        "\n",
        "These settings control creativity and output length:\n",
        "\n",
        "- Temperature → randomness of the output\n",
        "- Top-P → probability distribution filtering\n",
        "- Max Tokens → maximum output length\n",
        "- Proper tuning improves code quality.\n",
        "\n",
        "####**6. Response Parsing**\n",
        "\n",
        "Models return **structured JSON**. You will learn how to extract the generated code from the response and display or store it."
      ],
      "metadata": {
        "id": "gpjmnpRAqZE2"
      },
      "id": "gpjmnpRAqZE2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prerequisites**\n",
        "\n",
        "**1. AWS Account (Paid Version):**\n",
        "\n",
        "You will need an active paid-tier AWS account.\n",
        "If you don’t have one, follow the **Activity Guide: Create AWS Account [Here](//https://www.skool.com/k21academy/classroom/d1c70428?md=18c29ea269d0408aad49d9f710329ef8)**\n",
        "\n",
        "2. **Nurtured AWS Account:**\n",
        "\n",
        "Make sure your AWS account is nurtured and ready for AIML labs.\n",
        "\n",
        "**Reference link:** https://www.skool.com/k21academy/classroom/d1c70428?md=fdf3083b947e442c931382225c7af88e\n",
        "\n",
        "3. **Amazon Bedrock Enabled in Your Region:**\n",
        "Ensure that Amazon Bedrock is available and activated in the AWS Region you are using commonly ```us-east-1```.\n",
        "\n",
        "4. **Basic Knowledge of Python:**\n",
        "You should be familiar with writing and running Python code, especially in Jupyter notebooks.\n",
        "\n",
        "5. **AWS CLI Installed & Configured:**\n",
        "Your local environment or notebook must have AWS CLI configured with proper IAM permissions.\n",
        "\n",
        "6. **IAM Role / User Permissions:**\n",
        "Ensure your IAM user or role has the required permissions for:\n",
        "\n",
        "- bedrock:InvokeModel\n",
        "\n",
        "- bedrock:InvokeModelWithResponseStream\n",
        "\n",
        "- bedrock:ListFoundationModels\n",
        "\n",
        "7. **Python Environment Ready:**\n",
        "Make sure you have:\n",
        "\n",
        "- Python 3.9+\n",
        "\n",
        "- Boto3 installed\n",
        "\n",
        "- Jupyter Notebook or AWS SageMaker Studio (optional but recommended)"
      ],
      "metadata": {
        "id": "9zQsmGJ7rIUG"
      },
      "id": "9zQsmGJ7rIUG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 5.1: Environment setup**\n",
        "\n",
        "In this task, you configure the required environment to work with Amazon Bedrock inside your notebook. This includes **installing libraries, importing dependencies, and setting up your AWS credentials** so that you can interact with Bedrock securely and smoothly.\n",
        "\n",
        "####**Explanation of the Code (What this step does)**\n",
        "\n",
        "In this step, you prepare the Python environment so it can communicate with Amazon Bedrock. The code imports the required libraries, configures the system path, and creates a Bedrock Runtime client using the **Boto3 SDK**. This setup ensures that your notebook has all the tools needed to interact with the Bedrock API.\n",
        "\n",
        "This step includes:\n",
        "\n",
        "- Importing essential Python modules\n",
        "\n",
        "- Importing **Boto3** (AWS SDK for Python)\n",
        "\n",
        "- Adjusting the module path if needed\n",
        "\n",
        "- Creating the **Bedrock Runtime client** used to invoke models\n",
        "\n",
        "####**Why this step is needed**\n",
        "\n",
        "You must set up the environment before calling any Bedrock model. This setup establishes the AWS connection and prepares the client that will send prompts for code generation.\n",
        "\n",
        "This step is important because:\n",
        "\n",
        "- Without the **Boto3 client**, you cannot invoke any Bedrock model\n",
        "\n",
        "- AWS region and environment settings must be properly configured\n",
        "\n",
        "- It ensures the notebook can securely access **AWS services**\n",
        "\n",
        "- It prepares the foundation for all upcoming **API calls**"
      ],
      "metadata": {
        "id": "gk15j9hBtMAP"
      },
      "id": "gk15j9hBtMAP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "776fd083",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "collapsed": true,
        "id": "776fd083",
        "outputId": "fd0850c9-7cfb-48da-fed8-b8df44e9a150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.41.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting botocore<1.42.0,>=1.41.2 (from boto3)\n",
            "  Downloading botocore-1.41.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.16.0,>=0.15.0 (from boto3)\n",
            "  Downloading s3transfer-0.15.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.42.0,>=1.41.2->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.42.0,>=1.41.2->boto3) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.42.0,>=1.41.2->boto3) (1.17.0)\n",
            "Downloading boto3-1.41.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.41.2-py3-none-any.whl (14.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.15.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.41.2 botocore-1.41.2 jmespath-1.0.1 s3transfer-0.15.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'boto3' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4043597931.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mbedrock_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bedrock-runtime'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregion_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AWS_DEFAULT_REGION\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'boto3' is not defined"
          ]
        }
      ],
      "source": [
        "#create a service client by name using the default session.\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "!pip install boto3\n",
        "\n",
        "module_path = \"..\"\n",
        "sys.path.append(os.path.abspath(module_path))\n",
        "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f634211-3de1-4390-8c3f-367af5554c39",
      "metadata": {
        "id": "4f634211-3de1-4390-8c3f-367af5554c39"
      },
      "source": [
        "## **Task 5.2: Code Generation**\n",
        "\n",
        "In this task, you prepare the input dataset that the **Amazon Bedrock model** will use to generate a Python program for your use case. Before invoking the model, you create a sample dataset ```sales.csv``` that will be referenced in your code generation prompt.\n",
        "\n",
        "####**Explanation of the Code (What this step does)**\n",
        "\n",
        "This code creates a small sample sales dataset and saves it as ```sales.csv``` in your notebook environment. The file contains basic sales information such as **date, product ID, price, and units sold**. This dataset will later be used by the model to generate Python code for analysis or reporting.\n",
        "\n",
        "The code accomplishes the following:\n",
        "\n",
        "- Imports the ```csv``` module required to write CSV files\n",
        "\n",
        "- Prepares **sample sales data** in a list format\n",
        "\n",
        "- Writes the data into a CSV file named ```sales.csv```\n",
        "\n",
        "- Confirms that the file has been successfully created\n",
        "\n",
        "####**Why this step is needed**\n",
        "\n",
        "Before generating any code, the model needs context—in this case, a dataset it can work with. Creating this CSV file ensures that:\n",
        "\n",
        "- You have a **consistent dataset** for testing code generation\n",
        "\n",
        "- The generated Python program can reference a real file\n",
        "\n",
        "- The model can produce **accurate and relevant code** based on actual data\n",
        "\n",
        "- You simulate a **realistic business scenario** where AI writes code for analytics\n",
        "\n",
        "This step sets the foundation for the upcoming prompts where Amazon Bedrock will generate Python code to process and analyze your sales data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a0ad24",
      "metadata": {
        "id": "89a0ad24"
      },
      "outputs": [],
      "source": [
        "# create sales.csv file\n",
        "import csv\n",
        "\n",
        "data = [\n",
        "    [\"date\", \"product_id\", \"price\", \"units_sold\"],\n",
        "    [\"2023-01-01\", \"P001\", 50, 20],\n",
        "    [\"2023-01-02\", \"P002\", 60, 15],\n",
        "    [\"2023-01-03\", \"P001\", 50, 18],\n",
        "    [\"2023-01-04\", \"P003\", 70, 30],\n",
        "    [\"2023-01-05\", \"P001\", 50, 25],\n",
        "    [\"2023-01-06\", \"P002\", 60, 22],\n",
        "    [\"2023-01-07\", \"P003\", 70, 24],\n",
        "    [\"2023-01-08\", \"P001\", 50, 28],\n",
        "    [\"2023-01-09\", \"P002\", 60, 17],\n",
        "    [\"2023-01-10\", \"P003\", 70, 29],\n",
        "    [\"2023-02-11\", \"P001\", 50, 23],\n",
        "    [\"2023-02-12\", \"P002\", 60, 19],\n",
        "    [\"2023-02-13\", \"P001\", 50, 21],\n",
        "    [\"2023-02-14\", \"P003\", 70, 31],\n",
        "    [\"2023-03-15\", \"P001\", 50, 26],\n",
        "    [\"2023-03-16\", \"P002\", 60, 20],\n",
        "    [\"2023-03-17\", \"P003\", 70, 33],\n",
        "    [\"2023-04-18\", \"P001\", 50, 27],\n",
        "    [\"2023-04-19\", \"P002\", 60, 18],\n",
        "    [\"2023-04-20\", \"P003\", 70, 32],\n",
        "    [\"2023-04-21\", \"P001\", 50, 22],\n",
        "    [\"2023-04-22\", \"P002\", 60, 16],\n",
        "    [\"2023-04-23\", \"P003\", 70, 34],\n",
        "    [\"2023-05-24\", \"P001\", 50, 24],\n",
        "    [\"2023-05-25\", \"P002\", 60, 21]\n",
        "]\n",
        "\n",
        "# Write data to sales.csv\n",
        "with open('sales.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(\"sales.csv has been created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68e8af6",
      "metadata": {
        "id": "d68e8af6"
      },
      "source": [
        "## **Task 5.3: Analyzing sales with Amazon Bedrock generated Python program**\n",
        "\n",
        "In this task, you create the prompt structure and prepare the input that will be sent to the **Amazon Bedrock model**. This prompt tells the model to generate a Python program that analyzes the ```sales.csv``` dataset.\n",
        "\n",
        "####**Code Cell 1 — Define Prompt Template**\n",
        "####**What this step does:**\n",
        "\n",
        "- Creates a **reusable function** to format prompts properly.\n",
        "\n",
        "- Ensures the prompt follows the **Bedrock model’s expected structure**.\n",
        "\n",
        "- Helps the model clearly understand who is speaking (user or assistant).\n",
        "\n",
        "- Prevents **formatting issues** that can break model output.\n",
        "\n",
        "####**Why this step?**\n",
        "\n",
        "This step is needed because Amazon Bedrock models expect text in a **specific structured format** with headers like ```<|start_header_id|>``` and ```<|end_header_id|>```.\n",
        "\n",
        "If you send an unstructured prompt:\n",
        "\n",
        "- the model may misunderstand the instruction,\n",
        "\n",
        "- generate incomplete code,\n",
        "\n",
        "- or fail to follow your instructions correctly.\n",
        "\n",
        "By defining a prompt template:\n",
        "\n",
        "- You make the instructions **clear and consistent**.\n",
        "\n",
        "- You ensure the model receives your request in the exact format it was trained to interpret.\n",
        "\n",
        "- You avoid prompt formatting mistakes that commonly lead to incorrect or low-quality code generation.\n",
        "\n",
        "This template is the foundation that ensures the LLM understands your request accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2422c099-58ee-4b15-be5e-cf25810e52fa",
      "metadata": {
        "id": "2422c099-58ee-4b15-be5e-cf25810e52fa"
      },
      "outputs": [],
      "source": [
        "# define prompt template\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "def format_prompt(actor:str, input:str):\n",
        "    match actor:\n",
        "        case \"user\":\n",
        "            prompt_template =  \"\"\"<|begin_of_text|><|start_header_id|>{actor}<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\n",
        "\"\"\"\n",
        "            prompt = PromptTemplate.from_template(prompt_template)\n",
        "            return prompt.format(actor=actor,input=input)\n",
        "        case _:\n",
        "            print(\"requested actor >\" + actor + \"< is not supported\")\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Code Cell 2 — Create the Prompt**\n",
        "####**What this step does:**\n",
        "\n",
        "- Writes the actual **instruction** you want the model to follow.\n",
        "\n",
        "- Provides **context** about the dataset ```sales.csv```.\n",
        "\n",
        "- Clearly lists what **outputs** the generated Python code must produce.\n",
        "\n",
        "- Ensures the model returns **only clean, runnable Python code**.\n",
        "\n",
        "####**Why this step?**\n",
        "\n",
        "This step is essential because the model can only generate accurate code if it receives a **well-structured, detailed prompt**.\n",
        "\n",
        "Here’s why it matters:\n",
        "\n",
        "- It tells the model **exactly what problem it needs to solve**.\n",
        "\n",
        "- It specifies the **required analysis tasks**, reducing ambiguity.\n",
        "\n",
        "- It prevents the model from returning extra text by clearly stating:\n",
        "**“Return only Python code, no explanation.”**\n",
        "\n",
        "It ensures the generated program will correctly read and analyze your ```sales.csv``` file.\n",
        "\n",
        "A well-crafted prompt directly **improves accuracy, completeness, and quality** of the generated Python script."
      ],
      "metadata": {
        "id": "BVGY9-daxqWF"
      },
      "id": "BVGY9-daxqWF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ee2bae-6415-4dba-af98-a19028305c98",
      "metadata": {
        "tags": [],
        "id": "45ee2bae-6415-4dba-af98-a19028305c98"
      },
      "outputs": [],
      "source": [
        "# Create the prompt\n",
        "# Analyzing sales\n",
        "\n",
        "prompt_data = \"\"\"\n",
        "\n",
        "You have a CSV, sales.csv, with columns:\n",
        "- date (YYYY-MM-DD)\n",
        "- product_id\n",
        "- price\n",
        "- units_sold\n",
        "\n",
        "Create a python program to analyze the sales data from a CSV file. The program should be able to read the data, and determine below:\n",
        "\n",
        "- Total revenue for the year\n",
        "- Total revenue by product\n",
        "- The product with the highest revenue\n",
        "- The date with the highest revenue and the revenue achieved on that date\n",
        "- Visualize monthly sales using a bar chart\n",
        "\n",
        "Ensure the code is syntactically correct, bug-free, optimized, not span multiple lines unnessarily, and prefer to use standard libraries. Return only python code without any surrounding text, explanation or context.\n",
        "\n",
        "\"\"\"\n",
        "prompt=format_prompt(\"user\",prompt_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Code Cell 3 — Prepare Request Body**\n",
        "####**What this step does:**\n",
        "\n",
        "- Converts the prompt and generation settings into **JSON format**.\n",
        "\n",
        "- This JSON gets passed to the **Bedrock model for execution**.\n",
        "\n",
        "- Controls **output size, temperature, and randomness**.\n",
        "\n",
        "####**Why this step?**\n",
        "\n",
        "This step is necessary because Amazon Bedrock expects inputs in a **structured JSON format.**\n",
        "Without this, the model cannot process your instructions.\n",
        "\n",
        "By creating this JSON request body:\n",
        "\n",
        "- You specify **how long** the model is allowed to generate ```max_gen_len```.\n",
        "\n",
        "- You set **temperature = 0** to keep the output deterministic and avoid random variations.\n",
        "\n",
        "- You control **top_p**, which defines how much of the probability distribution the model should consider.\n",
        "\n",
        "- You ensure the final request is **API-ready**, meaning it can be directly sent to the Bedrock endpoint.\n",
        "\n",
        "This step creates the exact structure the model needs to generate correct and stable Python code."
      ],
      "metadata": {
        "id": "0Oe7hlpax5yq"
      },
      "id": "0Oe7hlpax5yq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1ec809",
      "metadata": {
        "id": "ab1ec809"
      },
      "outputs": [],
      "source": [
        "body = json.dumps({\n",
        "    \"prompt\": prompt,\n",
        "    \"max_gen_len\": 2048,\n",
        "    \"temperature\": 0,\n",
        "    \"top_p\": 1,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f28504",
      "metadata": {
        "id": "a1f28504"
      },
      "source": [
        "## **Task 5.4: Invoke the model**\n",
        "\n",
        "In this task, you send your prepared prompt to the Amazon Bedrock model. The model receives your JSON request, processes the prompt, and returns Python code that analyzes the ```sales.csv``` dataset.\n",
        "\n",
        "####**What this step does:**\n",
        "\n",
        "- Sends your JSON body and prompt to the Bedrock model.\n",
        "\n",
        "- Uses the model ID meta.llama3-8b-instruct-v1:0 for code generation.\n",
        "\n",
        "- Reads the response returned by the model.\n",
        "\n",
        "- Extracts and prints the generated Python code.\n",
        "\n",
        "####**Why this step?**\n",
        "\n",
        "**This step is important because:**\n",
        "\n",
        "- **You have already prepared everything** — environment, dataset, and prompt.\n",
        "Now the model must actually process the prompt and generate Python code.\n",
        "\n",
        "- **The Bedrock model runs only when you “invoke” it**, which means sending a request with:\n",
        "     - the prompt\n",
        "     - generation settings (temperature, max tokens, etc.)\n",
        "     - the model ID\n",
        "\n",
        "- **The invoke call sends your request to the Bedrock API**, which returns the model’s output.\n",
        "\n",
        "- **The model returns a structured JSON response**, so you must read and extract the generated program.\n",
        "\n",
        "- **Without this invocation**, the model will never generate the Python script you asked for.\n",
        "\n",
        "    It is the actual “execution step” where Bedrock performs the work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f46b39",
      "metadata": {
        "id": "d1f46b39"
      },
      "outputs": [],
      "source": [
        "modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
        "response = bedrock_client.invoke_model(body=body, modelId=modelId)\n",
        "response_body = json.loads(response.get('body').read())\n",
        "output_list = response_body.get(\"generation\", [])\n",
        "print(output_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddddd1ec",
      "metadata": {
        "id": "ddddd1ec"
      },
      "source": [
        "##### <i aria-hidden=\"true\" class=\"far fa-copy\"></i> (Optional) Copy the generated code from the printed output and run the Bedrock generated code in the cell below for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fcf0eb",
      "metadata": {
        "id": "91fcf0eb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "64b08b3b",
      "metadata": {
        "id": "64b08b3b"
      },
      "source": [
        "\n",
        "You have now experimented with using `boto3` SDK which provides a vanilla exposure to Amazon Bedrock API. Using this API you generate a python program to analyze and visualize given sales data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Summary**\n",
        "\n",
        "In this lab, you learned how to invoke an **Amazon Bedrock model** through an API to automatically generate a Python program for analyzing sales data. You began by setting up your environment and creating a sample sales.csv dataset. Then, you prepared a structured prompt using LangChain and formatted it for the model.\n",
        "\n",
        "Next, you explored how to package the prompt into a request body and finally invoked the **Llama 3 model** to generate optimized, clean, and fully functional Python code — without writing the program manually. This step demonstrated the power of large language models in automating code generation for real-world tasks.\n",
        "\n",
        "By the end of this lab, you were able to:\n",
        "\n",
        "- Create and prepare data for **AI-driven code generation**\n",
        "\n",
        "- Build a **structured prompt** to control model behavior\n",
        "\n",
        "- Use **Boto3** to call Amazon Bedrock models programmatically\n",
        "\n",
        "- Retrieve, read, and display the **model-generated output**\n",
        "\n",
        "This workflow gives you a solid foundation in **LLM-based code automation**, a critical skill in modern AI engineering and enterprise productivity."
      ],
      "metadata": {
        "id": "C3yeKJBC0GEo"
      },
      "id": "C3yeKJBC0GEo"
    },
    {
      "cell_type": "markdown",
      "id": "e6536211",
      "metadata": {
        "id": "e6536211"
      },
      "source": [
        "### Try it yourself\n",
        "\n",
        "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
        "- Play with the token length to understand the latency and responsiveness of the service.\n",
        "- Apply different prompt engineering principles to get better outputs.\n",
        "\n",
        "### Cleanup\n",
        "\n",
        "You have completed this notebook. To move to the next part of the lab, do the following:\n",
        "\n",
        "- Close this notebook file and continue with the **Task6**"
      ]
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      }
    ],
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "00878cbed564b904a98b4a19808853cb6b9988746b881ea025a8408713879bf5"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}