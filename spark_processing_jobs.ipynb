{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Perform data processing with SageMaker Processing\n",
    "\n",
    "In this notebook, you set up the environment needed to run a basic Apache Spark application using Amazon SageMaker Processing. By using Apache Spark on SageMaker Processing, you can run Spark jobs without having to provision an Amazon EMR cluster. You then define and run a Spark job using the **PySparkProcessor** class from the **SageMaker Python SDK**. Finally, you validate the data processing results saved in Amazon Simple Storage Service (Amazon S3).\n",
    "\n",
    "The processing script does some basic data processing, such as string indexing, one-hot encoding, vector assembly, and an 80-20 split of the processed data to train and validate datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Setup the environment\n",
    "\n",
    "Install the latest SageMaker Python SDK package and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T08:18:10.243701Z",
     "iopub.status.busy": "2025-07-17T08:18:10.243328Z",
     "iopub.status.idle": "2025-07-17T08:18:16.433296Z",
     "shell.execute_reply": "2025-07-17T08:18:16.432422Z",
     "shell.execute_reply.started": "2025-07-17T08:18:10.243674Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install awscli --upgrade\n",
    "%pip install boto3 --upgrade\n",
    "%pip install -U \"sagemaker>2.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After upgrading the SDK, restart your notebook kernel. \n",
    "\n",
    "1. Choose the **Restart kernel** icon from the notebook toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, import the required libraries, get the execution role to run the SageMaker processing job, and set up the Amazon S3 bucket to store the Spark job outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T08:25:22.445718Z",
     "iopub.status.busy": "2025-07-17T08:25:22.445327Z",
     "iopub.status.idle": "2025-07-17T08:25:24.820011Z",
     "shell.execute_reply": "2025-07-17T08:25:24.819043Z",
     "shell.execute_reply.started": "2025-07-17T08:25:22.445695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "SageMaker Execution Role:  arn:aws:iam::442042521384:role/service-role/AmazonSageMaker-ExecutionRole-20250717T120670\n",
      "Bucket:  modeldevelopmentk21\n"
     ]
    }
   ],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the Spark processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'modeldevelopmentk21' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** If you get an error, make sure you restarted your notebook kernel by selecting the **Restart kernel** icon from the notebook toolbar. Then, rerun the cell. However, you may ignore any warnings from the above cell. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Run the SageMaker processing job\n",
    "\n",
    "In this task, you import and review the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T08:29:13.753996Z",
     "iopub.status.busy": "2025-07-17T08:29:13.753360Z",
     "iopub.status.idle": "2025-07-17T08:29:13.916706Z",
     "shell.execute_reply": "2025-07-17T08:29:13.915940Z",
     "shell.execute_reply.started": "2025-07-17T08:29:13.753967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>20</td>\n",
       "      <td>?</td>\n",
       "      <td>38455</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>166248</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>70</td>\n",
       "      <td>Private</td>\n",
       "      <td>573446</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>1455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>66</td>\n",
       "      <td>Private</td>\n",
       "      <td>104936</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>182274</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1       2         3   4                    5   \\\n",
       "516  20         ?   38455   HS-grad   9        Never-married   \n",
       "624  31   Private  166248   HS-grad   9   Married-civ-spouse   \n",
       "892  70   Private  573446   HS-grad   9   Married-civ-spouse   \n",
       "511  66   Private  104936      10th   6              Widowed   \n",
       "569  32   Private  182274   HS-grad   9            Separated   \n",
       "\n",
       "                     6           7       8        9     10  11  12  \\\n",
       "516                   ?   Unmarried   White     Male     0   0  40   \n",
       "624   Machine-op-inspct     Husband   White     Male     0   0  40   \n",
       "892        Craft-repair     Husband   White     Male  1455   0  40   \n",
       "511       Other-service   Unmarried   White   Female     0   0  38   \n",
       "569       Other-service   Own-child   White   Female     0   0  37   \n",
       "\n",
       "                 13      14  \n",
       "516   United-States   <=50K  \n",
       "624   United-States   <=50K  \n",
       "892   United-States   <=50K  \n",
       "511   United-States   <=50K  \n",
       "569   United-States   <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import-data\n",
    "prefix = 'data/input'\n",
    "\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/spark_adult_data.csv\", local_path= 'data/')\n",
    "\n",
    "shape=pd.read_csv(\"data/spark_adult_data.csv\", header=None)\n",
    "shape.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the SageMaker Spark PySparkProcessor class to define and run a spark application as a processing job. Refer to [SageMaker Spark PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor) for more information about this class.\n",
    "\n",
    "For creating the PySparkProcessor class, you configure the following parameters:\n",
    "- **base_job_name**: Prefix for the processing job name\n",
    "- **framework_version**: SageMaker PySpark version\n",
    "- **role**: SageMaker execution role\n",
    "- **instance_count**: Number of instances to run the processing job\n",
    "- **instance_type**: Type of Amazon Elastic Compute Cloud (Amazon EC2) instance used for the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T08:30:34.514943Z",
     "iopub.status.busy": "2025-07-17T08:30:34.514461Z",
     "iopub.status.idle": "2025-07-17T08:30:34.568807Z",
     "shell.execute_reply": "2025-07-17T08:30:34.567912Z",
     "shell.execute_reply.started": "2025-07-17T08:30:34.514918Z"
    }
   },
   "outputs": [],
   "source": [
    "#pyspark-processor\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# create a PySparkProcessor\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-preprocessor\",\n",
    "    framework_version=\"3.1\", # Spark version\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you use the PySparkProcessor run method to run the **pyspark_preprocessing.py** script as a processing job. Refer to [PySparkProcessor run method](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor.run) for more information about this method. For this lab, data transformations such as string indexing and one-hot encoding are performed on the categorical features.\n",
    "\n",
    "For running the processing job, you configure the following parameters:\n",
    "- **submit_app**: Path of the preprocessing script \n",
    "- **outputs**: Path of output for the preprocessing script (Amazon S3 output locations)\n",
    "- **arguments**: Command-line arguments to the preprocessing script (such as the Amazon S3 input and output locations)\n",
    "\n",
    "The processing job takes approximately 5 minutes to complete. While the job is running, you can review the source for the preprocessing script (which has been preconfigured as part of this lab) by opening the **pyspark_preprocessing.py** file from the file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T08:37:37.264760Z",
     "iopub.status.busy": "2025-07-17T08:37:37.264249Z",
     "iopub.status.idle": "2025-07-17T08:43:03.589314Z",
     "shell.execute_reply": "2025-07-17T08:43:03.588463Z",
     "shell.execute_reply.started": "2025-07-17T08:37:37.264738Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sm-spark-preprocessor-2025-07-17-08-37-37-267\n",
      "INFO:sagemaker:Creating processing-job with name sm-spark-preprocessor-2025-07-17-08-37-37-267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............\u001b[34m07-17 08:39 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'modeldevelopmentk21', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'modeldevelopmentk21', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'modeldevelopmentk21', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'modeldevelopmentk21', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1', 'algo-2'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-2', 'algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:442042521384:processing-job/sm-spark-preprocessor-2025-07-17-08-37-37-267', 'ProcessingJobName': 'sm-spark-preprocessor-2025-07-17-08-37-37-267', 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:3.1-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py'], 'ContainerArguments': ['--s3_input_bucket', 'modeldevelopmentk21', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'modeldevelopmentk21', '--s3_output_key_prefix', 'data/output']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://modeldevelopmentk21/scripts/pyspark_preprocessing.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/train', 'S3Uri': 's3://modeldevelopmentk21/data/output/train', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'validation_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/validation', 'S3Uri': 's3://modeldevelopmentk21/data/output/validation', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'output-3', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://modeldevelopmentk21/logs/spark_event_logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::442042521384:role/service-role/AmazonSageMaker-ExecutionRole-20250717T120670', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/pyspark_preprocessing.py --s3_input_bucket modeldevelopmentk21 --s3_input_key_prefix data/input --s3_output_bucket modeldevelopmentk21 --s3_output_key_prefix data/output\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34m07-17 08:39 waitress     INFO     Serving on http://10.0.100.233:5555\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     Found hadoop jar hadoop-aws-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m07-17 08:39 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[34m07-17 08:39 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m07-17 08:39 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.100.233</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI,AWS_REGION</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\u001b[0m\n",
      "\u001b[34m07-17 08:39 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m07-17 08:39 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.0.100.233\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34m# Fix for \"Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot\u001b[0m\n",
      "\u001b[34m# receive any reply from 10.0.109.30:35219 in 120 seconds.\"\"\u001b[0m\n",
      "\u001b[34mspark.rpc.askTimeout=300s\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 4\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 2\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[34mspark.yarn.appMasterEnv.AWS_REGION us-east-1\u001b[0m\n",
      "\u001b[34mspark.executorEnv.AWS_REGION us-east-1\u001b[0m\n",
      "\u001b[34m07-17 08:39 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m07-17 08:39 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34mWARNING: /usr/lib/hadoop/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:45,697 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.100.233\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop\u001b[0m\n",
      "\u001b[34m-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:45,708 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:45,782 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-9cd47124-2e03-4630-ab76-56751d1429e3\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,225 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,236 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,237 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,237 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,242 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,242 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,242 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,242 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,280 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,290 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,290 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,294 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,294 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Jul 17 08:39:46\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,295 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,295 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,297 INFO util.GSet: 2.0% max memory 3.1 GB = 63.0 MB\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,297 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,321 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,321 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,327 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,327 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,327 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,327 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,328 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,328 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,328 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,328 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,328 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,328 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,328 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,347 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,347 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,347 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,347 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,358 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,358 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,358 INFO util.GSet: 1.0% max memory 3.1 GB = 31.5 MB\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,358 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,368 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,368 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,368 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,368 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,373 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,374 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,378 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,378 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,378 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,378 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,385 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,385 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,385 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,388 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,388 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,390 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,390 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,390 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 967.2 KB\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,390 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,413 INFO namenode.FSImage: Allocated new BlockPoolId: BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,429 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,452 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,530 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,538 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,542 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:46,542 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.100.233\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:47,574 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.100.233\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop\u001b[0m\n",
      "\u001b[34m-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:47,587 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:47,628 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.100.233\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop\u001b[0m\n",
      "\u001b[34m-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:47,651 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:47,699 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.100.233\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop\u001b[0m\n",
      "\u001b[34m-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:47,705 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.100.233\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop\u001b[0m\n",
      "\u001b[34m-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:47,719 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:47,734 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:47,931 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,251 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,519 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,545 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,545 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,579 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://10.0.100.233/\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,600 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,601 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,678 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,713 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,713 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,779 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,817 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,856 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,882 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,893 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,899 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,904 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,912 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,915 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,922 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,923 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,924 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,924 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,925 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,934 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,934 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,937 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,951 INFO util.log: Logging initialized @2253ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,957 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,961 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,962 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,969 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:48,970 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,009 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,010 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,011 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,012 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,033 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,082 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,082 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,108 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,163 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,184 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,184 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,194 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,202 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,204 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,204 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,205 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,247 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,247 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,250 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,258 INFO http.HttpServer2: Jetty bound to port 9870\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,259 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,275 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,275 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,279 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,292 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,296 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,303 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,321 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,321 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,326 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,327 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,327 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,328 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,329 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,345 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@302c971f{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,346 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,346 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61c4eee0{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,348 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@17f9d882\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,350 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,351 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,351 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,351 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,371 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,371 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,390 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,405 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,408 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@7c137fd5\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,409 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,410 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,410 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,411 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,411 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,411 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,418 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.3 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,418 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,439 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,443 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,443 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,444 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,445 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,450 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,452 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,451 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,455 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,459 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,459 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,462 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,462 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,474 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[0m\n",
      "\u001b[34m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,474 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,486 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4a00d9cf{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,497 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,499 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,501 INFO server.AbstractConnector: Started ServerConnector@50378a4{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,501 INFO server.Server: Started @2804ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,503 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,505 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,505 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,509 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34meffectiveMinResource=<memory:0, vCores:0>\n",
      " , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34mpriority = 0\u001b[0m\n",
      "\u001b[34mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,512 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,518 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,522 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,524 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,526 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,527 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,528 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,532 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,537 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,537 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,540 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,552 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'modeldevelopmentk21', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'modeldevelopmentk21', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'modeldevelopmentk21', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'modeldevelopmentk21', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     {'current_host': 'algo-2', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1', 'algo-2'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-2', 'algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:442042521384:processing-job/sm-spark-preprocessor-2025-07-17-08-37-37-267', 'ProcessingJobName': 'sm-spark-preprocessor-2025-07-17-08-37-37-267', 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:3.1-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py'], 'ContainerArguments': ['--s3_input_bucket', 'modeldevelopmentk21', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'modeldevelopmentk21', '--s3_output_key_prefix', 'data/output']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://modeldevelopmentk21/scripts/pyspark_preprocessing.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/train', 'S3Uri': 's3://modeldevelopmentk21/data/output/train', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'validation_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/validation', 'S3Uri': 's3://modeldevelopmentk21/data/output/validation', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'output-3', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://modeldevelopmentk21/logs/spark_event_logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::442042521384:role/service-role/AmazonSageMaker-ExecutionRole-20250717T120670', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/pyspark_preprocessing.py --s3_input_bucket modeldevelopmentk21 --s3_input_key_prefix data/input --s3_output_bucket modeldevelopmentk21 --s3_output_key_prefix data/output\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     Status server listening on algo-2:5555\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[35m07-17 08:39 waitress     INFO     Serving on http://10.0.80.5:5555\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     Found hadoop jar hadoop-aws.jar\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[35m07-17 08:39 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[35m07-17 08:39 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m07-17 08:39 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[35m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[35m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.100.233</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-2</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-2:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI,AWS_REGION</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[35m</configuration>\u001b[0m\n",
      "\u001b[35m07-17 08:39 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m07-17 08:39 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[35mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.driver.host=10.0.100.233\u001b[0m\n",
      "\u001b[35mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[35m# Fix for \"Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot\u001b[0m\n",
      "\u001b[35m# receive any reply from 10.0.109.30:35219 in 120 seconds.\"\"\u001b[0m\n",
      "\u001b[35mspark.rpc.askTimeout=300s\u001b[0m\n",
      "\u001b[35mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[35mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[35mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[35mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[35mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[35mspark.executor.cores 4\u001b[0m\n",
      "\u001b[35mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[35mspark.executor.instances 2\u001b[0m\n",
      "\u001b[35mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[35mspark.yarn.appMasterEnv.AWS_REGION us-east-1\u001b[0m\n",
      "\u001b[35mspark.executorEnv.AWS_REGION us-east-1\u001b[0m\n",
      "\u001b[35m07-17 08:39 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[35m07-17 08:39 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/lib/hadoop/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:46,558 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.0.80.5\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib\u001b[0m\n",
      "\u001b[35m/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:46,571 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:46,649 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.0.80.5\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib\u001b[0m\n",
      "\u001b[35m/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:46,664 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,160 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,160 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,225 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,231 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,281 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,282 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,284 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,284 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,284 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,285 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,286 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,288 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,305 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,306 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,323 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,356 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,401 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,401 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,441 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,441 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,462 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,472 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,511 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@79e4c792\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,513 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,515 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,515 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,515 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,541 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,546 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,550 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@183ec003\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,550 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,552 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,552 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,552 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,552 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,552 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,556 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.2 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,557 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,580 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,581 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,587 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,587 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,590 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,596 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,598 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,602 INFO datanode.DataNode: Configured hostname is algo-2\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,602 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,605 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,612 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,632 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,635 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,635 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,667 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,676 INFO util.log: Logging initialized @1629ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,685 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,885 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,892 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,898 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,900 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,900 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,900 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,922 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,930 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,931 INFO http.HttpServer2: Jetty bound to port 35103\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,931 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,932 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,945 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:36391\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,953 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,954 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,955 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,955 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,957 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,958 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,958 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,959 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,959 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,962 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.80.5:36391\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,962 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/10.0.80.5:0\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,963 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,967 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,968 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@463fd068{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,968 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@42b3b079{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:47,998 INFO util.log: Logging initialized @1948ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,031 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,040 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2421cc4{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,048 INFO server.AbstractConnector: Started ServerConnector@518caac3{HTTP/1.1,[http/1.1]}{localhost:35103}\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,049 INFO server.Server: Started @2002ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,101 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,105 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,113 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,114 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,115 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,115 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,116 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,116 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,116 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,119 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,119 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,193 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,199 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,201 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,201 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,248 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,264 INFO ipc.Server: Starting Socket Reader #1 for port 9867\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,477 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,495 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,496 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,498 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,500 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,504 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,514 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.100.233:8020 starting to offer service\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,521 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,523 INFO ipc.Server: IPC Server listener on 9867: starting\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,538 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,538 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,539 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,551 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,554 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,555 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,565 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:48,694 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[35mJul 17, 2025 8:39:48 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[35mJul 17, 2025 8:39:48 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[35mJul 17, 2025 8:39:48 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[35mJul 17, 2025 8:39:48 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[35mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[35mJul 17, 2025 8:39:48 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mJul 17, 2025 8:39:49 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mJul 17, 2025 8:39:49 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,380 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-2-8042-_-any-3843069425983043104.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/node}\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,390 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-2:8042}\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,391 INFO server.Server: Started @3341ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,391 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,392 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:36391\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,393 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,400 INFO client.RMProxy: Connecting to ResourceManager at /10.0.100.233:8031\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,448 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,460 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:49,599 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[35mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     waiting for the primary to come up\u001b[0m\n",
      "\u001b[35m07-17 08:39 smspark-submit INFO     waiting for the primary to go down\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,566 INFO util.log: Logging initialized @2870ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,566 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,594 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,637 INFO util.log: Logging initialized @2934ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,847 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,854 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,861 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,865 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,865 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,865 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,865 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,866 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,866 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,867 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,867 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,867 INFO http.HttpServer2: adding path spec: /app/*\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,986 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:49,994 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,001 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,003 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,003 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,004 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,024 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,025 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,027 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,035 INFO http.HttpServer2: Jetty bound to port 35631\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,036 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,078 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:44691\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,101 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,103 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,105 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,107 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,110 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,116 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@463fd068{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,117 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@42b3b079{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,118 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,118 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,132 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,135 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,136 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,141 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,144 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.100.233:44691\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,145 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.0.100.233:0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,145 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,157 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,190 INFO util.log: Logging initialized @3486ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,219 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,233 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,234 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,235 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,242 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,243 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,243 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,243 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,277 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,303 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,319 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2421cc4{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,321 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,330 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,332 INFO server.AbstractConnector: Started ServerConnector@518caac3{HTTP/1.1,[http/1.1]}{localhost:35631}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,332 INFO server.Server: Started @3637ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,335 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,346 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Jul 17 08:39:50\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,348 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,348 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,350 INFO util.GSet: 2.0% max memory 3.1 GB = 63.0 MB\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,350 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,360 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,364 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,376 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,379 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,379 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,381 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,382 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,383 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,383 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,384 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,385 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,385 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,386 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,392 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,393 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,393 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,393 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,394 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,394 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,394 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,394 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,395 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,395 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,395 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,434 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,434 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,434 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,435 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,454 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,455 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,455 INFO util.GSet: 1.0% max memory 3.1 GB = 31.5 MB\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,455 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,458 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,459 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,459 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,459 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,465 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,473 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,569 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,582 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,582 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,583 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,601 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,601 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,602 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,605 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,606 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,608 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,608 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,616 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,616 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,616 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,617 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 967.2 KB\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,617 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,635 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,659 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 115@algo-1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,663 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:50,568 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:50,601 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,681 INFO ipc.Server: Starting Socket Reader #1 for port 9867\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,701 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,708 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,709 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,806 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,813 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,821 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,838 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,873 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,873 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,879 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,882 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,990 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,990 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:50,993 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,006 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,018 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,027 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,043 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,049 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,069 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,069 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,069 INFO namenode.FSNamesystem: Finished loading FSImage in 446 msecs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,081 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@352c1b98{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,081 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d898981{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,087 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,097 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,099 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,115 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,116 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,125 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.100.233:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,149 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,172 INFO ipc.Server: IPC Server listener on 9867: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,228 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,228 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,230 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,253 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,275 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,297 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4218500f{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,298 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@c2db68f{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,313 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,475 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,483 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,520 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,543 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:51,568 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:51,601 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:52,569 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:52,978 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.100.233:8020\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:52,981 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:52,986 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 17@algo-2\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:52,987 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 1505011654. Formatting...\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:52,987 INFO common.Storage: Generated new storageID DS-54294dad-0a08-4cf0-a734-c9f779a57e38 for directory /opt/amazon/hadoop/hdfs/datanode \u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,007 INFO common.Storage: Analyzing storage directories for bpid BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,007 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,008 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-665918883-10.0.100.233-1752741586407 is not formatted. Formatting ...\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,008 INFO common.Storage: Formatting block pool BP-665918883-10.0.100.233-1752741586407 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,011 INFO datanode.DataNode: Setting up storage: nsid=1505011654;bpid=BP-665918883-10.0.100.233-1752741586407;lv=-57;nsInfo=lv=-65;cid=CID-9cd47124-2e03-4630-ab76-56751d1429e3;nsid=1505011654;c=1752741586407;bpid=BP-665918883-10.0.100.233-1752741586407;dnuuid=null\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,012 INFO datanode.DataNode: Generated and persisted new Datanode UUID f124b4f4-45dd-443e-9259-2327f9b9b381\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,084 INFO impl.FsDatasetImpl: Added new volume: DS-54294dad-0a08-4cf0-a734-c9f779a57e38\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,085 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,088 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,105 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,114 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,116 INFO impl.FsDatasetImpl: Adding block pool BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,117 INFO impl.FsDatasetImpl: Scanning block pool BP-665918883-10.0.100.233-1752741586407 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,141 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-665918883-10.0.100.233-1752741586407 on /opt/amazon/hadoop/hdfs/datanode: 24ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,141 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-665918883-10.0.100.233-1752741586407: 24ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,143 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-665918883-10.0.100.233-1752741586407 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,143 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,144 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-665918883-10.0.100.233-1752741586407 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,144 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-665918883-10.0.100.233-1752741586407: 3ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,146 INFO datanode.VolumeScanner: Now scanning bpid BP-665918883-10.0.100.233-1752741586407 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,148 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-54294dad-0a08-4cf0-a734-c9f779a57e38): finished scanning block pool BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,160 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-54294dad-0a08-4cf0-a734-c9f779a57e38): no suitable block pools found to scan.  Waiting 1814399986 ms.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,163 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 7/17/25 2:31 PM with interval of 21600000ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,167 INFO datanode.DataNode: Block pool BP-665918883-10.0.100.233-1752741586407 (Datanode Uuid f124b4f4-45dd-443e-9259-2327f9b9b381) service to algo-1/10.0.100.233:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,877 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,901 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:51,928 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:51 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,024 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,046 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,046 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,047 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,099 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,108 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:52 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,246 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.0.100.233:8020\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,298 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,298 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,338 INFO namenode.FSDirectory: Quota initialization completed in 39 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,354 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,394 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,394 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,394 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,394 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,394 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,394 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,394 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 348 msec\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:52 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:52,985 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1640c151{node,/,file:///tmp/jetty-algo-1-8042-_-any-3641098054954582522.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/node}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,006 INFO server.AbstractConnector: Started ServerConnector@14dda234{HTTP/1.1,[http/1.1]}{algo-1:8042}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,007 INFO server.Server: Started @6303ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,007 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,058 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:44691\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,059 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,077 INFO client.RMProxy: Connecting to ResourceManager at /10.0.100.233:8031\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,092 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.100.233:8020\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,102 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,124 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 116@algo-1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,125 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 1505011654. Formatting...\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,126 INFO common.Storage: Generated new storageID DS-ebabe3a2-2c0f-452f-bad1-fc2526ccde2d for directory /opt/amazon/hadoop/hdfs/datanode \u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,197 INFO common.Storage: Analyzing storage directories for bpid BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,198 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,198 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-665918883-10.0.100.233-1752741586407 is not formatted. Formatting ...\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,199 INFO common.Storage: Formatting block pool BP-665918883-10.0.100.233-1752741586407 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,207 INFO datanode.DataNode: Setting up storage: nsid=1505011654;bpid=BP-665918883-10.0.100.233-1752741586407;lv=-57;nsInfo=lv=-65;cid=CID-9cd47124-2e03-4630-ab76-56751d1429e3;nsid=1505011654;c=1752741586407;bpid=BP-665918883-10.0.100.233-1752741586407;dnuuid=null\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,211 INFO datanode.DataNode: Generated and persisted new Datanode UUID ebb7cd6a-dc0d-421b-b556-137dd4c37184\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,215 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,229 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.80.5:9866, datanodeUuid=f124b4f4-45dd-443e-9259-2327f9b9b381, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-9cd47124-2e03-4630-ab76-56751d1429e3;nsid=1505011654;c=1752741586407) storage f124b4f4-45dd-443e-9259-2327f9b9b381\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,242 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.80.5:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,242 INFO blockmanagement.BlockReportLeaseManager: Registered DN f124b4f4-45dd-443e-9259-2327f9b9b381 (10.0.80.5:9866).\u001b[0m\n",
      "\u001b[34mJul 17, 2025 8:39:53 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,265 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,352 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@499683c4{cluster,/,file:///tmp/jetty-10_0_100_233-8088-_-any-4722425228429601618.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/cluster}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,391 INFO server.AbstractConnector: Started ServerConnector@51549490{HTTP/1.1,[http/1.1]}{10.0.100.233:8088}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,391 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-54294dad-0a08-4cf0-a734-c9f779a57e38 for DN 10.0.80.5:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,391 INFO server.Server: Started @6688ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,391 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,464 INFO BlockStateChange: BLOCK* processReport 0x7d8a95253df6e67c: Processing first storage report for DS-54294dad-0a08-4cf0-a734-c9f779a57e38 from datanode f124b4f4-45dd-443e-9259-2327f9b9b381\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,466 INFO BlockStateChange: BLOCK* processReport 0x7d8a95253df6e67c: from storage DS-54294dad-0a08-4cf0-a734-c9f779a57e38 node DatanodeRegistration(10.0.80.5:9866, datanodeUuid=f124b4f4-45dd-443e-9259-2327f9b9b381, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-9cd47124-2e03-4630-ab76-56751d1429e3;nsid=1505011654;c=1752741586407), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,516 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,558 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,594 INFO impl.FsDatasetImpl: Added new volume: DS-ebabe3a2-2c0f-452f-bad1-fc2526ccde2d\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,594 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,605 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,626 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,687 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,690 INFO impl.FsDatasetImpl: Adding block pool BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,702 INFO impl.FsDatasetImpl: Scanning block pool BP-665918883-10.0.100.233-1752741586407 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,814 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-665918883-10.0.100.233-1752741586407 on /opt/amazon/hadoop/hdfs/datanode: 104ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,814 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-665918883-10.0.100.233-1752741586407: 124ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,830 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-665918883-10.0.100.233-1752741586407 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,830 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,874 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-665918883-10.0.100.233-1752741586407 on volume /opt/amazon/hadoop/hdfs/datanode: 44ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,874 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-665918883-10.0.100.233-1752741586407: 59ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,877 INFO datanode.VolumeScanner: Now scanning bpid BP-665918883-10.0.100.233-1752741586407 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,879 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-ebabe3a2-2c0f-452f-bad1-fc2526ccde2d): finished scanning block pool BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,894 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-ebabe3a2-2c0f-452f-bad1-fc2526ccde2d): no suitable block pools found to scan.  Waiting 1814399982 ms.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,903 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 7/17/25 9:56 AM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,928 INFO datanode.DataNode: Block pool BP-665918883-10.0.100.233-1752741586407 (Datanode Uuid ebb7cd6a-dc0d-421b-b556-137dd4c37184) service to algo-1/10.0.100.233:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m07-17 08:39 sagemaker-spark-event-logs-publisher INFO     Start to copy the spark event logs file.\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m07-17 08:39 sagemaker-spark-event-logs-publisher INFO     Writing event log config to spark-defaults.conf\u001b[0m\n",
      "\u001b[34m07-17 08:39 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m07-17 08:39 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2025-07-17T08:39:53.959772')), ('algo-2', StatusMessage(status='WAITING', timestamp='2025-07-17T08:39:53.963616'))])\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,970 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,279 INFO datanode.DataNode: Block pool Block pool BP-665918883-10.0.100.233-1752741586407 (Datanode Uuid f124b4f4-45dd-443e-9259-2327f9b9b381) service to algo-1/10.0.100.233:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,279 INFO datanode.DataNode: For namenode algo-1/10.0.100.233:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,570 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8031. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,572 INFO datanode.DataNode: Successfully sent block report 0x7d8a95253df6e67c,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 140 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:53,572 INFO datanode.DataNode: Got finalize command for block pool BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,988 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.100.233:9866, datanodeUuid=ebb7cd6a-dc0d-421b-b556-137dd4c37184, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-9cd47124-2e03-4630-ab76-56751d1429e3;nsid=1505011654;c=1752741586407) storage ebb7cd6a-dc0d-421b-b556-137dd4c37184\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,990 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.100.233:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,990 INFO blockmanagement.BlockReportLeaseManager: Registered DN ebb7cd6a-dc0d-421b-b556-137dd4c37184 (10.0.100.233:9866).\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,993 INFO datanode.DataNode: Block pool Block pool BP-665918883-10.0.100.233-1752741586407 (Datanode Uuid ebb7cd6a-dc0d-421b-b556-137dd4c37184) service to algo-1/10.0.100.233:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,994 INFO datanode.DataNode: For namenode algo-1/10.0.100.233:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,003 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:53,990 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,010 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,027 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,028 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,028 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,028 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,028 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,028 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,034 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-ebabe3a2-2c0f-452f-bad1-fc2526ccde2d for DN 10.0.100.233:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,038 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,038 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,038 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,038 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,057 INFO BlockStateChange: BLOCK* processReport 0x25982cf5d9b77a8e: Processing first storage report for DS-ebabe3a2-2c0f-452f-bad1-fc2526ccde2d from datanode ebb7cd6a-dc0d-421b-b556-137dd4c37184\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,058 INFO BlockStateChange: BLOCK* processReport 0x25982cf5d9b77a8e: from storage DS-ebabe3a2-2c0f-452f-bad1-fc2526ccde2d node DatanodeRegistration(10.0.100.233:9866, datanodeUuid=ebb7cd6a-dc0d-421b-b556-137dd4c37184, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-9cd47124-2e03-4630-ab76-56751d1429e3;nsid=1505011654;c=1752741586407), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,078 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,237 INFO datanode.DataNode: Successfully sent block report 0x25982cf5d9b77a8e,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 185 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,241 INFO datanode.DataNode: Got finalize command for block pool BP-665918883-10.0.100.233-1752741586407\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,476 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,493 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,509 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,547 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,558 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,583 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,584 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,588 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,593 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,604 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,614 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,663 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,686 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,692 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,699 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,709 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,710 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,798 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,801 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,811 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,814 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,816 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,895 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,909 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 44691 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:44691\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,914 INFO rmnode.RMNodeImpl: algo-1:44691 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,930 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1679517312\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,931 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1658400126\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,932 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:44691 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:54,941 INFO capacity.CapacityScheduler: Added node algo-1:44691 clusterResource: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:55,582 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 36391 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:36391\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:55,582 INFO rmnode.RMNodeImpl: algo-2:36391 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:55,585 INFO capacity.CapacityScheduler: Added node algo-2:36391 clusterResource: <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:54,571 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:55,572 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8031. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:55,593 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1679517312\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:55,594 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1658400126\u001b[0m\n",
      "\u001b[35m2025-07-17 08:39:55,594 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:36391 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,316 INFO spark.SparkContext: Running Spark version 3.1.1-amzn-0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,358 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,359 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,359 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,359 INFO spark.SparkContext: Submitted application: PySparkApp\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,379 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 12399, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,393 INFO resource.ResourceProfile: Limiting resource is cpus at 4 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,395 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,452 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,452 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,453 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,453 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,453 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,699 INFO util.Utils: Successfully started service 'sparkDriver' on port 39837.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,739 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,786 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,804 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,805 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,836 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,849 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-ecfd3644-4a08-4197-8692-6255d59edd27\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,870 INFO memory.MemoryStore: MemoryStore started with capacity 1028.8 MiB\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:57,913 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,022 INFO util.log: Logging initialized @3419ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,098 INFO server.Server: jetty-9.4.37.v20210219; built: 2021-02-19T15:16:47.689Z; git: 27afab2bd37780d179836e313e0fe11bc4fa0ce9; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,120 INFO server.Server: Started @3518ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,156 INFO server.AbstractConnector: Started ServerConnector@3661f38b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,156 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,177 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b26ce89{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,180 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e13b4b8{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,180 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55406aea{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,181 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c83a18{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,182 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30bdc132{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,183 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a065ceb{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,184 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ec58603{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,185 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d32b92f{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,186 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1aeffdf2{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,187 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13f28fca{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,188 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56c3956{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,189 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34e26d61{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,189 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ac59032{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,190 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27b82a05{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,191 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52efd8a6{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,192 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f15c3e{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,192 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4682da31{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,193 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5729d5b9{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,194 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4dcd1dc4{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,195 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a3b339{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,203 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@487ef90a{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,204 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1370b974{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,205 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c5192af{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,206 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@272c5304{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,207 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77382419{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,209 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.100.233:4040\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,479 INFO client.RMProxy: Connecting to ResourceManager at /10.0.100.233:8032\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,724 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:58,749 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:59,188 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:59,189 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:59,204 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:59,204 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:59,205 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:59,206 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:59,211 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-07-17 08:39:59,289 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:05,570 INFO yarn.Client: Uploading resource file:/tmp/spark-701e440a-5fca-46a3-a136-04546a52bad2/__spark_libs__5193836196964757518.zip -> hdfs://10.0.100.233/user/root/.sparkStaging/application_1752741594005_0001/__spark_libs__5193836196964757518.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:05,677 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.0.100.233:9866, 10.0.80.5:9866 for /user/root/.sparkStaging/application_1752741594005_0001/__spark_libs__5193836196964757518.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:05,795 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741825_1001 src: /10.0.100.233:36096 dest: /10.0.100.233:9866\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:05,857 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741825_1001 src: /10.0.100.233:43926 dest: /10.0.80.5:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:06,407 INFO DataNode.clienttrace: src: /10.0.100.233:36096, dest: /10.0.100.233:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: ebb7cd6a-dc0d-421b-b556-137dd4c37184, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741825_1001, duration(ns): 523341618\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:06,407 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.80.5:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:06,417 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.0.100.233:9866, 10.0.80.5:9866 for /user/root/.sparkStaging/application_1752741594005_0001/__spark_libs__5193836196964757518.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:06,422 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741826_1002 src: /10.0.100.233:36106 dest: /10.0.100.233:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:06,871 INFO DataNode.clienttrace: src: /10.0.100.233:36106, dest: /10.0.100.233:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: ebb7cd6a-dc0d-421b-b556-137dd4c37184, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741826_1002, duration(ns): 444600618\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:06,871 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.80.5:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:06,875 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.0.100.233:9866, 10.0.80.5:9866 for /user/root/.sparkStaging/application_1752741594005_0001/__spark_libs__5193836196964757518.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:06,880 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741827_1003 src: /10.0.100.233:36110 dest: /10.0.100.233:9866\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:06,404 INFO DataNode.clienttrace: src: /10.0.100.233:43926, dest: /10.0.80.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: f124b4f4-45dd-443e-9259-2327f9b9b381, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741825_1001, duration(ns): 524040662\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:06,404 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:06,424 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741826_1002 src: /10.0.100.233:43928 dest: /10.0.80.5:9866\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:06,869 INFO DataNode.clienttrace: src: /10.0.100.233:43928, dest: /10.0.80.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: f124b4f4-45dd-443e-9259-2327f9b9b381, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741826_1002, duration(ns): 443325476\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:06,869 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:06,882 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741827_1003 src: /10.0.100.233:43944 dest: /10.0.80.5:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,273 INFO DataNode.clienttrace: src: /10.0.100.233:36110, dest: /10.0.100.233:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: ebb7cd6a-dc0d-421b-b556-137dd4c37184, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741827_1003, duration(ns): 389238946\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,274 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.80.5:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,279 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.0.100.233:9866, 10.0.80.5:9866 for /user/root/.sparkStaging/application_1752741594005_0001/__spark_libs__5193836196964757518.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,286 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741828_1004 src: /10.0.100.233:36122 dest: /10.0.100.233:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,416 INFO DataNode.clienttrace: src: /10.0.100.233:36122, dest: /10.0.100.233:9866, bytes: 46002121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: ebb7cd6a-dc0d-421b-b556-137dd4c37184, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741828_1004, duration(ns): 111096429\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,416 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.80.5:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,424 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1752741594005_0001/__spark_libs__5193836196964757518.zip is closed by DFSClient_NONMAPREDUCE_1983024488_18\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,535 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://10.0.100.233/user/root/.sparkStaging/application_1752741594005_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,542 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.0.100.233:9866, 10.0.80.5:9866 for /user/root/.sparkStaging/application_1752741594005_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,546 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741829_1005 src: /10.0.100.233:36134 dest: /10.0.100.233:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,555 INFO DataNode.clienttrace: src: /10.0.100.233:36134, dest: /10.0.100.233:9866, bytes: 889814, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: ebb7cd6a-dc0d-421b-b556-137dd4c37184, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741829_1005, duration(ns): 5711420\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,555 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.80.5:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,557 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1752741594005_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_1983024488_18\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,566 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://10.0.100.233/user/root/.sparkStaging/application_1752741594005_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,574 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.0.100.233:9866, 10.0.80.5:9866 for /user/root/.sparkStaging/application_1752741594005_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,578 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741830_1006 src: /10.0.100.233:36142 dest: /10.0.100.233:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,584 INFO DataNode.clienttrace: src: /10.0.100.233:36142, dest: /10.0.100.233:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: ebb7cd6a-dc0d-421b-b556-137dd4c37184, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741830_1006, duration(ns): 3130835\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,584 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.80.5:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,586 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1752741594005_0001/py4j-0.10.9-src.zip is closed by DFSClient_NONMAPREDUCE_1983024488_18\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,713 INFO yarn.Client: Uploading resource file:/tmp/spark-701e440a-5fca-46a3-a136-04546a52bad2/__spark_conf__3859146954933460588.zip -> hdfs://10.0.100.233/user/root/.sparkStaging/application_1752741594005_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,721 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.0.100.233:9866, 10.0.80.5:9866 for /user/root/.sparkStaging/application_1752741594005_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,726 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741831_1007 src: /10.0.100.233:36158 dest: /10.0.100.233:9866\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,734 INFO DataNode.clienttrace: src: /10.0.100.233:36158, dest: /10.0.100.233:9866, bytes: 266133, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: ebb7cd6a-dc0d-421b-b556-137dd4c37184, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741831_1007, duration(ns): 4102330\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,735 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.80.5:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,737 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1752741594005_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_1983024488_18\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,760 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,760 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,760 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,760 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,760 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,781 INFO yarn.Client: Submitting application application_1752741594005_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,875 INFO capacity.CapacityScheduler: Application 'application_1752741594005_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,875 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,891 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,892 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,893 INFO rmapp.RMAppImpl: Storing application with id application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,894 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.100.233#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,900 INFO recovery.RMStateStore: Storing info for app: application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,900 INFO rmapp.RMAppImpl: application_1752741594005_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,901 INFO rmapp.RMAppImpl: application_1752741594005_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,903 INFO capacity.ParentQueue: Application added - appId: application_1752741594005_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,904 INFO capacity.CapacityScheduler: Accepted application application_1752741594005_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,913 INFO rmapp.RMAppImpl: application_1752741594005_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,937 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,938 INFO attempt.RMAppAttemptImpl: appattempt_1752741594005_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,272 INFO DataNode.clienttrace: src: /10.0.100.233:43944, dest: /10.0.80.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: f124b4f4-45dd-443e-9259-2327f9b9b381, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741827_1003, duration(ns): 388133123\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,272 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,302 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741828_1004 src: /10.0.100.233:43950 dest: /10.0.80.5:9866\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,414 INFO DataNode.clienttrace: src: /10.0.100.233:43950, dest: /10.0.80.5:9866, bytes: 46002121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: f124b4f4-45dd-443e-9259-2327f9b9b381, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741828_1004, duration(ns): 110947087\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,415 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,548 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741829_1005 src: /10.0.100.233:43956 dest: /10.0.80.5:9866\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,554 INFO DataNode.clienttrace: src: /10.0.100.233:43956, dest: /10.0.80.5:9866, bytes: 889814, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: f124b4f4-45dd-443e-9259-2327f9b9b381, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741829_1005, duration(ns): 4849678\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,554 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,580 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741830_1006 src: /10.0.100.233:43968 dest: /10.0.80.5:9866\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,584 INFO DataNode.clienttrace: src: /10.0.100.233:43968, dest: /10.0.80.5:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: f124b4f4-45dd-443e-9259-2327f9b9b381, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741830_1006, duration(ns): 2361882\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,584 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,728 INFO datanode.DataNode: Receiving BP-665918883-10.0.100.233-1752741586407:blk_1073741831_1007 src: /10.0.100.233:43984 dest: /10.0.80.5:9866\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,733 INFO DataNode.clienttrace: src: /10.0.100.233:43984, dest: /10.0.80.5:9866, bytes: 266133, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1983024488_18, offset: 0, srvID: f124b4f4-45dd-443e-9259-2327f9b9b381, blockid: BP-665918883-10.0.100.233-1752741586407:blk_1073741831_1007, duration(ns): 3148782\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:07,733 INFO datanode.DataNode: PacketResponder: BP-665918883-10.0.100.233-1752741586407:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:07,992 INFO impl.YarnClientImpl: Submitted application application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,004 INFO capacity.LeafQueue: Application application_1752741594005_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,004 INFO capacity.LeafQueue: Application added - appId: application_1752741594005_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,004 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1752741594005_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,012 INFO attempt.RMAppAttemptImpl: appattempt_1752741594005_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,086 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1752741594005_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,089 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,090 INFO fica.FiCaSchedulerNode: Assigned container container_1752741594005_0001_01_000001 of capacity <memory:896, max memory:15892, vCores:1, max vCores:4> on host algo-1:44691, which has 1 containers, <memory:896, vCores:1> used and <memory:14996, vCores:3> available after allocation\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,090 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,108 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:44691 for container : container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,115 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,115 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,115 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1752741594005_0001 AttemptId: appattempt_1752741594005_0001_000001 MasterContainer: Container: [ContainerId: container_1752741594005_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:44691, NodeHttpAddress: algo-1:8042, Resource: <memory:896, max memory:15892, vCores:1, max vCores:4>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.100.233:44691 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,116 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,116 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,126 INFO attempt.RMAppAttemptImpl: appattempt_1752741594005_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,129 INFO attempt.RMAppAttemptImpl: appattempt_1752741594005_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,140 INFO amlauncher.AMLauncher: Launching masterappattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,187 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1752741594005_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:44691, NodeHttpAddress: algo-1:8042, Resource: <memory:896, max memory:15892, vCores:1, max vCores:4>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.100.233:44691 }, ExecutionType: GUARANTEED, ] for AM appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,188 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,190 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,310 INFO ipc.Server: Auth successful for appattempt_1752741594005_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,397 INFO containermanager.ContainerManagerImpl: Start request for container_1752741594005_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,449 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,458 INFO application.ApplicationImpl: Application application_1752741594005_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,458 INFO application.ApplicationImpl: Adding container_1752741594005_0001_01_000001 to application application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,458 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.100.233#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,462 INFO application.ApplicationImpl: Application application_1752741594005_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,466 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,466 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,474 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1752741594005_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:44691, NodeHttpAddress: algo-1:8042, Resource: <memory:896, max memory:15892, vCores:1, max vCores:4>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.100.233:44691 }, ExecutionType: GUARANTEED, ] for AM appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,475 INFO attempt.RMAppAttemptImpl: appattempt_1752741594005_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,475 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1752741594005_0001, attemptId: appattempt_1752741594005_0001_000001launchTime: 1752741608474\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,475 INFO recovery.RMStateStore: Updating info for app: application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,477 INFO localizer.ResourceLocalizationService: Created localizer for container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,547 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1752741594005_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,559 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,565 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1752741594005_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,566 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:08,997 INFO yarn.Client: Application report for application_1752741594005_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:09,000 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1752741607891\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1752741594005_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:09,083 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:10,003 INFO yarn.Client: Application report for application_1752741594005_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:11,005 INFO yarn.Client: Application report for application_1752741594005_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:12,008 INFO yarn.Client: Application report for application_1752741594005_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:12,213 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000001 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:12,214 INFO scheduler.ContainerScheduler: Starting container [container_1752741594005_0001_01_000001]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:12,241 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:12,241 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:12,247 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/directory.info\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:13,012 INFO yarn.Client: Application report for application_1752741594005_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m07-17 08:40 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:14,015 INFO yarn.Client: Application report for application_1752741594005_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:14,152 INFO monitor.ContainersMonitorImpl: container_1752741594005_0001_01_000001's ip = 10.0.100.233, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:14,158 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1752741594005_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:14,172 INFO ipc.Server: Auth successful for appattempt_1752741594005_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:14,195 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:14,196 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.100.233#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011APPATTEMPTID=appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:14,196 INFO attempt.RMAppAttemptImpl: appattempt_1752741594005_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:14,197 INFO rmapp.RMAppImpl: application_1752741594005_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:14,428 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1752741594005_0001), /proxy/application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,018 INFO yarn.Client: Application report for application_1752741594005_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,019 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.100.233\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1752741607891\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1752741594005_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,021 INFO cluster.YarnClientSchedulerBackend: Application application_1752741594005_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,034 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40183.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,035 INFO netty.NettyBlockTransferService: Server created on 10.0.100.233:40183\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,036 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,045 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.100.233, 40183, None)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,052 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.100.233:40183 with 1028.8 MiB RAM, BlockManagerId(driver, 10.0.100.233, 40183, None)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,059 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.100.233, 40183, None)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,059 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.100.233, 40183, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,167 INFO util.SignalUtils: Registering signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,169 INFO util.SignalUtils: Registering signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,169 INFO util.SignalUtils: Registering signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,528 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,529 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,529 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,530 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,530 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,636 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:13,809 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:14,040 INFO client.RMProxy: Connecting to ResourceManager at /10.0.100.233:8030\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:14,089 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:14,322 INFO client.TransportClientFactory: Successfully created connection to /10.0.100.233:39837 after 75 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:14,440 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:15,010 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] Default YARN executor launch context:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     AWS_REGION -> us-east-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://10.0.100.233/user/root/.sparkStaging/application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       -Xmx12399m \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-Dspark.driver.port=39837' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       '-Dspark.rpc.askTimeout=300s' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.0.100.233:39837 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       4 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       application_1752741594005_0001 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       --resourceProfileId \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       0 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.0.100.233\" port: -1 file: \"/user/root/.sparkStaging/application_1752741594005_0001/pyspark.zip\" } size: 889814 timestamp: 1752741607556 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"hdfs\" host: \"10.0.100.233\" port: -1 file: \"/user/root/.sparkStaging/application_1752741594005_0001/__spark_libs__5193836196964757518.zip\" } size: 448655305 timestamp: 1752741607423 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     py4j-0.10.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.0.100.233\" port: -1 file: \"/user/root/.sparkStaging/application_1752741594005_0001/py4j-0.10.9-src.zip\" } size: 41587 timestamp: 1752741607586 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"hdfs\" host: \"10.0.100.233\" port: -1 file: \"/user/root/.sparkStaging/application_1752741594005_0001/__spark_conf__.zip\" } size: 266133 timestamp: 1752741607737 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:15,104 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 12399, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/stderr] 2025-07-17 08:40:15,105 INFO yarn.YarnAllocator: Resource profile 0 doesn't exist, adding it\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,293 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,326 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,331 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60cdfdd7{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,365 INFO history.SingleEventLogFileWriter: Logging events to file:/tmp/spark-events/application_1752741594005_0001.inprogress\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,611 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,680 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1752741594005_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,681 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,682 INFO fica.FiCaSchedulerNode: Assigned container container_1752741594005_0001_01_000002 of capacity <memory:13638, vCores:1> on host algo-2:36391, which has 1 containers, <memory:13638, vCores:1> used and <memory:2254, vCores:3> available after allocation\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,682 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,683 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.4572741 absoluteUsedCapacity=0.4572741 used=<memory:14534, vCores:2> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,683 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,780 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:36391 for container : container_1752741594005_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,782 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,894 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,894 INFO internal.SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,909 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,911 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cf4cb09{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,911 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,912 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c81a725{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,913 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,914 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593cd6bf{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,914 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,915 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c5c7b64{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,916 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:15,917 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f2a3d55{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:15,969 INFO ipc.Server: Auth successful for appattempt_1752741594005_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,051 INFO containermanager.ContainerManagerImpl: Start request for container_1752741594005_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,112 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1752741594005_0001\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,123 INFO application.ApplicationImpl: Application application_1752741594005_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,123 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.100.233#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000002\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,123 INFO application.ApplicationImpl: Adding container_1752741594005_0001_01_000002 to application application_1752741594005_0001\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,128 INFO application.ApplicationImpl: Application application_1752741594005_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,132 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,132 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1752741594005_0001\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,143 INFO localizer.ResourceLocalizationService: Created localizer for container_1752741594005_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,104 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1752741594005_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,104 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000003 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,105 INFO fica.FiCaSchedulerNode: Assigned container container_1752741594005_0001_01_000003 of capacity <memory:13638, vCores:1> on host algo-1:44691, which has 2 containers, <memory:14534, vCores:2> used and <memory:1358, vCores:2> available after allocation\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,105 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,105 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8863579 absoluteUsedCapacity=0.8863579 used=<memory:28172, vCores:3> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,105 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,214 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:44691 for container : container_1752741594005_0001_01_000003\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,215 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,229 INFO ipc.Server: Auth successful for appattempt_1752741594005_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,232 INFO containermanager.ContainerManagerImpl: Start request for container_1752741594005_0001_01_000003 by user root\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,235 INFO application.ApplicationImpl: Adding container_1752741594005_0001_01_000003 to application application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,236 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.100.233#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000003\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,237 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000003 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,237 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,238 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000003 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,239 INFO scheduler.ContainerScheduler: Starting container [container_1752741594005_0001_01_000003]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,259 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000003 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,260 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1752741594005_0001_01_000003\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,264 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000003/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000001/sHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/launch_container.sh\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/directory.info\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,696 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,880 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,892 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:16,892 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,228 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1752741594005_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,245 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,250 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1752741594005_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:16,250 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:17,106 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:17,160 INFO monitor.ContainersMonitorImpl: container_1752741594005_0001_01_000003's ip = 10.0.100.233, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:17,167 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1752741594005_0001_01_000003 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:18,106 INFO datasources.InMemoryFileIndex: It took 85 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:19,221 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:19,556 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.100.233:35654) with ID 2,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:19,835 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:43019 with 6.3 GiB RAM, BlockManagerId(2, algo-1, 43019, None)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:19,595 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:19,596 INFO scheduler.ContainerScheduler: Starting container [container_1752741594005_0001_01_000002]\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:19,626 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:19,627 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1752741594005_0001_01_000002\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:19,631 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/directory.info\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:17,497 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1215@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:17,506 INFO util.SignalUtils: Registering signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:17,507 INFO util.SignalUtils: Registering signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:17,507 INFO util.SignalUtils: Registering signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,211 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,212 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,214 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,215 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,215 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,642 INFO client.TransportClientFactory: Successfully created connection to /10.0.100.233:39837 after 89 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,837 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,837 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,838 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,838 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,838 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:18,907 INFO client.TransportClientFactory: Successfully created connection to /10.0.100.233:39837 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,006 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/blockmgr-7c3c12ca-2498-4fa1-a241-c13aae1fdc22\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,056 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GiB\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:20,981 INFO monitor.ContainersMonitorImpl: container_1752741594005_0001_01_000002's ip = 10.0.80.5, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:20,989 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1752741594005_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,151 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,152 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,154 INFO datasources.FileSourceStrategy: Output Data Schema: struct<workclass: string>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,603 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 317.5 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,664 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,667 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,671 INFO spark.SparkContext: Created broadcast 0 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,728 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,732 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,813 INFO scheduler.DAGScheduler: Registering RDD 3 (collect at StringIndexer.scala:204) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,817 INFO scheduler.DAGScheduler: Got map stage job 0 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,817 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 0 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,818 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,819 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,826 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:21,985 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 19.4 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:22,004 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:22,006 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.100.233:40183 (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:22,006 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:22,021 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:22,022 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:22,052 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 2, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:22,089 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.80.5:56566) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:22,281 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:35829 with 6.3 GiB RAM, BlockManagerId(1, algo-2, 35829, None)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:22,393 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:43019 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:20,615 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 433@algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:20,626 INFO util.SignalUtils: Registering signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:20,627 INFO util.SignalUtils: Registering signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:20,627 INFO util.SignalUtils: Registering signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,115 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,116 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,116 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,117 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,117 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,439 INFO client.TransportClientFactory: Successfully created connection to /10.0.100.233:39837 after 58 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,527 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,527 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,528 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,528 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,528 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,581 INFO client.TransportClientFactory: Successfully created connection to /10.0.100.233:39837 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,658 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/blockmgr-77185185-52e4-424c-9bc0-566cea739355\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,700 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GiB\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,337 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.100.233:39837\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,351 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,352 INFO resource.ResourceUtils: No custom resources configured for spark.executor.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,352 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,572 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,590 INFO executor.Executor: Starting executor ID 2 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,798 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,814 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43019.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,814 INFO netty.NettyBlockTransferService: Server created on algo-1:43019\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,816 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,825 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-1, 43019, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,834 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,834 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,849 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-1, 43019, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:19,850 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, algo-1, 43019, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:21,788 INFO executor.YarnCoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 2335 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:22,082 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:22,096 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:22,244 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:23,501 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:43019 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,359 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3315 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,361 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,369 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (collect at StringIndexer.scala:204) finished in 3.520 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,369 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,370 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,370 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,370 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,477 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,481 INFO scheduler.DAGScheduler: Got job 1 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,481 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,481 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,482 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,484 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,496 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.0 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,498 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,499 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.100.233:40183 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,500 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,501 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,501 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,509 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (algo-1, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,534 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:43019 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,727 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.100.233:35654\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,932 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 425 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,932 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,933 INFO scheduler.DAGScheduler: ResultStage 2 (collect at StringIndexer.scala:204) finished in 0.441 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,935 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,936 INFO cluster.YarnScheduler: Killing all running tasks in stage 2: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:25,937 INFO scheduler.DAGScheduler: Job 1 finished: collect at StringIndexer.scala:204, took 0.459582 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,145 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:43019 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,164 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.100.233:40183 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,190 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.100.233:40183 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,192 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:43019 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:22,321 INFO client.TransportClientFactory: Successfully created connection to /10.0.100.233:40183 after 4 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:22,388 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:22,399 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 155 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:22,480 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,027 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,490 INFO codegen.CodeGenerator: Code generated in 221.571053 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,492 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,499 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,502 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,557 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,798 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,843 INFO codegen.CodeGenerator: Code generated in 9.115683 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,859 INFO codegen.CodeGenerator: Code generated in 6.102567 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:23,874 INFO codegen.CodeGenerator: Code generated in 9.649953 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:24,936 INFO codegen.CodeGenerator: Code generated in 14.083917 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,074 INFO codegen.CodeGenerator: Code generated in 14.443718 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,345 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2162 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,513 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,513 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,310 INFO codegen.CodeGenerator: Code generated in 289.886213 ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,790 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,790 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,790 INFO datasources.FileSourceStrategy: Output Data Schema: struct<education: string>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,821 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 317.5 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,835 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,836 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,837 INFO spark.SparkContext: Created broadcast 3 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,838 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,838 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,844 INFO scheduler.DAGScheduler: Registering RDD 10 (collect at StringIndexer.scala:204) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,844 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,844 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,845 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,845 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,846 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,857 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.4 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,859 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,860 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.100.233:40183 (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,860 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,861 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,861 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:26,865 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (algo-2, executor 1, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:27,106 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:35829 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,938 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.100.233:39837\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,952 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,952 INFO resource.ResourceUtils: No custom resources configured for spark.executor.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:21,952 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,094 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,098 INFO executor.Executor: Starting executor ID 1 on host algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,261 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35829.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,261 INFO netty.NettyBlockTransferService: Server created on algo-2:35829\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,263 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,272 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,273 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-2, 35829, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,288 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-2, 35829, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,288 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-2, 35829, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,294 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:22,294 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:23,840 INFO executor.YarnCoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 1821 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:26,873 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:26,884 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 2)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:26,943 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:28,329 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:35829 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,188 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 3324 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,188 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,189 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (collect at StringIndexer.scala:204) finished in 3.341 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,189 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,189 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,189 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,189 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,217 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:35829 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,222 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,229 INFO scheduler.DAGScheduler: Got job 3 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,230 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,230 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,230 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.100.233:40183 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,230 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,231 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[13] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,235 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.0 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,237 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,237 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.100.233:40183 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,238 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,239 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,239 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,240 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (algo-2, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,258 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:35829 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,411 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.80.5:56566\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,568 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 328 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,568 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,569 INFO scheduler.DAGScheduler: ResultStage 5 (collect at StringIndexer.scala:204) finished in 0.336 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,570 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,570 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,570 INFO scheduler.DAGScheduler: Job 3 finished: collect at StringIndexer.scala:204, took 0.347881 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,685 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,686 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,686 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marital_status: string>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,711 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,723 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,724 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,724 INFO spark.SparkContext: Created broadcast 6 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,726 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,726 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,731 INFO scheduler.DAGScheduler: Registering RDD 17 (collect at StringIndexer.scala:204) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,731 INFO scheduler.DAGScheduler: Got map stage job 4 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,731 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,731 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,731 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,732 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,740 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,741 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,742 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.100.233:40183 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,743 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,744 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,744 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,745 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (algo-2, executor 1, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,758 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:35829 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,777 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:35829 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,885 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 140 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,885 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,886 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at StringIndexer.scala:204) finished in 0.153 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,887 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,887 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,887 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,887 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,914 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,915 INFO scheduler.DAGScheduler: Got job 5 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,915 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,915 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,916 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,916 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[20] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,920 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,922 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,922 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.100.233:40183 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,923 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,923 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,923 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,924 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (algo-2, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,940 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:35829 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,947 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.80.5:56566\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,978 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 54 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,979 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,980 INFO scheduler.DAGScheduler: ResultStage 8 (collect at StringIndexer.scala:204) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,980 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,980 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:30,980 INFO scheduler.DAGScheduler: Job 5 finished: collect at StringIndexer.scala:204, took 0.066065 s\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:27,019 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:27,069 INFO client.TransportClientFactory: Successfully created connection to /10.0.100.233:40183 after 2 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:27,099 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:27,111 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 91 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:27,186 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:27,898 INFO datasources.FileScanRDD: TID: 2 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:28,317 INFO codegen.CodeGenerator: Code generated in 198.581581 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:28,319 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:28,327 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:28,330 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 11 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:28,385 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:28,559 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:28,604 INFO codegen.CodeGenerator: Code generated in 9.108734 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:28,623 INFO codegen.CodeGenerator: Code generated in 6.867228 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:28,640 INFO codegen.CodeGenerator: Code generated in 10.218809 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:29,769 INFO codegen.CodeGenerator: Code generated in 14.133716 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:29,910 INFO codegen.CodeGenerator: Code generated in 17.67107 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,184 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 2). 2119 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,243 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,089 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,090 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,090 INFO datasources.FileSourceStrategy: Output Data Schema: struct<occupation: string>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,113 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 317.5 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,126 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,126 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,127 INFO spark.SparkContext: Created broadcast 9 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,129 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,129 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,134 INFO scheduler.DAGScheduler: Registering RDD 24 (collect at StringIndexer.scala:204) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,135 INFO scheduler.DAGScheduler: Got map stage job 6 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,135 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 9 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,135 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,135 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,135 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,145 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.4 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,172 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,177 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.100.233:40183 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,177 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,178 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,178 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,180 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:35829 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,180 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (algo-1, executor 2, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,192 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.100.233:40183 in memory (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,201 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:43019 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,226 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.100.233:40183 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,233 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:43019 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,250 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:35829 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,272 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:35829 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,281 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.100.233:40183 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,291 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.100.233:40183 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,292 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:35829 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,299 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:35829 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,521 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,522 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,533 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,536 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,537 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,716 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,718 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.100.233:39837)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,775 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,808 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,809 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,871 INFO codegen.CodeGenerator: Code generated in 23.868622 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:25,921 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 1). 3573 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,182 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,183 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 6)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,184 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,186 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,199 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,202 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 16 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,204 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,310 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.100.233:40183 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,402 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 222 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,402 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,407 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (collect at StringIndexer.scala:204) finished in 0.269 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,410 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,411 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,411 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,411 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,449 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,450 INFO scheduler.DAGScheduler: Got job 7 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,451 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,451 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,452 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,452 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,460 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 22.0 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,462 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,463 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.100.233:40183 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,464 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,464 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,465 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,466 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (algo-1, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,483 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:43019 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,493 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.100.233:35654\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,539 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 73 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,539 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,541 INFO scheduler.DAGScheduler: ResultStage 11 (collect at StringIndexer.scala:204) finished in 0.085 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,542 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,542 INFO cluster.YarnScheduler: Killing all running tasks in stage 11: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,542 INFO scheduler.DAGScheduler: Job 7 finished: collect at StringIndexer.scala:204, took 0.092916 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,683 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,683 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,683 INFO datasources.FileSourceStrategy: Output Data Schema: struct<relationship: string>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,704 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,717 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,717 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,719 INFO spark.SparkContext: Created broadcast 12 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,720 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,721 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,726 INFO scheduler.DAGScheduler: Registering RDD 31 (collect at StringIndexer.scala:204) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,727 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,727 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 12 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,727 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,728 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,728 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,739 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,741 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,742 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.100.233:40183 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,742 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,743 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,743 INFO cluster.YarnScheduler: Adding task set 12.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,744 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (algo-2, executor 1, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,758 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:35829 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,772 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:35829 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,862 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 118 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,862 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,863 INFO scheduler.DAGScheduler: ShuffleMapStage 12 (collect at StringIndexer.scala:204) finished in 0.134 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,863 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,863 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,863 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,863 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,892 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,894 INFO scheduler.DAGScheduler: Got job 9 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,894 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,894 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,894 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,896 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[34] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,899 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,901 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,901 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.100.233:40183 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,902 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,902 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[34] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,902 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,904 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (algo-2, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,916 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:35829 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,923 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.80.5:56566\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,955 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 51 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,955 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,956 INFO scheduler.DAGScheduler: ResultStage 14 (collect at StringIndexer.scala:204) finished in 0.059 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,957 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,957 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:31,958 INFO scheduler.DAGScheduler: Job 9 finished: collect at StringIndexer.scala:204, took 0.065393 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,053 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,053 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,054 INFO datasources.FileSourceStrategy: Output Data Schema: struct<race: string>\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,243 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 3)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,249 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,251 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,256 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,259 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,260 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,404 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,406 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.100.233:39837)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,433 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,464 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,465 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,504 INFO codegen.CodeGenerator: Code generated in 15.588603 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,564 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 3). 3604 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,748 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,748 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 4)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,751 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,757 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,760 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,761 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,767 INFO datasources.FileScanRDD: TID: 4 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,770 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,775 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,778 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,793 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,832 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,882 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 4). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,930 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,930 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 5)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,932 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,933 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,938 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,941 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,942 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,946 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,946 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.100.233:39837)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,949 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,950 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,087 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:35829 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,095 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.100.233:40183 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,108 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 317.5 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,118 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:35829 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,121 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,122 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,123 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.100.233:40183 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,123 INFO spark.SparkContext: Created broadcast 15 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,125 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,125 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,130 INFO scheduler.DAGScheduler: Registering RDD 38 (collect at StringIndexer.scala:204) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,130 INFO scheduler.DAGScheduler: Got map stage job 10 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,131 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,131 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,131 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,132 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[38] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,154 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.4 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,157 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,162 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.100.233:40183 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,163 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,163 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[38] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,163 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,165 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (algo-1, executor 2, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,172 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.100.233:40183 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,183 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:43019 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,185 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:43019 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,207 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.100.233:40183 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,209 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:35829 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,211 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:43019 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,258 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.100.233:40183 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,260 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:43019 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,269 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.100.233:40183 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,270 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:43019 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,213 INFO datasources.FileScanRDD: TID: 6 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,216 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,231 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,235 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 18 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,255 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,344 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,393 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 6). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,469 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,470 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 7)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,471 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,473 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,481 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,484 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,486 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,492 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,492 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.100.233:39837)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,495 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,311 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 147 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,311 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,312 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at StringIndexer.scala:204) finished in 0.179 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,312 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,312 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,312 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,312 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,333 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,334 INFO scheduler.DAGScheduler: Got job 11 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,334 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,335 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,335 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,335 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[41] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,337 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 21.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,338 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,339 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.100.233:40183 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,339 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,340 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[41] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,340 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,341 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (algo-1, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,353 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:43019 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,359 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.100.233:35654\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,402 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 61 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,402 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,403 INFO scheduler.DAGScheduler: ResultStage 17 (collect at StringIndexer.scala:204) finished in 0.067 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,403 INFO scheduler.DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,404 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,404 INFO scheduler.DAGScheduler: Job 11 finished: collect at StringIndexer.scala:204, took 0.070828 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,497 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,497 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,497 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,514 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,526 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,527 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,528 INFO spark.SparkContext: Created broadcast 18 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,530 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,530 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,534 INFO scheduler.DAGScheduler: Registering RDD 45 (collect at StringIndexer.scala:204) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,535 INFO scheduler.DAGScheduler: Got map stage job 12 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,535 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,535 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,535 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,535 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[45] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,544 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 19.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,546 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,547 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.100.233:40183 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,547 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,548 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[45] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,548 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,549 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (algo-2, executor 1, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,561 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:35829 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,575 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:35829 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,658 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 109 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,658 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,658 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (collect at StringIndexer.scala:204) finished in 0.122 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,659 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,659 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,659 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,659 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,682 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,683 INFO scheduler.DAGScheduler: Got job 13 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,683 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,683 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,683 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,683 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[48] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,686 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 21.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,687 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,690 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.100.233:40183 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,690 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,690 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[48] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,690 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,692 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 13) (algo-2, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,703 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:35829 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,709 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.80.5:56566\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,737 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 13) in 46 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,737 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,738 INFO scheduler.DAGScheduler: ResultStage 20 (collect at StringIndexer.scala:204) finished in 0.054 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,739 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,739 INFO cluster.YarnScheduler: Killing all running tasks in stage 20: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,740 INFO scheduler.DAGScheduler: Job 13 finished: collect at StringIndexer.scala:204, took 0.057630 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,759 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-2:35829 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,764 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.100.233:40183 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,771 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:43019 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,775 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.100.233:40183 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,785 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.100.233:40183 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,790 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:43019 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,803 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:43019 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,806 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.100.233:40183 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,818 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.100.233:40183 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,821 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-2:35829 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,863 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,863 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,864 INFO datasources.FileSourceStrategy: Output Data Schema: struct<native_country: string>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,883 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 317.5 KiB, free 1027.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,895 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,895 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,897 INFO spark.SparkContext: Created broadcast 21 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,898 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,898 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,903 INFO scheduler.DAGScheduler: Registering RDD 52 (collect at StringIndexer.scala:204) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,903 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,903 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 21 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,903 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,904 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,904 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[52] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,914 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,916 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,918 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.100.233:40183 (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,919 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,919 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[52] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,919 INFO cluster.YarnScheduler: Adding task set 21.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,926 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (algo-1, executor 2, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,940 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:43019 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:32,976 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:43019 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,950 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:30,975 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 5). 3537 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,747 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,747 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 8)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,748 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,749 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,756 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,759 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 9 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,760 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,763 INFO datasources.FileScanRDD: TID: 8 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,766 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,771 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,773 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,782 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,829 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,860 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 8). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,907 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,907 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 9)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,908 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,909 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,914 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,917 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,917 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,922 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,922 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.100.233:39837)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,925 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,926 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,926 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:31,953 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 9). 3516 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,552 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,552 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 12)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,553 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,555 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,560 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,562 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,563 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,083 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 157 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,083 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,085 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (collect at StringIndexer.scala:204) finished in 0.180 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,085 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,085 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,085 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,086 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,129 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,129 INFO scheduler.DAGScheduler: Got job 15 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,130 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,130 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,130 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,130 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[55] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,133 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 22.0 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,134 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,135 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.100.233:40183 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,135 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,136 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[55] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,136 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,138 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 15) (algo-1, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,150 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:43019 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,158 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.100.233:35654\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,193 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 15) in 55 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,193 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,194 INFO scheduler.DAGScheduler: ResultStage 23 (collect at StringIndexer.scala:204) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,195 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,195 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,195 INFO scheduler.DAGScheduler: Job 15 finished: collect at StringIndexer.scala:204, took 0.066052 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,496 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,496 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:31,535 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 7). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,167 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,167 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 10)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,168 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,169 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,179 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,184 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 15 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,186 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,189 INFO datasources.FileScanRDD: TID: 10 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,196 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,207 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,212 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 15 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,234 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,276 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,309 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 10). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,343 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,344 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 11)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,345 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,346 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,352 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,354 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,355 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 21.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,358 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,358 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.100.233:39837)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,361 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,362 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,363 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,399 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 11). 3524 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,929 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,930 INFO executor.Executor: Running task 0.0 in stage 21.0 (TID 14)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,931 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,932 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,938 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,942 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,943 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,954 INFO datasources.FileScanRDD: TID: 14 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,966 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,974 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,977 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:32,989 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,050 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,080 INFO executor.Executor: Finished task 0.0 in stage 21.0 (TID 14). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,141 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,142 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 15)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,143 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,144 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,149 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,152 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,153 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,156 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 7, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,156 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.100.233:39837)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,160 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,164 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (868.0 B) non-empty blocks including 1 (868.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,647 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.100.233:40183 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,648 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:43019 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,660 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:43019 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,660 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.100.233:40183 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,664 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.100.233:40183 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,666 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-2:35829 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,669 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.100.233:40183 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,672 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:43019 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,834 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,933 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,933 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:33,933 INFO datasources.FileSourceStrategy: Output Data Schema: struct<age: int, workclass: string, fnlwgt: int, education: string, education_num: int ... 13 more fields>\u001b[0m\n",
      "\u001b[34m07-17 08:40 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1752741594005_0001.inprogress\u001b[0m\n",
      "\u001b[34m07-17 08:40 root         INFO     copying /tmp/spark-events/application_1752741594005_0001.inprogress to /opt/ml/processing/spark-events/application_1752741594005_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,567 INFO datasources.FileScanRDD: TID: 12 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,569 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,574 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,576 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,588 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,626 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,655 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 12). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,694 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,695 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 13)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,696 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,697 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,701 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,704 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,705 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 21.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,708 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,708 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.100.233:39837)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,710 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,711 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,712 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,523 INFO codegen.CodeGenerator: Code generated in 308.670564 ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,526 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 317.5 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,534 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,535 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,536 INFO spark.SparkContext: Created broadcast 24 from toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,537 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,537 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,644 INFO spark.SparkContext: Starting job: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,644 INFO scheduler.DAGScheduler: Got job 16 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,644 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,644 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,645 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,645 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[59] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,652 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 230.1 KiB, free 1027.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,654 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 60.9 KiB, free 1027.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,655 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.100.233:40183 (size: 60.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,655 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,656 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[59] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,656 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,657 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 16) (algo-1, executor 2, partition 0, PROCESS_LOCAL, 5066 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:34,678 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:43019 (size: 60.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:35,374 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:43019 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:35,773 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 16) in 1115 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:35,773 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:35,774 INFO scheduler.DAGScheduler: ResultStage 24 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108) finished in 1.128 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:35,774 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:35,774 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:35,774 INFO scheduler.DAGScheduler: Job 16 finished: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108, took 1.130370 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,164 INFO storage.ShuffleBlockFetcherIterator: Started 0[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:17.415+0000: [GC (Allocation Failure) [PSYoungGen: 57344K->6814K(66560K)] 57344K->6822K(218112K), 0.0094660 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:18.103+0000: [GC (Allocation Failure) [PSYoungGen: 64158K->7221K(66560K)] 64166K->7237K(218112K), 0.0151033 secs] [Times: user=0.02 sys=0.00, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:18.449+0000: [GC (Metadata GC Threshold) [PSYoungGen: 58549K->7533K(66560K)] 58565K->7557K(218112K), 0.0058300 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:18.455+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 7533K->0K(66560K)] [ParOldGen: 24K->7306K(94720K)] 7557K->7306K(161280K), [Metaspace: 20445K->20445K(1067008K)], 0.0351611 secs] [Times: user=0.06 sys=0.00, real=0.04 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:18.985+0000: [GC (Allocation Failure) [PSYoungGen: 57344K->4051K(90112K)] 64650K->11358K(184832K), 0.0098775 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:19.483+0000: [GC (Allocation Failure) [PSYoungGen: 90067K->7937K(118784K)] 97374K->15251K(213504K), 0.0078496 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:19.675+0000: [GC (Metadata GC Threshold) [PSYoungGen: 37277K->4547K(137728K)] 44592K->11870K(232448K), 0.0085175 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:19.684+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 4547K->0K(137728K)] [ParOldGen: 7322K->9391K(142336K)] 11870K->9391K(280064K), [Metaspace: 33945K->33945K(1079296K)], 0.0412550 secs] [Times: user=0.06 sys=0.01, real=0.04 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:22.537+0000: [GC (Allocation Failure) [PSYoungGen: 128512K->9716K(138240K)] 137903K->28417K(280576K), 0.0196775 secs] [Times: user=0.03 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:22.766+0000: [GC (Metadata GC Threshold) [PSYoungGen: 55139K->10809K(199168K)] 73841K->29519K(341504K), 0.0112119 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:22.777+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 10809K->0K(199168K)] [ParOldGen: 18709K->18197K(195072K)] 29519K->18197K(394240K), [Metaspace: 56000K->56000K(1101824K)], 0.0857097 secs] [Times: user=0.21 sys=0.00, real=0.09 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:24.113+0000: [GC (Allocation Failure) [PSYoungGen: 183808K->12539K(199680K)] 202005K->30744K(394752K), 0.0133277 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:25.660+0000: [GC (Allocation Failure) [PSYoungGen: 196347K->16886K(250368K)] 214552K->35242K(445440K), 0.0249214 secs] [Times: user=0.06 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stdout] 2025-07-17T08:40:35.520+0000: [GC (Allocation Failure) [PSYoungGen: 248274K->15873K(252416K)] 266629K->34236K(447 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:33,191 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 15). 3726 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:34,660 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:34,660 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 16)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:34,663 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:34,677 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 60.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:34,679 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 16 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:34,680 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 230.1 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,131 INFO codegen.CodeGenerator: Code generated in 264.360157 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,292 INFO codegen.CodeGenerator: Code generated in 85.484788 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,331 INFO codegen.CodeGenerator: Code generated in 13.611926 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,348 INFO datasources.FileScanRDD: TID: 16 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,365 INFO codegen.CodeGenerator: Code generated in 10.342633 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,366 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,372 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,375 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,387 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,424 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,435 INFO codegen.CodeGenerator: Code generated in 6.921677 ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,371 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,374 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,374 INFO datasources.FileSourceStrategy: Output Data Schema: struct<age: int, workclass: string, fnlwgt: int, education: string, education_num: int ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,426 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:43019 in memory (size: 60.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,443 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.100.233:40183 in memory (size: 60.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,655 INFO codegen.CodeGenerator: Code generated in 142.403164 ms\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,659 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 317.5 KiB, free 1027.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,669 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,669 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.100.233:40183 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,670 INFO spark.SparkContext: Created broadcast 26 from toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,671 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,671 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,695 INFO spark.SparkContext: Starting job: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,696 INFO scheduler.DAGScheduler: Got job 17 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,696 INFO scheduler.DAGScheduler: Final stage: ResultStage 25 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,696 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,696 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,696 INFO scheduler.DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[63] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,703 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 230.1 KiB, free 1027.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,705 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 60.9 KiB, free 1027.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,706 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.100.233:40183 (size: 60.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,706 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,707 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[63] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,707 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,709 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 17) (algo-2, executor 1, partition 0, PROCESS_LOCAL, 5066 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:36,721 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-2:35829 (size: 60.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:37,336 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-2:35829 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:37,753 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 17) in 1044 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:37,753 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:37,755 INFO scheduler.DAGScheduler: ResultStage 25 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110) finished in 1.056 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:37,756 INFO scheduler.DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:37,756 INFO cluster.YarnScheduler: Killing all running tasks in stage 25: Stage finished\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:37,756 INFO scheduler.DAGScheduler: Job 17 finished: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110, took 1.061139 s\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:37,995 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,004 INFO server.AbstractConnector: Stopped Spark@3661f38b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,004 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.100.233:4040\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,008 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,021 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,021 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,027 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,040 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,066 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,066 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,072 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[35m[/[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:20.541+0000: [GC (Allocation Failure) [PSYoungGen: 56320K->6697K(65536K)] 56320K->6705K(216064K), 0.0067930 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:21.059+0000: [GC (Allocation Failure) [PSYoungGen: 63017K->7310K(65536K)] 63025K->7326K(216064K), 0.0072844 secs] [Times: user=0.01 sys=0.01, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:21.290+0000: [GC (Metadata GC Threshold) [PSYoungGen: 41527K->7552K(65536K)] 41543K->7576K(216064K), 0.0053572 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:21.295+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 7552K->0K(65536K)] [ParOldGen: 24K->7240K(93184K)] 7576K->7240K(158720K), [Metaspace: 20387K->20387K(1067008K)], 0.0260316 secs] [Times: user=0.05 sys=0.01, real=0.03 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:21.759+0000: [GC (Allocation Failure) [PSYoungGen: 56320K->4919K(88064K)] 63560K->12168K(181248K), 0.0054467 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:22.110+0000: [GC (Allocation Failure) [PSYoungGen: 87863K->8273K(117760K)] 95112K->15521K(210944K), 0.0080951 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:22.180+0000: [GC (Metadata GC Threshold) [PSYoungGen: 19223K->3646K(133632K)] 26472K->10902K(226816K), 0.0045727 secs] [Times: user=0.00 sys=0.01, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:22.184+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 3646K->0K(133632K)] [ParOldGen: 7256K->8558K(132608K)] 10902K->8558K(266240K), [Metaspace: 33930K->33928K(1079296K)], 0.0275946 secs] [Times: user=0.05 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:27.219+0000: [GC (Allocation Failure) [PSYoungGen: 124416K->9707K(134144K)] 132974K->27624K(266752K), 0.0196414 secs] [Times: user=0.03 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:27.510+0000: [GC (Metadata GC Threshold) [PSYoungGen: 48885K->10861K(201728K)] 66802K->28785K(334336K), 0.0123283 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:27.523+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 10861K->0K(201728K)] [ParOldGen: 17924K->18335K(180224K)] 28785K->18335K(381952K), [Metaspace: 56003K->56003K(1101824K)], 0.1027403 secs] [Times: user=0.23 sys=0.00, real=0.10 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:29.112+0000: [GC (Allocation Failure) [PSYoungGen: 185856K->15230K(201728K)] 204191K->33574K(381952K), 0.0159656 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:31.224+0000: [GC (Allocation Failure) [PSYoungGen: 201086K->17161K(256000K)] 219430K->35512K(436224K), 0.0176022 secs] [Times: user=0.03 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stdout] 2025-07-17T08:40:37.587+0000: [GC (Allocation Failure) [PSYoungGen: 255753K->15200K(259072K)] 274104K->99095K(439var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:32,735 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 13). 3428 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:36,711 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:36,711 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 17)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:36,712 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:36,713 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:36,719 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 60.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:36,722 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:36,723 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 230.1 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,140 INFO codegen.CodeGenerator: Code generated in 258.420396 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,278 INFO codegen.CodeGenerator: Code generated in 78.344177 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,298 INFO codegen.CodeGenerator: Code generated in 6.694943 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,311 INFO datasources.FileScanRDD: TID: 17 - Reading current file: path: s3://modeldevelopmentk21/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,327 INFO codegen.CodeGenerator: Code generated in 10.484905 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,328 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,335 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,337 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,347 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,392 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000002/stderr] 2025-07-17 08:40:37,403 INFO codegen.CodeGenerator: Code generated in 6.907286 ms\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,135 INFO launcher.ContainerLaunch: Container container_1752741594005_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,144 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,146 INFO launcher.ContainerCleanup: Cleaning up container container_1752741594005_0001_01_000002\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,148 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000002\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,148 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000002\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,151 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,151 INFO application.ApplicationImpl: Removing container_1752741594005_0001_01_000002 from application application_1752741594005_0001\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,151 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1752741594005_0001_01_000002\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,152 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1752741594005_0001\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,165 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,165 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000002/launch_container.sh]\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,165 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000002/container_tokens\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,165 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000002/container_tokens]\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,165 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000002/sysfs\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:38,165 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000002/sysfs]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,094 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,119 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,119 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,120 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-701e440a-5fca-46a3-a136-04546a52bad2/pyspark-fa5fcfa8-16b7-42aa-a1eb-fce8b44d784f\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,125 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-701e440a-5fca-46a3-a136-04546a52bad2\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,127 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-bc42d147-30eb-4149-bfab-68410c0cbbbf\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,133 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,133 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1752741594005_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,133 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,134 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,134 INFO attempt.RMAppAttemptImpl: appattempt_1752741594005_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,134 INFO rmapp.RMAppImpl: Updating application application_1752741594005_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,134 INFO recovery.RMStateStore: Updating info for app: application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,134 INFO rmapp.RMAppImpl: application_1752741594005_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,135 INFO attempt.RMAppAttemptImpl: appattempt_1752741594005_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,135 INFO rmapp.RMAppImpl: application_1752741594005_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,146 INFO launcher.ContainerLaunch: Container container_1752741594005_0001_01_000003 succeeded \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,154 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,154 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,154 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,155 INFO launcher.ContainerCleanup: Cleaning up container container_1752741594005_0001_01_000003\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,157 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000003\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,157 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000003\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,160 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,161 INFO application.ApplicationImpl: Removing container_1752741594005_0001_01_000003 from application application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,161 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1752741594005_0001_01_000003\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,162 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,164 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000003 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,164 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,174 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000003/launch_container.sh\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,174 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000003/launch_container.sh]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,174 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000003/container_tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,174 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000003/container_tokens]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,174 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000003/sysfs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,175 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000003/sysfs]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,244 INFO resourcemanager.ApplicationMasterService: application_1752741594005_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[34m07-17 08:40 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,644 INFO launcher.ContainerLaunch: Container container_1752741594005_0001_01_000001 succeeded \u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,644 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,645 INFO launcher.ContainerCleanup: Cleaning up container container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,645 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,645 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,646 INFO container.ContainerImpl: Container container_1752741594005_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,646 INFO application.ApplicationImpl: Removing container_1752741594005_0001_01_000001 from application application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,647 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,647 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,649 INFO rmcontainer.RMContainerImpl: container_1752741594005_0001_01_000001 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,650 INFO resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,650 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000001#011RESOURCE=<memory:896, max memory:15892, vCores:1, max vCores:4>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,651 INFO security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,653 INFO attempt.RMAppAttemptImpl: appattempt_1752741594005_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,657 INFO rmapp.RMAppImpl: application_1752741594005_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,657 INFO capacity.CapacityScheduler: Application Attempt appattempt_1752741594005_0001_000001 is done. finalState=FINISHED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,657 INFO scheduler.AppSchedulingInfo: Application application_1752741594005_0001 requests cleared\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,658 INFO capacity.LeafQueue: Application removed - appId: application_1752741594005_0001 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,658 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=Application Finished - Succeeded#011TARGET=RMAppManager#011RESULT=SUCCESS#011APPID=application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,658 INFO capacity.ParentQueue: Application removed - appId: application_1752741594005_0001 user: root leaf-queue of parent: root #applications: 0\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,659 INFO amlauncher.AMLauncher: Cleaning master appattempt_1752741594005_0001_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,660 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,660 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000001/launch_container.sh]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,660 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000001/container_tokens\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,660 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000001/container_tokens]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,660 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000001/sysfs\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,660 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001/container_1752741594005_0001_01_000001/sysfs]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,661 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1752741594005_0001,name=PySparkApp,user=root,queue=default,state=FINISHED,trackingUrl=http://algo-1:8088/proxy/application_1752741594005_0001/,appMasterHost=10.0.100.233,submitTime=1752741607821,startTime=1752741607891,launchTime=1752741608474,finishTime=1752741638134,finalStatus=SUCCEEDED,memorySeconds=634742,vcoreSeconds=74,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=SPARK,resourceSeconds=634742 MB-seconds\\, 74 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\\, 0 vcore-seconds,applicationTags=\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,668 INFO ipc.Server: Auth successful for appattempt_1752741594005_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,672 INFO containermanager.ContainerManagerImpl: Stopping container with container Id: container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:38,672 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.100.233#011OPERATION=Stop Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1752741594005_0001#011CONTAINERID=container_1752741594005_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:39,158 INFO application.ApplicationImpl: Application application_1752741594005_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:39,158 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:39,158 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1752741594005_0001\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:39,159 INFO application.ApplicationImpl: Application application_1752741594005_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:39,159 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1752741594005_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:39,651 INFO nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1752741594005_0001_01_000001]\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:39,653 INFO application.ApplicationImpl: Application application_1752741594005_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:39,654 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:39,654 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:39,655 INFO application.ApplicationImpl: Application application_1752741594005_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:39,655 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1752741594005_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,005 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 replica FinalizedReplica, blk_1073741825_1001, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,006 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 replica FinalizedReplica, blk_1073741826_1002, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,006 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 replica FinalizedReplica, blk_1073741827_1003, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,007 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 replica FinalizedReplica, blk_1073741828_1004, FINALIZED\n",
      "  getNumBytes()     = 46002121\n",
      "  getBytesOnDisk()  = 46002121\n",
      "  getVisibleLength()= 46002121\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,007 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 replica FinalizedReplica, blk_1073741829_1005, FINALIZED\n",
      "  getNumBytes()     = 889814\n",
      "  getBytesOnDisk()  = 889814\n",
      "  getVisibleLength()= 889814\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,007 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 replica FinalizedReplica, blk_1073741830_1006, FINALIZED\n",
      "  getNumBytes()     = 41587\n",
      "  getBytesOnDisk()  = 41587\n",
      "  getVisibleLength()= 41587\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,007 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 replica FinalizedReplica, blk_1073741831_1007, FINALIZED\n",
      "  getNumBytes()     = 266133\n",
      "  getBytesOnDisk()  = 266133\n",
      "  getVisibleLength()= 266133\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,022 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741825_1001 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,038 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741826_1002 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,054 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741827_1003 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,059 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741828_1004 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,060 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741829_1005 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,060 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741830_1006 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[34m2025-07-17 08:40:42,060 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741831_1007 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,284 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 replica FinalizedReplica, blk_1073741825_1001, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,285 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 replica FinalizedReplica, blk_1073741826_1002, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,285 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 replica FinalizedReplica, blk_1073741827_1003, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,285 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 replica FinalizedReplica, blk_1073741828_1004, FINALIZED\n",
      "  getNumBytes()     = 46002121\n",
      "  getBytesOnDisk()  = 46002121\n",
      "  getVisibleLength()= 46002121\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,286 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 replica FinalizedReplica, blk_1073741829_1005, FINALIZED\n",
      "  getNumBytes()     = 889814\n",
      "  getBytesOnDisk()  = 889814\n",
      "  getVisibleLength()= 889814\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,286 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 replica FinalizedReplica, blk_1073741830_1006, FINALIZED\n",
      "  getNumBytes()     = 41587\n",
      "  getBytesOnDisk()  = 41587\n",
      "  getVisibleLength()= 41587\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,286 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 replica FinalizedReplica, blk_1073741831_1007, FINALIZED\n",
      "  getNumBytes()     = 266133\n",
      "  getBytesOnDisk()  = 266133\n",
      "  getVisibleLength()= 266133\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,302 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741825_1001 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,318 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741826_1002 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,334 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741827_1003 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,340 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741828_1004 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,341 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741829_1005 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,341 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741830_1006 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:44,341 INFO impl.FsDatasetAsyncDiskService: Deleted BP-665918883-10.0.100.233-1752741586407 blk_1073741831_1007 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-665918883-10.0.100.233-1752741586407/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[34m07-17 08:40 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m07-17 08:40 root         INFO     copying /tmp/spark-events/application_1752741594005_0001 to /opt/ml/processing/spark-events/application_1752741594005_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1752741594005_0001/container_1752741594005_0001_01_000003/stderr] 2025-07-17 08:40:35,472 INFO codegen.CodeGenerator: Code generated in 1\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:54,186 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: \"algo-2/10.0.80.5\"; destination host is: \"algo-1\":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.\u001b[0m\n",
      "\u001b[35m07-17 08:40 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4da47c0b50>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m07-17 08:40 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4da4745130>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m2025-07-17 08:40:56,282 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.io.EOFException: End of File Exception between local host is: \"algo-2/10.0.80.5\"; destination host is: \"algo-1\":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:750)\u001b[0m\n",
      "\u001b[35mCaused by: java.io.EOFException\u001b[0m\n",
      "\u001b[35m#011at java.io.DataInputStream.readInt(DataInputStream.java:392)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)\u001b[0m\n",
      "\u001b[35m07-17 08:40 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4da4745250>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:00,282 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:01,283 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m07-17 08:41 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4da4745460>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:02,284 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:03,284 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:04,285 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:05,286 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:06,287 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:07,287 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:08,288 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:09,289 INFO ipc.Client: Retrying connect to server: algo-1/10.0.100.233:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-07-17 08:41:09,290 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.net.ConnectException: Call From algo-2/10.0.80.5 to algo-1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:750)\u001b[0m\n",
      "\u001b[35mCaused by: java.net.ConnectException: Connection refused\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1435)\u001b[0m\n",
      "\u001b[35m#011... 9 more\u001b[0m\n",
      "\u001b[35m07-17 08:41 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4da4745670>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m07-17 08:41 smspark-submit INFO     primary is down, worker now exiting\u001b[0m\n",
      "\u001b[35m[\u001b[0m\n",
      "\n",
      "Spark Processing Job Completed.\n"
     ]
    }
   ],
   "source": [
    "#processing-job\n",
    "import os\n",
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "input_raw_data_prefix = \"data/input\"\n",
    "output_preprocessed_data_prefix = \"data/output\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor.run(\n",
    "    submit_app=\"s3://modeldevelopmentk21/scripts/pyspark_preprocessing.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"validation_data\", \n",
    "                         source=\"/opt/ml/processing/validation\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"validation\")),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\", bucket,\n",
    "        \"--s3_input_key_prefix\", input_raw_data_prefix,\n",
    "        \"--s3_output_bucket\", bucket,\n",
    "        \"--s3_output_key_prefix\", output_preprocessed_data_prefix],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, logs_prefix),\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"Spark Processing Job Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** While the processing job is running, you can also monitor the job progress from the SageMaker AI console. To monitor the processing job: \n",
    "\n",
    "1. Navigate to the SageMaker AI console. \n",
    "\n",
    "2. From the left pane, select **Prossecing**, then **Processing jobs**. \n",
    "\n",
    "3. The processing job name starts with **sm-spark-preprocessor-**. \n",
    "\n",
    "4. After the processing job completes, return to this notebook. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Validate the data processing results\n",
    "\n",
    "Validate the output of the data processing job that you ran by reviewing the first five rows of the train and validation output datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T08:43:27.559403Z",
     "iopub.status.busy": "2025-07-17T08:43:27.558733Z",
     "iopub.status.idle": "2025-07-17T08:43:28.577427Z",
     "shell.execute_reply": "2025-07-17T08:43:28.576334Z",
     "shell.execute_reply.started": "2025-07-17T08:43:27.559376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://modeldevelopmentk21/data/output/train/\n",
      "\"(84,[3,10,22,34,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,143331.0,7.0,40.0])\"\n",
      "\"(84,[0,10,22,32,42,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,28544.0,7.0,20.0])\"\n",
      "\"(84,[0,10,22,32,43,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,40299.0,7.0,25.0])\"\n",
      "\"(84,[0,10,22,37,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,61838.0,7.0,40.0])\"\n",
      "\"(84,[0,12,22,32,43,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,190941.0,6.0,20.0])\"\n"
     ]
    }
   ],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T08:43:29.803414Z",
     "iopub.status.busy": "2025-07-17T08:43:29.803005Z",
     "iopub.status.idle": "2025-07-17T08:43:30.812658Z",
     "shell.execute_reply": "2025-07-17T08:43:30.811881Z",
     "shell.execute_reply.started": "2025-07-17T08:43:29.803388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://modeldevelopmentk21/data/output/validation/\n",
      "\"(84,[3,12,22,34,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,165361.0,6.0,40.0])\"\n",
      "\"(84,[3,6,22,34,43,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,240183.0,9.0,45.0])\"\n",
      "\"(84,[0,10,22,30,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,78528.0,7.0,20.0])\"\n",
      "\"(84,[0,6,22,31,43,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,170183.0,9.0,10.0])\"\n",
      "\"(84,[0,7,22,31,43,48,55,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,262118.0,10.0,22.0])\"\n"
     ]
    }
   ],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/validation/validation_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You have used SageMaker Processing to successfully create a Spark processing job using the SageMaker Python SDK and run a processing job.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with the **Conclusion**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
